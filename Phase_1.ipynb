{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    " \n",
    "# Load the CSV file\n",
    "file_path = '/content/final_gemma9b_150datapoints.csv'\n",
    "df = pd.read_csv(file_path)\n",
    " \n",
    "display(df.head())\n",
    " \n",
    "unique_status_values = df['actual_status'].unique().tolist()\n",
    "print(unique_status_values)\n",
    " \n",
    "df = df.drop(columns=['predicted_status'])\n",
    " \n",
    "display(df.head())\n",
    " \n",
    "target_samples = 30\n",
    " \n",
    "# Create a new balanced dataset by undersampling each class to 100 samples\n",
    "undersampled_df = df.groupby('actual_status').apply(lambda x: x.sample(n=target_samples, random_state=42) if len(x) > target_samples else x)\n",
    " \n",
    "# Reset index after sampling\n",
    "undersampled_df = undersampled_df.reset_index(drop=True)\n",
    " \n",
    "# Display the new balanced dataset size\n",
    "print(undersampled_df['actual_status'].value_counts())\n",
    " \n",
    " \n",
    "# Display the first few rows of the undersampled dataset\n",
    "display(undersampled_df.head())\n",
    "print(len(undersampled_df))\n",
    " \n",
    " \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    " \n",
    " \n",
    " \n",
    "READER_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME\n",
    "    #, quantization_config=bnb_config\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    " \n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    device =0,\n",
    ")\n",
    "csv_file_path = \"/content/phase1_answers.csv\"\n",
    "txt_file_path = \"/content/phase1_answers.txt\"\n",
    " \n",
    " \n",
    "start_time = time.time()\n",
    "with open(txt_file_path, 'w', encoding='utf-8') as txt_file, \\\n",
    "     open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    " \n",
    "    # Setup CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"User Query\", \"Predicted Disorder\", \"Full Response\"])\n",
    " \n",
    "    user_queries = undersampled_df[\"statement\"].tolist()\n",
    " \n",
    " \n",
    "    for sample_question in tqdm(user_queries, desc=\"Processing Queries\"):\n",
    "        # prompt_text = tokenizer.apply_chat_template(\n",
    "        #     prompt_in_chat_format, question = sample_question, tokenize=False, add_generation_prompt=True\n",
    "        # )\n",
    " \n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"You are an AI assistant specialized in diagnosing mental disorders in humans.\n",
    "              Using the information contained in the context, answer the question comprehensively.\n",
    " \n",
    "              The **Diagnosed Mental Disorder** must be exactly one from the list below:\n",
    "              [Depression, Suicidal, Anxiety, Stress, Bi-Polar, Personality Disorder]\n",
    " \n",
    "              ---\n",
    " \n",
    "              Question: {sample_question}\"\"\"\n",
    "                      },\n",
    "                      {\"role\": \"assistant\", \"content\": \"\"}\n",
    "                  ],\n",
    "                  tokenize=False,\n",
    "                  add_generation_prompt=True\n",
    "              )\n",
    " \n",
    " \n",
    "        # Step 4: Call your pipeline\n",
    "        output = READER_LLM(prompt_text)[0][\"generated_text\"]\n",
    " \n",
    "        # Write the answer to the text file\n",
    "        match = re.search(r\"\\*\\*Diagnosed Mental Disorder\\*\\*[:\\-]*\\s*(\\w+(?:\\s*\\-*\\w+)*)\", output)\n",
    "        predicted_disorder = match.group(1).strip() if match else \"Unknown\"\n",
    " \n",
    "        # ✏️ Write to TXT file\n",
    "        txt_file.write(f\"Query: {sample_question}\\n\")\n",
    "        txt_file.write(f\"Answer:\\n{output}\\n\")\n",
    "        txt_file.write(f\"{'='*50}\\n\\n\")\n",
    " \n",
    "        # ✏️ Write to CSV file\n",
    "        csv_writer.writerow([sample_question, predicted_disorder, output])\n",
    " \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    " \n",
    "print(f\"\\n✅ All user queries processed successfully and answers saved to 'phase1_answers.txt' & 'phase1_answers.csv' \")\n",
    "print(f\"⏱️ Total processing time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
