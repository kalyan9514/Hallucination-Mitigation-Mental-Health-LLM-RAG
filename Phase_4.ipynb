{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFY-2jgN4X1U",
    "outputId": "95c22ef8-abca-4df7-fa13-dcd1e99ae99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3jRsjd5BD39",
    "outputId": "159f450e-c06f-4b2b-f696-1b0580f54e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.47)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrW2DcWkBO4Q",
    "outputId": "f96bfac6-4d2d-4215-b9cb-49b4ac63d6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADwhpIlPC-4m",
    "outputId": "5e1d65fa-b569-4443-e911-a1447ce20872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWgwR07gDFPD",
    "outputId": "8ae86695-ccde-41a8-c4e3-7c00fc14b12d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x0nLus5fCCGM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKjiihOY_1ti",
    "outputId": "7ebef652-acf7-48c1-9d74-5c3983043da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded s41586-024-07421-0.pdf with 12 pages/documents\n",
      "Loaded tacl_a_00695.pdf with 19 pages/documents\n",
      "Loaded 2023.findings-emnlp.182.pdf with 17 pages/documents\n",
      "Total documents loaded: 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the folder path in your Google Drive where PDFs are stored\n",
    "folder_path = '/content/drive/My Drive/documents_RAG'\n",
    "\n",
    "# List all files and filter to include only PDFs\n",
    "files = os.listdir(folder_path)\n",
    "pdf_files = [file for file in files if file.lower().endswith('.pdf')]\n",
    "\n",
    "documents = []\n",
    "for file in pdf_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        # Load the PDF using PyPDFLoader\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()  # This returns a list of Document objects\n",
    "        documents.extend(docs)\n",
    "        print(f\"Loaded {file} with {len(docs)} pages/documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "print(\"Total documents loaded:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGYZXUU7ABqP",
    "outputId": "8a00c699-f579-4a74-a845-f18551634a24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 0, 'page_label': '625'}, page_content='Nature | Vol 630 | 20 June 2024 | 625\\nArticle\\nDetecting hallucinations in large language \\nmodels using semantic entropy\\nSebastian Farquhar1,2\\u2009✉, Jannik Kossen1,2, Lorenz Kuhn1,2 & Yarin Gal1\\nLarge language model (LLM) systems, such as ChatGPT1 or Gemini2, can show \\nimpressive reasoning and question-answering capabilities but often ‘hallucinate’  \\nfalse outputs and unsubstantiated answers3,4. Answering unreliably or without the \\nnecessary information prevents adoption in diverse fields, with problems including \\nfabrication of legal precedents5 or untrue facts in news articles6 and even posing a  \\nrisk to human life in medical domains such as radiology7. Encouraging truthfulness \\nthrough supervision or reinforcement has\\xa0been only partially successful8. Researchers \\nneed a general method for detecting hallucinations in LLMs that works even with new \\nand unseen questions to which humans might not know the answer. Here we develop \\nnew methods grounded in statistics, proposing entropy-based uncertainty estimators \\nfor LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and \\nincorrect generations. Our method addresses the fact that one idea can be expressed \\nin many ways by computing uncertainty at the level of meaning rather than specific \\nsequences of words. Our method works across datasets and tasks without a\\xa0priori \\nknowledge of the task, requires no task-specific data and robustly generalizes to new \\ntasks not seen before. By detecting when a prompt is likely to produce a confabulation, \\nour method helps users understand when they must take extra care with LLMs and \\nopens up new possibilities for using LLMs that are otherwise prevented by their \\nunreliability.\\n‘Hallucinations’ are a critical problem9 for natural language genera-\\ntion systems using large language models (LLMs), such as ChatGPT1 or \\nGemini2, because users cannot trust that any given output is correct.\\nHallucinations are often defined as LLMs generating “content \\nthat is nonsensical or unfaithful to the provided source content”9–11 \\nbut they have come to include a vast array of failures of faithfulness \\nand factuality. We focus on a subset of hallucinations which we call \\n‘confabulations’12 for which LLMs fluently make claims that are both \\nwrong and arbitrary—by which we mean that the answer is sensitive \\nto irrelevant details such as random seed. For example, when asked a \\nmedical question “What is the target of Sotorasib?” an LLM confabu-\\nlates by sometimes answering KRASG12\\xa0‘C’ (correct) and other times \\nKRASG12\\xa0‘D’ (incorrect) despite identical instructions. We distinguish \\nthis from cases in which a similar ‘symptom’ is caused by the following \\ndifferent mechanisms: when LLMs are consistently wrong as a result of \\nbeing trained on erroneous data such as common misconceptions13; \\nwhen the LLM ‘lies’ in pursuit of a reward14; or systematic failures of \\nreasoning or generalization. We believe that combining these dis -\\ntinct mechanisms in the broad category hallucination is unhelpful. \\nOur method makes progress on a portion of the problem of providing \\nscalable oversight15 by detecting confabulations that people might \\notherwise find plausible. However, it does not guarantee factuality \\nbecause it does not help when LLM outputs are systematically bad. Nev-\\nertheless, we significantly improve question-answering accuracy for \\nstate-of-the-art LLMs, revealing that confabulations are a great source of  \\nerror at present.\\nWe show how to detect confabulations by developing a quantita -\\ntive measure of when an input is likely to cause an LLM to generate \\narbitrary and ungrounded answers. Detecting confabulations allows \\nsystems built on LLMs to avoid answering questions likely to cause \\nconfabulations, to make users aware of the unreliability of answers \\nto a question or to supplement the LLM with more grounded search \\nor retrieval. This is essential for the critical emerging field of free-  \\nform generation in which naive approaches, suited to closed vocabu -\\nlary and multiple choice, fail. Past work on uncertainty for LLMs has \\nfocused on simpler settings, such as classifiers 16,17 and regressors18,19, \\nwhereas the most exciting applications of LLMs relate to free-form \\ngenerations.\\nThe term hallucination in the context of machine learning originally \\ncomes from filling in ungrounded details, either as a deliberate strat-\\negy20 or as a reliability problem4. The appropriateness of the meta-\\nphor has been questioned as promoting undue anthropomorphism21. \\nAlthough we agree that metaphor must be used carefully with LLMs22, \\nthe widespread adoption of the term hallucination reflects the fact \\nthat it points to an important phenomenon. This work represents a \\nstep towards making that phenomenon more precise.\\nT o detect confabulations, we use probabilistic tools to define and \\nthen measure the ‘semantic’ entropy of the generations of an LLM—an \\nentropy that is computed over meanings of sentences. High entropy \\ncorresponds to high uncertainty23–25—so semantic entropy is one way \\nto estimate semantic uncertainties. Semantic uncertainty, the broader \\ncategory of measures we introduce, could be operationalized with other \\nhttps://doi.org/10.1038/s41586-024-07421-0\\nReceived: 17 July 2023\\nAccepted: 12 April 2024\\nPublished online: 19 June 2024\\nOpen access\\n Check for updates\\n1OATML, Department of Computer Science, University of Oxford, Oxford, UK. 2These authors contributed equally: Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn. ✉e-mail: sebfar@gmail.com'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 1, 'page_label': '626'}, page_content='626 | Nature | Vol 630 | 20 June 2024\\nArticle\\nmeasures of uncertainty, such as mutual information, instead. Entropy \\nin free-form generation is normally hard to measure because answers \\nmight mean the same thing (be semantically equivalent) despite being \\nexpressed differently (being syntactically or lexically distinct). This \\ncauses naive estimates of entropy or other lexical variation scores26 to \\nbe misleadingly high when the same correct answer might be written \\nin many ways without changing its meaning.\\nBy contrast, our semantic entropy moves towards estimating the \\nentropy of the distribution of meanings of free-form answers to ques-\\ntions, insofar as that is possible, rather than the distribution over the \\n‘tokens’ (words or word-pieces) which LLMs natively represent. This \\ncan be seen as a kind of semantic consistency check27 for random seed \\nvariation. An overview of our approach is provided in Fig.\\xa01 and a worked \\nexample in Supplementary Table\\xa01.\\nIntuitively, our method works by sampling several possible answers \\nto each question and clustering them algorithmically into answers that \\nhave similar meanings, which we determine on the basis of whether \\nanswers in the same cluster entail each other bidirectionally28. That \\nis, if sentence A entails that sentence B is true and vice versa, then we \\nconsider them to be in the same semantic cluster. We measure entail-\\nment using both general-purpose LLMs and natural language inference \\n(NLI) tools developed specifically for detecting entailment for which \\nwe show direct evaluations in Supplementary Tables\\xa02 and 3 and Sup-\\nplementary Fig.\\xa01. T extual entailment has previously been shown to \\ncorrelate with faithfulness10 in the context of factual consistency29 as \\nwell as being used to measure factuality in abstractive summarization30, \\nespecially when applied at the right granularity31.\\nSemantic entropy detects confabulations in free-form text genera-\\ntion across a range of language models and domains, without previous \\ndomain knowledge. Our evaluations cover question answering in trivia \\nknowledge (TriviaQA32), general knowledge (SQuAD 1.1; ref. 33), life \\nsciences (BioASQ34) and open-domain natural questions (NQ-Open35) \\nderived from actual queries to Google Search36. In addition, seman-\\ntic entropy detects confabulations in mathematical word problems \\n(SVAMP37) and in a biography-generation dataset, FactualBio, accom-\\npanying this paper.\\nOur results for TriviaQA, SQuAD, BioASQ, NQ-Open and SVAMP are \\nall evaluated context-free and involve sentence-length answers (96\\u2009±\\u200970 \\ncharacters, mean\\u2009±\\u2009s.d.) and use LLaMA 2 Chat (7B, 13B and 70B param-\\neters)38, Falcon Instruct (7B and 40B)39 and Mistral Instruct (7B)40. In the \\nSupplementary Information, we further consider short-phrase-length \\nanswers. Results for FactualBio (442\\u2009±\\u2009122 characters) use GPT-4  \\n(ref. 1). At the time of writing, GPT-4 (ref. 1) did not expose output prob-\\nabilities41 or hidden states, although it does now. As a result, we propose \\na discrete approximation of our estimator for semantic entropy which \\nallows us to run experiments without access to output probabilities, \\nwhich we use for all GPT-4 results in this paper and which performs  \\nsimilarly well.\\nOur confabulation detection with semantic entropy is more robust \\nto user inputs from previously unseen domains than methods which \\nWho is Freddie Frith?\\nFreddie Frith was an\\nEnglish motorcycle\\nroad racer who\\nbecame a champion\\nin both pre-World\\nWar II and post-war\\neras. He won the\\n1935 and 1937 Grand\\nPrix motorcycle\\nracing European\\nChampionships.\\nAfter retiring\\nfrom competition,\\nhe became the\\npresident of the Auto\\nCycle Union, the\\ngoverning body of\\nBritish motorcycle\\nracing. He was also\\nan accomplished\\nmotorcycle dealer\\nand manufacturer.\\nFrith was born in 1911\\nand died in 1988.\\nFact 1 of 7 :\\nFreddie Frith was an\\nEnglish motorcycle\\nroad racer\\nFact 6 of 7 :\\nFrith was born\\nin 1911\\nQ1 of M for Fact 1:\\nWhat notable\\naccomplishments\\ndid Freddie Frith\\nachieve?\\n30 May 1909\\n29 March 1909\\n26 October 1911\\nQ2 of M for Fact 1:\\nWhat was Freddie\\nFrith known for?\\nQ2 of M for Fact 6:\\nWhen was Freddie\\nFrith born?\\nQ1 of M for Fact 6:\\nWhen was Freddie\\nFrith’s year of\\nbirth?\\nFive-time motorcycle racing world...\\nMotorcycle road racing world...\\nMotorcycle racing champion...\\n1909\\n1909\\n1909\\nMotorcycle racing.\\nHe was a world champion motorcycle...\\nHe was president of the\\nAuto Cycle Union... \\nWhere is the\\nEiffel Tower? \\nParis\\nIt’s Paris\\nFrance’s capital Paris\\nRome\\nIt’s Rome\\nBerlin\\nParis\\nIt’s Paris\\nFrance’s capital Paris\\nRome\\nIt’s Rome\\nBerlin\\na Semantic entropy\\nb Application to FactualBio paragraphs User: Question\\nFactoid decomposition Possible questions\\nGenerate answers and\\ncluster by meaning \\nSemantic entropy\\nprobability \\nNot likely confabulationLikely confabulation\\nGenerate\\nUser: Question Generate\\nMisleadingly high naive entropy \\nLLM answers Probability\\nLow semantic entropy\\nLLM answers Probability\\nCluster\\nanswers by\\nsemantic\\nmeaning\\nLLMLLM\\nLLMLLM LLMLLM\\nFig. 1 | Overview of semantic entropy and confabulation detection.  a, Naive \\nentropy-based uncertainty measures variation in the exact answers, treating \\n‘Paris’, ‘It’s Paris’ and ‘France’s capital Paris’ as different. But this is unsuitable \\nfor language tasks for which sometimes different answers mean the same \\nthings. Our semantic entropy clusters answers which share meanings before \\ncomputing the entropy. A low semantic entropy shows that the LLM is confident \\nabout the meaning. b , Semantic entropy can also detect confabulations in \\nlonger passages. We automatically decompose a long generated answer into \\nfactoids. For each factoid, an LLM generates questions to which that factoid \\nmight have been the answer. The original LLM then samples\\xa0 M possible answers \\nto these questions. Finally, we compute the semantic entropy over the answers \\nto each specific question, including the original factoid. Confabulations are \\nindicated by high average semantic entropy for questions associated with that \\nfactoid. Here, semantic entropy classifies Fact 1 as probably not a confabulation \\nbecause generations often mean the same thing, despite very different \\nwordings, which a naive entropy would have missed.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 2, 'page_label': '627'}, page_content='Nature | Vol 630 | 20 June 2024 | 627\\naim to ‘learn’ how to detect confabulations from a set of example dem-\\nonstrations. Our method is unsupervised, meaning that we do not \\nneed labelled examples of confabulations. By contrast, supervised \\nmethods detect confabulations by learning patterns behind examples \\nof confabulations, assuming that future questions preserve these pat-\\nterns. But this assumption is often untrue in new situations or with \\nconfabulations that human overseers are unable to identify (com -\\npare Fig. 17 of ref. 24). As a strong supervised baseline, we compare \\nto an embedding regression method inspired by ref. 24 which trains \\na logistic regression classifier to predict whether the model correctly \\nanswered a question on the basis of the final ‘embedding’ (hidden state) \\nof the LLM. We also use the P(True) method24 which looks at the prob-\\nability with which an LLM predicts that the next token is ‘True’ when \\nfew-shot prompted to compare a main answer with ‘brainstormed’  \\nalternatives.\\nConfabulations contribute\\xa0substantially to incorrect answers given \\nby language models. We show that semantic entropy can be used to pre-\\ndict many incorrect model answers and to improve question-answering \\naccuracy by refusing to answer those questions the model is uncertain \\nabout. Corresponding to these two uses, we evaluate two main metrics. \\nFirst, the widely used area under the receiver operating characteristic \\n(AUROC) curve for the binary event that a given answer is incorrect. \\nThis measure captures both precision and recall and ranges from \\n0 to 1, with 1 representing a perfect classifier and 0.5 representing \\nan un-informative classifier. We also show a new measure, the area \\nunder the ‘rejection accuracy’ curve (AURAC). This studies the case in \\nwhich the confabulation detection score is used to refuse to answer \\nthe questions judged most likely to cause confabulations. Rejection \\naccuracy is the accuracy of the answers of the model on the remaining \\nquestions and the area under this curve is a summary statistic over \\nmany thresholds (representative threshold accuracies are provided in  \\nSupplementary Material). The AURAC captures the accuracy improve-\\nment which users would experience if semantic entropy was used to \\nfilter out questions causing the highest entropy.\\nDetecting confabulations in QA and math\\nIn Fig.\\xa02, we show that both semantic entropy and its discrete approxi-\\nmation outperform our best baselines for sentence-length generations. \\nThese results are averaged across datasets and provide the actual scores \\non the held-out evaluation dataset. We report the raw average score \\nacross held-out evaluation datasets without standard error because \\nthe distributional characteristics are more a property of the models \\nand datasets selected than the method. Consistency of relative results \\nacross different datasets is a stronger indicator of variation in this case.\\nSemantic entropy greatly outperforms the naive estimation of uncer-\\ntainty using entropy: computing the entropy of the length-normalized \\njoint probability of the token sequences. Naive entropy estimation \\nignores the fact that token probabilities also express the uncertainty of \\nthe model over phrasings that do not change the meaning of an output.\\nOur methods also outperform the supervised embedding regression \\nmethod both in- and out-of-distribution. In pale-yellow\\xa0bars we show \\nthat embedding regression performance deteriorates when its train-\\ning data do not match the deployment distribution—which mirrors the \\ncommon real-world case in which there is a distribution shift between \\ntraining and deployment42—the plotted value is the average metric for \\nembedding regression trained on one of the four ‘off-distribution’ \\ndatasets for that evaluation. This is critical because reliable uncertainty \\nis most important when the data distribution shifts. Semantic entropy \\nalso outperforms P(True) which is supervised ‘in-context’; that is, it is \\nadapted to the deployment task with a few training examples provided \\nin the LLM prompt itself. The discrete variant of semantic entropy per-\\nforms similarly to our standard estimator, despite not requiring exact \\noutput probabilities.\\nAveraged across the 30 combinations of tasks and models we study, \\nsemantic entropy achieves the best AUROC value of 0.790 whereas \\nnaive entropy (0.691), P(True) (0.698) and the embedding regression \\nbaseline (0.687) lag behind it. Semantic entropy performs well con-\\nsistently, with stable performance (between 0.78 and 0.81 AUROC) \\nAUROC AURAC\\nLLaMA 2 Chat 7B\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAUROC AURAC\\nLLaMA 2 Chat 13B\\nSemantic entropy\\nDiscrete semantic entropy\\nNaive entropy\\nP(True), ref. 24\\nEmbedding regression\\nEmbedding regression - OOD\\nAUROC AURAC\\nLLaMA 2 Chat 70B\\nFalcon 7B Instruct\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nFalcon 40B Instruct Mistral 7B Instruct\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAUROC AURAC AUROC AURAC AUROC AURAC\\nFig. 2 | Detecting confabulations in sentence-length generations.  Semantic \\nentropy outperforms leading baselines and naive entropy. AUROC (scored on \\nthe y-axes)\\xa0measures how well methods predict LLM mistakes, which correlate \\nwith confabulations. AURAC (likewise scored on the y -axes)\\xa0measures the \\nperformance improvement of a system that refuses to answer questions which \\nare judged likely to cause confabulations. Results are an average over five \\ndatasets, with individual metrics provided in the Supplementary Information.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 3, 'page_label': '628'}, page_content='628 | Nature | Vol 630 | 20 June 2024\\nArticle\\nacross the different model families (LLaMA, Falcon and Mistral) and \\nscales (from 7B to 70B parameters) which we study (we report summary \\nstatistics for each dataset and model as before). Although semantic \\nentropy outperforms the baselines across all model sizes, P (True) \\nseems to improve with model size, suggesting that it might become \\nmore competitive for very capable honest models in settings that the \\nmodel understands well (which are, however, not the most important \\ncases to have good uncertainty). We use ten generations to compute \\nentropy, selected using analysis in Supplementary Fig.\\xa02. Further \\nresults for short-phrase generations are described in Supplementary  \\nFigs.\\xa07–10.\\nThe results in Fig.\\xa02 offer a lower bound on the effectiveness of seman-\\ntic entropy at detecting confabulations. These evaluations determine \\nwhether semantic entropy and baseline methods can detect when the \\nanswers of the model are incorrect (which we validate against human \\ncorrectness evaluations in Supplementary Table\\xa04). In addition to errors \\nfrom confabulations (arbitrary incorrectness), this also includes other \\ntypes of mistakes for which semantic entropy is not suited, such as \\nconsistent errors learned from the training data. The fact that methods \\nsuch as embedding regression are able to spot other kinds of errors, not \\njust confabulations, but still are outperformed by semantic entropy, \\nsuggests that confabulations are a principal category of errors for \\nactual generations.\\nExamples of questions and answers from TriviaQA, SQuAD and \\nBioASQ, for LLaMA 2 Chat 70B, are shown in Table\\xa01. These illustrate \\nhow only semantic entropy detects when the meaning is constant \\nbut the form varies (the first row of the table) whereas semantic \\nentropy and naive entropy both correctly predict the presence of \\nconfabulations when the form and meaning vary together (second \\nrow) and predict the absence of confabulations when the form and \\nmeaning are both constant across several resampled generations \\n(third row). In the final row, we give an example in which semantic \\nentropy is erroneously high as a result of overly sensitive semantic \\nclustering relative to the reference answer. Our clustering method \\ndistinguishes the answers which provide a precise date from those \\nwhich only provide a year. For some contexts that would have been \\ncorrect but in this context the distinction between the specific day \\nand the year is probably irrelevant. This highlights the importance \\nof context and judgement in clustering, especially in subtle cases, as \\nwell as the shortcomings of evaluating against fixed reference answers \\nwhich do not capture the open-ended flexibility of conversational  \\ndeployments of LLMs.\\nDetecting confabulations in biographies\\nSemantic entropy is most natural for sentences that express a single \\nproposition but the idea of semantic equivalence is trickier to apply to \\nlonger passages which express many propositions which might only \\nagree partially43. Nevertheless, we can use semantic entropy to detect \\nconfabulations in longer generations, such as entire paragraphs of text. \\nT o show this, we develop a dataset of biographical generations from \\nGPT-4 (v.0613) for 21 individuals notable enough to have their own \\nWikipedia page but without extensive online biographies. From each \\nbiography generated by GPT-4, we automatically extract propositional \\nTable 1 | Semantic entropy applied to examples\\nQuestion Model answer Reference \\nanswer\\nGenerations for entropy Prediction\\nSemantic entropy Naive entropy\\nRefineries, process \\nchemical, power \\ngeneration, mills and \\nmanufacturing plants \\nare under what sector \\nof construction?\\nRefineries, process \\nchemical, power \\ngeneration, mills and \\nmanufacturing plants \\nare under the industrial \\nsector of construction.\\nIndustrial or heavy \\ncivil or heavy \\nengineering.\\n• All the above are under the industrial sector of \\nconstruction.\\nNot confabulation Confabulation\\n• The refineries, process chemical, power \\ngeneration, mills and manufacturing plants are \\nunder the industrial sector of construction.\\n• These are all under the heavy industrial sector \\nof construction.\\nWhat is the function of \\nthe stard10 protein?\\nThe stard10 protein \\nfunctions as a negative \\nregulator of the mTOR \\npathway by inhibiting \\nthe activity of the \\nmTORC1 and mTORC2 \\ncomplexes.\\nSTARD10 is a lipid \\ntransfer protein.\\n• The stard10 protein plays a crucial role \\nin regulating the dynamics of the actin \\ncytoskeleton and cell adhesion during cell \\nmigration and cell division.\\nConfabulation Confabulation\\n• The stard10 protein functions as a negative \\nregulator of the meiotic recombination process, \\nspecifically by inhibiting the activity of the Dmc1 \\nrecombinase enzyme.\\n• The stard10 protein functions as a regulator of \\nlipid metabolism, promoting the breakdown of \\nlipids in the liver and inhibiting their synthesis in \\nthe liver and adipose tissue.\\nOf which European \\ncountry is ‘fado’ the \\nnational music?\\nPortugal. Portugal.\\n• ‘Fado’ is the national music of Portugal.\\nNot confabulation Not confabulation• ‘Fado’ is the national music of Portugal.\\n• Fado is the national music of Portugal.\\nWhen was BSkyB’s \\ndigital service \\nlaunched?\\n1 October 1998.\\nBSkyB’s digital \\nservice was \\nlaunched in 1998.\\n• BSkyB’s digital service was launched in 1998.\\nConfabulation Not confabulation\\n• BSkyB’s digital service was launched on  \\n1 October 1998.\\n• BSkyB’s digital service was launched on  \\n1 October 1998.\\nThe first row of Table\\xa01 demonstrates a case in which semantic entropy correctly predicts that an answer is not a confabulation if naive entropy would incorrectly predict a confabulation. All of \\nthe generations from the model mean the same thing as each other so they are clustered together despite using different phrasings. The second row provides an example in which semantic \\nentropy and naive entropy would both correctly predict a confabulation, in which each generation is both lexically distinct and also means something different. The third row is an example in \\nwhich semantic entropy and naive entropy would both correctly predict no confabulation because the multiple generations are almost lexically identical. The fourth row gives an example  \\nin which semantic entropy might fail but naive entropy might succeed. In our experiment, semantic entropy clustered the answers into those which provided a specific date and those which \\ngave only a year and treated the model as ‘uncertain’. This highlights the importance of context in semantic clustering. The examples come from LLaMA 2 Chat 70B generations for SQuAD, \\nBioASQ and TriviaQA.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 4, 'page_label': '629'}, page_content='Nature | Vol 630 | 20 June 2024 | 629\\nfactual claims about the individual (150 factual claims in total), which \\nwe manually label as true or false.\\nApplying semantic entropy to this problem is challenging. Naively, \\none might simply regenerate each sentence (conditioned on the text \\nso far) and then compute semantic entropy over these regenerations. \\nHowever, the resampled sentences often target different aspects of \\nthe biography: for example, one time describing family and the next \\ntime profession. This is analogous to the original problem semantic \\nentropy was designed to resolve: the model is uncertain about the right \\nordering of facts, not about the facts themselves. T o address this, we \\nbreak down the entire paragraph into factual claims and reconstruct \\nquestions which might have been answered by those claims. Only then \\ndo we apply semantic entropy (Fig.\\xa01) by generating three new answers \\nto each question (selected with analysis in Supplementary Figs.\\xa03 and 4) \\nand computing the semantic entropy over those generations plus the \\noriginal factual claim. We aggregate these by averaging the semantic \\nentropy over all the questions to get an uncertainty score for each \\nproposition, which we use to detect confabulations. Unaggregated \\nresults are shown in Supplementary Figs.\\xa05 and 6.\\nAs GPT-4 did not allow access to the probability of the generation \\nat the time of writing, we use a discrete variant of semantic entropy \\nwhich makes the further approximation that we can infer a discrete \\nempirical distribution over semantic meaning clusters from only the \\ngenerations (Methods). This allows us to compute semantic entropy \\nusing only the black-box outputs of an LLM. However, we were unable \\nto compute the naive entropy baseline, the standard semantic entropy \\nestimator or the embedding regression baseline for GPT-4 without \\noutput probabilities and embeddings.\\nIn Fig.\\xa03 we show that the discrete variant of semantic entropy effec-\\ntively detects confabulations on this dataset. Its AUROC and AURAC are \\nhigher than either a simple ‘self-check’ baseline—which just asks the \\nLLM whether the factoid is likely to be true—or a variant of P(True) which \\nhas been adapted to work for the paragraph-length setting. Discrete \\nsemantic entropy has better rejection accuracy performance until \\n20% of the questions have been rejected at which point P(True) has \\na narrow edge. This indicates that the questions predicted to cause \\nconfabulations are indeed more likely to be wrong.\\nDiscussion\\nOur probabilistic approach, accounting for semantic equivalence, \\ndetects an important class of hallucinations: those that are caused by a \\nlack of LLM knowledge. These are a\\xa0substantial portion of the failures at \\npresent and will continue even as models grow in capabilities because \\nsituations and cases that humans cannot reliably supervise will persist. \\nConfabulations are a particularly noteworthy\\xa0failure mode for question \\nanswering but appear in other domains too. Semantic entropy needs \\nno previous domain knowledge and we expect that algorithmic adap-\\ntations to other problems will allow similar advances in, for example, \\nabstractive summarization. In addition, extensions to alternative input \\nvariations such as rephrasing or counterfactual scenarios would allow \\na similar method to act as a form of cross-examination44 for scalable \\noversight through debate45.\\nThe success of semantic entropy at detecting errors suggests that \\nLLMs are even better at “knowing what they don’t know” than was \\nargued by ref. 24—they just don’t know they know what they don’t \\nknow. Our method explicitly does not directly address situations in \\nwhich LLMs are confidently wrong because they have been trained \\nwith objectives that systematically produce dangerous behaviour, \\ncause systematic reasoning errors or are systematically mislead -\\ning the user. We believe that these represent different underlying \\nmechanisms—despite similar ‘symptoms’—and need to be handled  \\nseparately.\\nOne exciting aspect of our approach is the way it makes use of clas-\\nsical probabilistic machine learning methods and adapts them to the \\nunique properties of modern LLMs and free-form language generation. \\nWe hope to inspire a fruitful exchange of well-studied methods and \\nemerging new problems by highlighting the importance of meaning \\nwhen addressing language-based machine learning problems.\\nOnline content\\nAny methods, additional references, Nature Portfolio reporting summa-\\nries, source data, extended data, supplementary information, acknowl-\\nedgements, peer review information; details of author contributions \\nand competing interests; and statements of data and code availability \\nare available at https://doi.org/10.1038/s41586-024-07421-0.\\n1. GPT-4 technical report. Preprint at https://arxiv.org/abs/2303.08774 (2023).\\n2. Gemini: a family of highly capable multimodal models. Preprint at https://arxiv.org/abs/ \\n2312.11805 (2023).\\n3. Xiao, Y. & Wang, W. Y. On hallucination and predictive uncertainty in conditional language \\ngeneration. In Proc. 16th Conference of the European Chapter of the Association for \\nComputational Linguistics 2734–2744 (Association for Computational Linguistics, 2021).\\n4. Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T. & Saenko, K. Object hallucination in \\nimage captioning. In Proc. 2018 Conference on Empirical Methods in Natural Language \\nProcessing (eds Riloff, E., Chiang, D., Hockenmaier, J. & Tsujii, J.) 4035–4045 (Association \\nfor Computational Linguistics, 2018).\\n5. Weiser, B. Lawyer who used ChatGPT faces penalty for made up citations. The New York \\nTimes (8 Jun 2023).\\n6. Opdahl, A. L. et\\xa0al. Trustworthy journalism through AI. Data Knowl. Eng. 146, 102182 \\n(2023).\\n7. Shen, Y. et\\xa0al. ChatGPT and other large language models are double-edged swords. \\nRadiology 307, e230163 (2023).\\n8. Schulman, J. Reinforcement learning from human feedback: progress and challenges. \\nPresented at the Berkeley EECS Colloquium. YouTube www.youtube.com/\\nwatch?v=hhiLw5Q_UFg (2023).\\n9. Ji, Z. et\\xa0al. Survey of hallucination in natural language generation. ACM Comput. Surv.55, \\n248 (2023).\\n10. Maynez, J., Narayan, S., Bohnet, B. & McDonald, R. On faithfulness and factuality in \\nabstractive summarization. In Proc. 58th Annual Meeting of the Association for \\nComputational Linguistics (eds Jurafsky, D., Chai, J., Schluter, N. & Tetreault, J.) 1906–1919 \\n(Association for Computational Linguistics, 2020).\\n11. Filippova, K. Controlled hallucinations: learning to generate faithfully from noisy data.  \\nIn Findings of the Association for Computational Linguistics: EMNLP 2020 (eds Webber, \\nB., Cohn, T., He, Y. & Liu, Y.) 864–870 (Association for Computational Linguistics, 2020).\\n12. Berrios, G. Confabulations: a conceptual history. J. Hist. Neurosci. 7, 225–241 (1998).\\n13. Lin, S., Hilton, J. & Evans, O. Teaching models to express their uncertainty in words. \\nTransact. Mach. Learn. Res. (2022).\\n14. Evans, O. et\\xa0al. Truthful AI: developing and governing AI that does not lie. Preprint at \\nhttps://arxiv.org/abs/2110.06674 (2021).\\n15. Amodei, D. et\\xa0al. Concrete problems in AI safety. Preprint at https://arxiv.org/abs/ \\n1606.06565 (2016).\\n16. Jiang, Z., Araki, J., Ding, H. & Neubig, G. How can we know when language models know? \\nOn the calibration of language models for question answering. Transact. Assoc. Comput. \\nLinguist. 9, 962–977 (2021).\\nAUROC AURAC 80% 90% 95% 100%\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nRejection accuracy\\nDiscrete semantic entropy\\nP(True), ref. 24 variant\\nSelf-check baseline\\nFig. 3 | Detecting GPT-4\\xa0confabulations in paragraph-length biographies.  \\nThe discrete variant of our semantic entropy estimator outperforms baselines \\nboth when measured by AUROC and AURAC metrics\\xa0(scored on the y -axis). The \\nAUROC and AURAC are substantially higher than for both baselines. At above \\n80% of questions being answered, semantic entropy has the highest accuracy. \\nOnly when the top 20% of answers judged most likely to be confabulations are \\nrejected does the answer accuracy on the remainder for the P(True) baseline \\nexceed semantic entropy.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 5, 'page_label': '630'}, page_content='630 | Nature | Vol 630 | 20 June 2024\\nArticle\\n17. Desai, S. & Durrett, G. Calibration of pre-trained transformers. In Proc. 2020 Conference \\non Empirical Methods in Natural Language Processing (EMNLP) (eds Webber, B., Cohn, T., \\nHe, Y. & Liu, Y.) 295–302 (Association for Computational Linguistics, 2020).\\n18. Glushkova, T., Zerva, C., Rei, R. & Martins, A. F. Uncertainty-aware machine translation \\nevaluation. In Findings of the Association for Computational Linguistics: EMNLP 2021 (eds \\nMoens, M-F., Huang, X., Specia, L. & Yih, S.) 3920–3938 (Association for Computational \\nLinguistics, 2021).\\n19. Wang, Y., Beck, D., Baldwin, T. & Verspoor, K. Uncertainty estimation and reduction of \\npre-trained models for text regression. Transact. Assoc. Comput. Linguist. 10, 680–696 \\n(2022).\\n20. Baker, S. & Kanade, T. Hallucinating faces. In Proc. Fourth IEEE International Conference \\non Automatic Face and Gesture Recognition. 83–88 (IEEE, Catalogue no PR00580,  \\n2002).\\n21. Eliot, L. AI ethics lucidly questioning this whole hallucinating AI popularized trend that \\nhas got to stop. Forbes Magazine (24 August 2022).\\n22. Shanahan, M. Talking about large language models. Commun. Assoc. Comp. Machinery \\n67, 68–79 (2024).\\n23. MacKay, D. J. C. Information-based objective functions for active data selection. Neural \\nComput. 4, 590–604 (1992).\\n24. Kadavath, S. et\\xa0al. Language models (mostly) know what they know. Preprint at https://\\narxiv.org/abs/2207.05221 (2022).\\n25. Lindley, D. V. On a measure of the information provided by an experiment. Ann. Math. \\nStat. 27, 986–1005 (1956).\\n26. Xiao, T. Z., Gomez, A. N. & Gal, Y. Wat zei je? Detecting out-of-distribution translations with \\nvariational transformers. In Workshop on Bayesian Deep Learning at the Conference on \\nNeural Information Processing Systems (NeurIPS, Vancouver, 2019).\\n27. Christiano, P., Cotra, A. & Xu, M. Eliciting Latent Knowledge (Alignment Research  \\nCenter, 2021); https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_\\nEpsnjrC1dwZXR37PC8/edit.\\n28. Negri, M., Bentivogli, L., Mehdad, Y., Giampiccolo, D. & Marchetti, A. Divide and conquer: \\ncrowdsourcing the creation of cross-lingual textual entailment corpora. In Proc. 2011 \\nConference on Empirical Methods in Natural Language Processing 670–679 (Association \\nfor Computational Linguistics, 2011).\\n29. Honovich, O. et\\xa0al. TRUE: Re-evaluating factual consistency evaluation. In Proc. Second \\nDialDoc Workshop on Document-grounded Dialogue and Conversational Question \\nAnswering 161–175 (Association for Computational Linguistics, 2022).\\n30. Falke, T., Ribeiro, L. F. R., Utama, P. A., Dagan, I. & Gurevych, I. Ranking generated \\nsummaries by correctness: an interesting but challenging application for natural \\nlanguage inference. In Proc. 57th Annual Meeting of the Association for Computational \\nLinguistics 2214–2220 (Association for Computational Linguistics, 2019).\\n31. Laban, P., Schnabel, T., Bennett, P. N. & Hearst, M. A. SummaC: re-visiting NLI-based \\nmodels for inconsistency detection in summarization. Trans. Assoc. Comput. Linguist. 10, \\n163–177 (2022).\\n32. Joshi, M., Choi, E., Weld, D. S. & Zettlemoyer, L. TriviaQA: a large scale distantly supervised \\nchallenge dataset for reading comprehension. In Proc. 55th Annual Meeting of the \\nAssociation for Computational Linguistics 1601–1611 (Association for Computational \\nLinguistics. 2017).\\n33. Rajpurkar, P., Zhang, J., Lopyrev, K. & Liang, P. SQuAD: 100,000+ questions for machine \\ncompression of text. In Proc. 2016 Conference on Empirical Methods in Natural Language \\nProcessing (eds Su, J., Duh, K. & Carreras, X.) 2383–2392 (Association for Computational \\nLinguistics, 2016).\\n34. Tsatsaronis, G. et\\xa0al. An overview of the BIOASQ large-scale biomedical semantic \\nindexing and question answering competition. BMC Bioinformatics 16, 138 (2015).\\n35. Lee, K., Chang, M.-W. & Toutanova, K. Latent retrieval for weakly supervised open domain \\nquestion answering. In Proc. 57th Annual Meeting of the Association for Computational \\nLinguistics 6086–6096 (Association for Computational Linguistics, 2019).\\n36. Kwiatkowski, T. et\\xa0al. Natural questions: a benchmark for question answering research. \\nTransact. Assoc. Comput. Linguist. 7, 452–466 (2019).\\n37. Patel, A., Bhattamishra, S. & Goyal, N. Are NLP models really able to solve simple math \\nword problems? In Proc. 2021 Conference of the North American Chapter of the Association \\nfor Computational Linguistics: Human Language Technologies (eds Toutanova, K. et\\xa0al.) \\n2080–2094 (Assoc. Comp. Linguistics, 2021).\\n38. Touvron, H. et\\xa0al. Llama 2: open foundation and fine-tuned chat models. Preprint at \\nhttps://arxiv.org/abs/2307.09288 (2023).\\n39. Penedo, G. et\\xa0al. The RefinedWeb dataset for Falcon LLM: outperforming curated  \\ncorpora with web data, and web data only. In Proc. 36th Conference on Neural \\nInformation Processing Systems (eds Oh, A. et\\xa0al.) 79155–79172 (Curran Associates,  \\n2023)\\n40. Jiang, A. Q. et\\xa0al. Mistral 7B. Preprint at https://arxiv.org/abs/2310.06825 (2023).\\n41. Manakul, P., Liusie, A. & Gales, M. J. F. SelfCheckGPT: Zero-Resource Black-Box \\nhallucination detection for generative large language models. In Findings of the \\nAssociation for Computational Linguistics: EMNLP 2023 (eds Bouamor, H., Pino, J. & Bali, K.) \\n9004–9017 (Assoc. Comp. Linguistics, 2023).\\n42. Mukhoti, J., Kirsch, A., van Amersfoort, J., Torr, P. H. & Gal, Y. Deep deterministic \\nuncertainty: a new simple baseline. In IEEE/CVF Conference on Computer Vision and \\nPattern Recognition 24384–24394 (Computer Vision Foundation, 2023).\\n43. Schuster, T., Chen, S., Buthpitiya, S., Fabrikant, A. & Metzler, D. Stretching sentence-pair \\nNLI models to reason over long documents and clusters. In Findings of the Association for \\nComputational Linguistics: EMNLP 2022 (eds Goldberg, Y. et\\xa0al.) 394–412 (Association for \\nComputational Linguistics, 2022).\\n44. Barnes, B. & Christiano, P. Progress on AI Safety via Debate. AI Alignment Forum  \\nwww.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via- \\ndebate-1 (2020).\\n45. Irving, G., Christiano, P. & Amodei, D. AI safety via debate. Preprint at https://arxiv.org/\\nabs/1805.00899 (2018).\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution \\n4.0 International License, which permits use, sharing, adaptation, distribution \\nand reproduction in any medium or format, as long as you give appropriate \\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \\nand indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons licence and your \\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \\nvisit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2024'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 6, 'page_label': '631'}, page_content='Methods\\nSemantic entropy as a strategy for overcoming confabulation builds \\non probabilistic tools for uncertainty estimation. It can be applied \\ndirectly to any LLM or similar foundation model without requiring any \\nmodifications to the architecture. Our ‘discrete’ variant of semantic \\nuncertainty can be applied even when the predicted probabilities for \\nthe generations are not available, for example, because access to the \\ninternals of the model is limited.\\nIn this section we introduce background on probabilistic methods \\nand uncertainty in machine learning, discuss how it applies to language \\nmodels and then discuss our contribution, semantic entropy, in detail.\\nBackground\\nUncertainty and machine learning. We aim to detect confabulations \\nin LLMs, using the principle that the model will be uncertain about \\ngenerations for which its output is going to be arbitrary.\\nOne measure of uncertainty is the predictive entropy of the output \\ndistribution, which measures the information one has about the output \\ngiven the input25. The predictive entropy (PE) for an input sentence x \\nis the conditional entropy\\xa0(H) of the output random variable Y with \\nrealization y given x,\\n∣∣ ∣∑HY Py PyPE() =( )= −( )ln( ). (1)\\ny\\nxx xx\\nA low predictive entropy indicates an output distribution which is \\nheavily concentrated whereas a high predictive entropy indicates that \\nmany possible outputs are similarly likely.\\nAleatoric and epistemic uncertainty. We do not distinguish between \\naleatoric and epistemic uncertainty in our analysis. Researchers some-\\ntimes separate aleatoric uncertainty (uncertainty in the underlying \\ndata distribution) from epistemic uncertainty (caused by having only \\nlimited information)46. Further advances in uncertainty estimation \\nwhich separate these kinds of uncertainty would enhance the potential \\nfor our semantic uncertainty approach by allowing extensions beyond \\nentropy.\\nJoint probabilities of sequences of tokens. Generative LLMs produce \\nstrings of text by selecting tokens in sequence. Each token is a wordpiece \\nthat often represents three or four characters (though especially com-\\nmon sequences and important words such as numbers typically get \\ntheir own token). T o compute entropies, we need access to the prob-\\nabilities the LLM assigns to the generated sequence of tokens. The \\nprobability of the entire sequence, s, conditioned on the context, x, is \\nthe product of the conditional probabilities of new tokens given past \\ntokens, whose resulting log-probability is xxss ∑PP slog () =l og (, )i ii <∣∣ , \\nwhere si is the i th output token and s <i denotes the set of previous  \\ntokens.\\nLength normalization. When comparing the log-probabilities of gen-\\nerated sequences, we use ‘length normalization’ , that is, we use an \\narithmetic mean log-probability, xsPs∑l og (, )N i\\nN\\nii\\n1\\n<∣ , instead of the \\nsum. In expectation, longer sequences have lower joint likelihoods \\nbecause of the conditional independence of the token probabilities47. \\nThe joint likelihood of a sequence of length N shrinks exponentially in \\nN. Its negative log-probability therefore grows linearly in N, so longer \\nsentences tend to contribute more to entropy. We therefore interpret \\nlength-normalizing the log-probabilities when estimating the entropy \\nas asserting that the expected uncertainty of generations is independ-\\nent of sentence length. Length normalization has some empirical  \\nsuccess48, including in our own preliminary experiments, but little \\ntheoretical justification in the literature.\\nPrinciples of semantic uncertainty\\nIf we naively calculate the predictive entropy directly from the probabil-\\nities of the generated sequence of tokens, we conflate the uncertainty \\nof the model over the meaning of its answer with the uncertainty over \\nthe exact tokens used to express that meaning. For example, even if the \\nmodel is confident in the meaning of a generation, there are still usually \\nmany different ways for phrasing that generation without changing its \\nmeaning. For the purposes of detecting confabulations, the uncertainty \\nof\\xa0the LLM over meanings is more important than the uncertainty over \\nthe exact tokens used to express those meanings.\\nOur semantic uncertainty method therefore seeks to estimate only \\nthe uncertainty the LLM has over the meaning of its generation, not \\nthe choice of words. T o do this, we introduce an algorithm that clusters \\nmodel generations by meaning and subsequently calculates semantic \\nuncertainty. At a high level this involves three steps:\\n1. Generation: sample output sequences of tokens from the predictive \\ndistribution of a LLM given a context x.\\n2. Clustering: cluster sequences by their meaning using our clustering \\nalgorithm based on bidirectional entailment.\\n3. Entropy estimation: estimate semantic entropy by summing prob-\\nabilities of sequences that share a meaning following equation\\xa0(2) \\nand compute their entropy.\\nGenerating a set of answers from the model. Given some context x \\nas input to the LLM, we sample M sequences, {s(1),\\u2009…,\\u2009s(M)} and record \\ntheir token probabilities, {P(s(1)∣x),\\u2009…,\\u2009P(s(M)∣x)}. We sample all our gen-\\nerations from a single model, varying only the random seed used for \\nsampling from the token probabilities. We do not observe the method \\nto be particularly sensitive to details of the sampling scheme. In our \\nimplementation, we sample at temperature 1 using nucleus sampling \\n(P\\u2009=\\u20090.9) (ref. 49) and top-K sampling (K\\u2009=\\u200950) (ref. 50). We also sample \\na single generation at low temperature (0.1) as an estimate of the ‘best \\ngeneration’ of the model to the context, which we use to assess the \\naccuracy of the model. (A lower sampling temperature increases the \\nprobability of sampling the most likely tokens).\\nClustering by semantic equivalence. T o estimate semantic entropy \\nwe need to cluster generated outputs from the model into groups of \\noutputs that mean the same thing as each other.\\nThis can be described using ‘semantic equivalence’ which is the rela-\\ntion that holds between two sentences when they mean the same thing. \\nWe can formalize semantic equivalence mathematically. Let the space \\nof tokens in a language be T . The space of all possible sequences of \\ntokens of length N is then S T≡N\\nN. Note that N can be made arbitrarily \\nlarge to accommodate whatever size of sentence one can imagine and \\none of the tokens can be a ‘padding’ token which occurs with certainty \\nfor each token after the end-of-sequence token. For some sentence \\n∈ Ns S , composed of a sequence of tokens, s ∈i T , there is an associ -\\nated meaning. Theories of meaning are contested 51. However, for  \\nspecific models and deployment contexts many considerations can  \\nbe set aside. Care should be taken comparing very different models  \\nand contexts.\\nLet us introduce a semantic equivalence relation, E(\\u2009⋅\\u2009,\\u2009⋅\\u2009), which holds \\nfor any two sentences that mean the same thing—we will operational-\\nize this presently. Recall that an equivalence relation is any reflexive, \\nsymmetric and transitive relation and that any equivalence relation on \\na set corresponds to a set of equivalence classes. Each semantic equiv-\\nalence class captures outputs that can be considered to express the \\nsame meaning. That is, for the space of semantic equivalence classes \\nC the sentences in the set Cc ∈  can be regarded in many settings as \\nexpressing a similar meaning such that ss sscE∀ ,/uni2032∈: (, /uni2032). So we can \\nbuild up these classes of semantically equivalent sentences by check-\\ning if new sentences share a meaning with any sentences we have already \\nclustered and, if so, adding them into that class.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 7, 'page_label': '632'}, page_content='Article\\nWe operationalize E(\\u2009⋅\\u2009,\\u2009⋅\\u2009) using the idea of bidirectional entailment, \\nwhich has a long history in linguistics52 and natural language process-\\ning28,53,54. A sequence, s, means the same thing as a second sequence, \\ns′, only if the sequences entail (that is, logically imply) each other. For \\nexample, ‘The capital of France is Paris’ entails ‘Paris is the capital of \\nFrance’ and vice versa because they mean the same thing. (See later \\nfor a discussion of soft equivalence and cases in which bidirectional \\nentailment does not guarantee equivalent meanings).\\nImportantly, we require that the sequences mean the same thing \\nwith respect to the context—key meaning is sometimes contained in \\nthe context. For example, ‘Paris’ does not entail ‘The capital of France \\nis Paris’ because ‘Paris’ is not a declarative sentence without context. \\nBut in the context of the question ‘What is the capital of France?’ , the \\none-word answer does entail the longer answer.\\nDetecting entailment has been the object of study of a great deal of \\nresearch in NLI55. We rely on language models to predict entailment, \\nsuch as DeBERTa-Large-MNLI56, which has been trained to predict entail-\\nment, or general-purpose LLMs such as GPT-3.5 (ref. 57), which can \\npredict entailment given suitable prompts.\\nWe then cluster sentences according to whether they bidirection-\\nally entail each other using the algorithm presented in Extended Data \\nFig.\\xa01. Note that, to check if a sequence should be added to an existing \\ncluster, it is sufficient to check if the sequence bidirectionally entails \\nany of the existing sequences in that cluster (we arbitrarily pick the first \\none), given the transitivity of semantic equivalence. If a sequence does \\nnot share meaning with any existing cluster, we assign it its own cluster.\\nComputing the semantic entropy. Having determined the classes of \\ngenerated sequences that mean the same thing, we can estimate the \\nlikelihood that a sequence generated by the LLM belongs to a given class \\nby computing the sum of the probabilities of all the possible sequences \\nof tokens which can be considered to express the same meaning as\\nxx xss\\nss\\n∣ ∑∑ ∏Pc PP s() =( )= (, ). (2)\\ncc i\\nii\\n∈∈\\n<\\nFormally, this treats the output as a random variable whose event- \\nspace is the space of all possible meaning-classes, C, a sub-σ-algebra of \\nthe standard event-space S. We can then estimate the semantic entropy \\n(SE) as the entropy over the meaning-distribution,\\nxx∣∣∑xP cP cSE() =− () log( ) (3)\\nc\\n\\uf8eb\\n\\uf8ed\\n\\uf8ec\\uf8ec\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8ee\\n\\uf8f0\\n\\uf8ef\\n\\uf8f9\\n\\uf8fb\\n\\uf8fa\\n\\uf8f6\\n\\uf8f8\\n\\uf8f7\\uf8f7xxss\\nss\\n∣∣∑∑ ∑PP=− () log( ). (4)\\ncc c∈∈\\nThere is a complication which prevents direct computation: we do \\nnot have access to every possible meaning-class c. Instead, we can only \\nsample c from the sequence-generating distribution induced by the \\nmodel. T o handle this, we estimate the expectation in equation\\xa0(3) \\nusing a Rao–Blackwellized Monte Carlo integration over the semantic \\nequivalence classes C,\\nxx∑xP CP CSE() ≈− () log( ), (5)\\ni\\nC\\nii\\n=1\\n∣∣\\n∣∣\\nwhere ∣\\n∣\\n∣PC() = ∑i\\nPc\\nPc\\n()\\n()\\ni\\nc\\nx\\nx\\nx  estimates a categorical distribution over the \\ncluster meanings, that is, ∑iP(Ci∣x)\\u2009=\\u20091. Without this normalization step \\ncluster ‘probabilities’ could exceed one because of length normaliza-\\ntion, resulting in degeneracies. Equation\\xa0(5) is the estimator giving our \\nmain method that we refer to as semantic entropy throughout the text.\\nFor scenarios in which the sequence probabilities are not available, \\nwe propose a variant of semantic entropy which we call ‘discrete’ seman-\\ntic entropy. Discrete semantic entropy approximates P(Ci∣x) directly \\nfrom the number of generations in each cluster, disregarding the token \\nprobabilities. That is, we approximate P(Ci∣x) as ∑ M I\\nM1\\ncC i=\\n, the proportion \\nof all the sampled answers which belong to that cluster. Effectively, \\nthis just assumes that each output that was actually generated was \\nequally probable—estimating the underlying distribution as the cat-\\negorical empirical distribution. In the limit of M the estimator converges \\nto equation\\xa0(5) by the law of large numbers. We find that discrete seman-\\ntic entropy results in similar performance empirically.\\nWe provide a worked example of the computation of semantic \\nentropy in Supplementary Note\\xa01.\\nDetecting confabulations in QA and math\\nSemantic entropy is designed to detect confabulations, that is, model \\noutputs with arbitrary meaning. In our experiments, we use semantic \\nuncertainty to predict model accuracy, demonstrating that confabula-\\ntions make up a notable fraction of model mistakes. We further show \\nthat semantic uncertainty can be used to improve model accuracy by \\nrefusing to answer questions when semantic uncertainty is high. Last, \\nsemantic uncertainty can be used to give users a way to know when \\nmodel generations are probably unreliable.\\nTasks. We use the datasets BioASQ34, SQuAD33, TriviaQA32, SVAMP37 \\nand NQ-Open35. BioASQ is a life-sciences question-answering dataset \\nbased on the annual challenge of the same name. The specific dataset \\nwe use is based on the QA dataset from Task B of the 2023 BioASQ \\nchallenge (11B). SQuAD is a reading comprehension dataset whose \\ncontext passages are drawn from Wikipedia and for which the answers \\nto questions can be found in these passages. We use SQuAD 1.1 which  \\nexcludes the unanswerable questions added in v.2.0 that are deliberate-\\nly constructed to induce mistakes so they do not in practice cause con-\\nfabulations to occur. TriviaQA is a trivia question-answering dataset. \\nSVAMP is a word-problem maths dataset containing elementary-school \\nmathematical reasoning tasks. NQ-Open is a dataset of realistic ques-\\ntions aggregated from Google Search which have been chosen to be \\nanswerable without reference to a source text. For each dataset, we \\nuse 400 train examples and 400 test examples randomly sampled \\nfrom the original larger dataset. Note that only some of the methods \\nrequire training, for example semantic entropy does not use the train-\\ning data. If the datasets themselves are already split into train and \\ntest (or validation) samples, we sample our examples from within the \\ncorresponding split.\\nAll these datasets are free-form, rather than multiple choice, because \\nthis better captures the opportunities created by LLMs to produce \\nfree-form sentences as answers. We refer to this default scenario as \\nour ‘sentence-length’ experiments. In Supplementary Note\\xa07, we also \\npresent results for confabulation detection in a ‘short-phrase’ scenario, \\nin which we constrain model answers on these datasets to be as concise \\nas possible.\\nT o make the problems more difficult and induce confabulations, \\nwe do not provide the context passages for any of the datasets. When \\nthe context passages are provided, the accuracy rate is too high for \\nthese datasets for the latest generations of models to meaningfully \\nstudy confabulations.\\nModels. For sentence-length generations we use: Falcon39 Instruct (7B \\nand 40B), LLaMA 2 Chat38 (7B, 13B and 70B) and Mistral40 Instruct (7B).\\nBaselines. In addition to reporting results for semantic entropy, dis-\\ncrete semantic entropy and naive entropy, we consider two strong \\nbaselines.\\nEmbedding regression is a supervised baseline inspired by the P(IK) \\nmethod24. In that paper, the authors fine-tune their proprietary LLM on \\na dataset of questions to predict whether the model would have been \\ncorrect. This requires access to a dataset of ground-truth answers to the \\nquestions. Rather than fine-tuning the entire LLM in this way, we simply'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 8, 'page_label': '633'}, page_content='take the final hidden units and train a logistic regression classifier to \\nmake the same prediction. By contrast to their method, this is much \\nsimpler because it does not require fine-tuning the entire language \\nmodel, as well as being more reproducible because the solution to the \\nlogistic regression optimization problem is not as seed-dependent as \\nthe fine-tuning procedure. As expected, this supervised approach per-\\nforms well in-distribution but fails when the distribution of questions \\nis different from that on which the classifier is trained.\\nThe second baseline we consider is the P(True) method24, in which \\nthe model first samples M answers (identically to our semantic entropy \\napproach) and then is prompted with the list of all answers generated \\nfollowed by the highest probability answer and a question whether \\nthis answer is “(a) True” or “(b) False” . The confidence score is then \\ntaken to be the probability with which the LLM responds with ‘a’ to the \\nmultiple-choice question. The performance of this method is boosted \\nwith a few-shot prompt, in which up to 20 examples from the training \\nset are randomly chosen, filled in as above, but then provided with the \\nactual ground truth of whether the proposed answer was true or false. \\nIn this way, the method can be considered as supervised ‘in-context’ \\nbecause it makes use of some ground-truth training labels but can \\nbe used without retraining the model. Because of context-size con-\\nstraints, this method cannot fit a full 20 few-shot examples in the \\ncontext when input questions are long or large numbers of genera-\\ntions are used. As a result, we sometimes have to reduce the number \\nof few-shot examples to suit the context size and we note this\\xa0in the\\xa0 \\nSupplementary Material.\\nEntailment estimator. Any NLI classification system could be used for \\nour bidirectional entailment clustering algorithm. We consider two \\ndifferent kinds of entailment detector.\\nOne option is to use an instruction-tuned LLM such as LLaMA 2, GPT-\\n3.5 (Turbo 1106) or GPT-4 to predict entailment between generations. \\nWe use the following prompt:\\nWe are evaluating answers to the question {question}\\nHere are two possible answers:\\nPossible Answer 1: {text1}\\nPossible Answer 2: {text2}\\nDoes Possible Answer 1 semantically entail Possible Answer 2? \\nRespond with entailment, contradiction, or neutral.\\nAlternatively, we consider using a language model trained for entail-\\nment prediction, specifically the DeBERTa-large model56 fine-tuned on \\nthe NLI dataset MNLI58. This builds on past work towards paraphrase \\nidentification based on embedding similarity 59,60 and BERT-style \\nmodels61,62. We template more simply, checking if DeBERTa predicts \\nentailment between the concatenation of the question and one \\nanswer and the concatenation of the question and another answer. \\nNote that DeBERTa-large is a relatively lightweight model with only \\n1.5B parameters which is much less powerful than most of the LLMs  \\nunder study.\\nIn Supplementary Note 2, we carefully evaluate the benefits and \\ndrawbacks of these methods for entailment prediction. We settle on \\nusing GPT-3.5 with the above prompt, as its entailment predictions \\nagree well with human raters and lead to good confabulation detec-\\ntion performance.\\nIn Supplementary Note\\xa03, we provide a discussion of the compu-\\ntational cost and choosing the number of generations for reliable \\nclustering.\\nPrompting templates. We use a simple generation template for all \\nsentence-length answer datasets:\\n Answer the following question in a single brief but complete  \\nsentence.\\nQuestion: {question}\\nAnswer:\\nMetrics and accuracy measurements. We use three main metrics to \\nevaluate our method: AUROC, rejection accuracy and AURAC. Each of \\nthese is grounded in an automated factuality estimation measurement \\nrelative to the reference answers provided by the datasets that we use.\\nAUROC, rejection accuracy and AURAC. First, we use the AUROC \\ncurve, which measures the reliability of a classifier accounting for both \\nprecision and recall. The AUROC can be interpreted as the probability \\nthat a randomly chosen correct answer has been assigned a higher con-\\nfidence score than a randomly chosen incorrect answer. For a perfect \\nclassifier, this is 1.\\nSecond, we compute the ‘rejection accuracy at X %’ , which is the \\nquestion-answering accuracy of the model on the most-confident \\nX% of the inputs as identified by the respective uncertainty method. \\nIf an uncertainty method works well, predictions on the confident \\nsubset should be more accurate than predictions on the excluded \\nsubset and the rejection accuracy should increase as we reject  \\nmore inputs.\\nT o summarize this statistic we compute the AURAC—the total \\narea enclosed by the accuracies at all cut-off percentages X %. This \\nshould increase towards 1 as given uncertainty method becomes \\nmore accurate and better at detecting likely-inaccurate responses \\nbut it is more sensitive to the overall accuracy of the model than the  \\nAUROC metric.\\nIn Supplementary Note\\xa05, we provide the unaggregated rejection \\naccuracies for sentence-length generations.\\nAssessing accuracy. For the short-phrase-length generation setting \\npresented in Supplementary Note\\xa07, we simply assess the accuracy \\nof the generations by checking if the F1 score of the commonly used \\nSQuAD metric exceeds 0.5. There are limitations to such simple scor-\\ning rules63 but this method is widely used in practice and its error is \\ncomparatively small on these standard datasets.\\nFor our default scenario, the longer sentence-length generations, \\nthis measure fails, as the overlap between the short reference answer \\nand our long model answer is invariably too small. For sentence-length \\ngenerations, we therefore automatically determine whether an answer \\nto the question is correct or incorrect by using GPT-4 to compare the \\ngiven answer to the reference answer. We use the template:\\n We are assessing the quality of answers to the following question: \\n{question}\\nThe expected answer is: {reference answer}\\nThe proposed answer is: {predicted answer}\\nWithin the context of the question, does the proposed answer mean \\nthe same as the expected answer? Respond only with yes or no.\\nWe make a small modification for datasets with several reference \\nanswers: line two becomes “The following are expected answers to this \\nquestion:” and the final line asks “does the proposed answer mean the \\nsame as any of the expected answers?” .\\nIn Supplementary Note 6, we check the quality of our automated \\nground-truth evaluations against human judgement by hand. We find \\nthat GPT-4 gives the best results for determining model accuracy and \\nthus use it in all our sentence-length experiments.\\nDetecting confabulations in biographies\\nIn this section we describe the application of semantic entropy to \\nconfabulation detection in longer model generations, specifically \\nparagraph-length biographies.\\nWe introduce a biography-generation dataset—FactualBio—  \\navailable alongside this paper. FactualBio is a collection of biographies \\nof individuals who are notable enough to have Wikipedia pages but not \\nnotable enough to have large amounts of detailed coverage, generated \\nby GPT-4 (v.0613). T o generate the dataset, we randomly sampled 21 \\nindividuals from the WikiBio dataset64. For each biography, we gener-\\nated a list of factual claims contained in each biography using GPT-4, \\nwith 150 total factual claims (the total number is only coincidentally a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 9, 'page_label': '634'}, page_content='Article\\nround number). For each of these factual claims, we manually deter-\\nmined whether the claim was correct or incorrect. Out of 150 claims, 45 \\nwere incorrect. As before, we apply confabulation detection to detect \\nincorrect model predictions, even though there may be model errors \\nwhich are not confabulations.\\nPrompting and generation. Given a paragraph-length piece of LLM- \\ngenerated text, we apply the following sequence of steps:\\n1. Automatically decompose the paragraph into specific factual claims \\nusing an LLM (not necessarily the same as the original).\\n2. For each factual claim, use an LLM to automatically construct Q ques-\\ntions which might have produced that claim.\\n3. For each question, prompt the original LLM to generate M answers.\\n4. For each question, compute the semantic entropy of the answers, \\nincluding the original factual claim.\\n5. Average the semantic entropies over the questions to arrive at a score \\nfor the original factual claim.\\nWe pursue this slightly indirect way of generating answers because \\nwe find that simply resampling each sentence creates variation unre-\\nlated to the uncertainty of the model about the factual claim, such as \\ndifferences in paragraph structure.\\nWe decompose the paragraph into factual claims using the follow-\\ning prompt:\\n Please list the specific factual propositions included in the answer \\nabove. Be complete and do not leave any factual claims out. Provide \\neach claim as a separate sentence in a separate bullet point.\\nWe found that we agreed with the decompositions in all cases in \\nthe dataset.\\nWe then generate six questions for each of the facts from the decom-\\nposition. We generate these questions by prompting the model twice \\nwith the following:\\nFollowing this text:\\n{text so far}\\nYou see the sentence:\\n{proposition}\\nGenerate a list of three questions, that might have generated the \\nsentence in the context of the preceding original text, as well as their \\nanswers. Please do not use specific facts that appear in the follow-up \\nsentence when formulating the question. Make the questions and \\nanswers diverse. Avoid yes-no questions. The answers should not be a \\nfull sentence and as short as possible, e.g. only a name, place, or thing. \\nUse the format “1. {question} – {answer}” .\\nThese questions are not necessarily well-targeted and the difficulty \\nof this step is the main source of errors in the procedure. We gener-\\nate three questions with each prompt, as this encourages diversity of \\nthe questions, each question targeting a different aspect of the fact. \\nHowever, we observed that the generated questions will sometimes \\nmiss obvious aspects of the fact. Executing the above prompt twice \\n(for a total of six questions) can improve coverage. We also ask for \\nbrief answers because the current version of GPT-4 tends to give long, \\nconvoluted and highly hedged answers unless explicitly told not to.\\n Then, for each question, we generate three new answers using the \\nfollowing prompt:\\n We are writing an answer to the question “{user question}” . So far \\nwe have written:\\n{text so far}\\nThe next sentence should be the answer to the following question:\\n{question}\\n Please answer this question. Do not answer in a full sentence. Answer \\nwith as few words as possible, e.g. only a name, place, or thing.\\nWe then compute the semantic entropy over these answers plus \\nthe original factual claim. Including the original fact ensures that the \\nestimator remains grounded in the original claim and helps detect \\nsituations in which the question has been interpreted completely dif-\\nferently from the original context. We make a small modification to \\nhandle the fact that GPT-4 generations often include refusals to answer \\nquestions. These refusals were not something we commonly observe \\nin our experiments with LLaMA 2, Falcon or Mistral models. If more \\nthan half of the answers include one of the strings ‘not available’ , ‘not \\nprovided’ , ‘unknown’ or ‘unclear’ then we treat the semantic uncertainty \\nas maximal.\\nWe then average the semantic entropies for each question corre-\\nsponding to the factual claim to get an entropy for this factual claim.\\nDespite the extra assumptions and complexity, we find that this \\nmethod greatly outperforms the baselines.\\nEntailment estimator. T o compute semantic entailment between the \\noriginal claim and regenerated answers, we rely on the DeBERTa entail-\\nment prediction model as we find empirically that DeBERTa predictions \\nresult in higher train-set AUROC than other methods. Because DeBERTa \\nhas slightly lower recall than GPT-3.5/4, we use a modified set-up for \\nwhich we say the answers mean the same as each other if at least one \\nof them entails the other and neither is seen to contradict the other—\\na kind of ‘non-defeating’ bidirectional entailment check rather than \\ntrue bidirectional entailment. The good performance of DeBERTa in \\nthis scenario is not surprising as both factual claims and regenerated \\nanswers are relatively short. We refer to Supplementary Notes 2 and 3  \\nfor ablations and experiments regarding our choice of entailment \\nestimator for paragraph-length generations.\\nBaselines. We implement two baselines. First, we implement a variant \\nof the P(True) method, which is adapted to the new setting. For each \\nfactoid, we generate a question with answers in the same way as for \\nsemantic entropy. We then use the following prompt:\\nQuestion: {question}\\nHere are some brainstormed ideas:\\n{list of regenerated answers}\\nPossible answer: {original answer}\\nIs the possible answer true? Respond with “yes” or “no” .\\nAs we cannot access the probabilities GPT-4 assigns to predicting \\n‘yes’ and ‘no’ as the next token, we approximate this using Monte Carlo  \\nsamples. Concretely, we execute the above prompt ten times (at tem-\\nperature 1) and then take the fraction of answers which was ‘yes’ as our \\nunbiased Monte Carlo estimate of the token probability GPT-4 assigns \\nto ‘yes’ .\\nAs a second, simpler, baseline we check if the model thinks the answer \\nis true. We simply ask:\\nFollowing this text:\\n{text so far}\\nYou see this statement:\\n{proposition}\\nIs it likely that the statement is true? Respond with ‘yes’ or ‘no’ .\\nIt is interesting that this method ought to perform very well if we think \\nthat the model has good ‘self-knowledge’ (that is, if “models mostly \\nknow what they don’t know”24) but in fact semantic entropy is much \\nbetter at detecting confabulations.\\nData availability\\nThe data used for the short-phrase and sentence-length generations \\nare publicly available and the released code details how to access it. We \\nrelease a public version of the FactualBio dataset as part of the code \\nbase for reproducing the paragraph-length experiments.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 10, 'page_label': '635'}, page_content='Code availability\\nWe release all code used to produce the main experiments. The code \\nfor short-phrase and sentence-length experiments can be found at \\ngithub.com/jlko/semantic_uncertainty and https://doi.org/10.5281/\\nzenodo.10964366 (ref. 65). The code for paragraph-length experiments \\ncan be found at github.com/jlko/long_hallucinations and https://doi.\\norg/10.5281/zenodo.10964366 (ref. 65).\\n \\n46. Der Kiureghian, A. & Ditlevsen, O. Aleatory or epistemic? Does it matter? Struct. Saf. 31, \\n105–112 (2009).\\n47. Malinin, A. & Gales, M. Uncertainty estimation in autoregressive structured prediction.  \\nIn Proceedings of the International Conference on Learning Representations https://\\nopenreview.net/forum?id=jN5y-zb5Q7m (2021).\\n48. Murray, K. & Chiang, D. Correcting length bias in neural machine translation. In Proc.  \\nThird Conference on Machine Translation (eds Bojar, O. et\\xa0al.) 212–223 (Assoc. Comp. \\nLinguistics, 2018).\\n49. Holtzman, A., Buys, J., Du, L., Forbes, M. & Choi, Y. The curious case of neural text \\ndegeneration. In Proceedings of the International Conference on Learning Representations \\nhttps://openreview.net/forum?id=rygGQyrFvH (2020).\\n50. Fan, A., Lewis, M. & Dauphin, Y. Hierarchical neural story generation. In Proc. 56th Annual \\nMeeting of the Association for Computational Linguistics (eds Gurevych, I. & Miyao, Y.) \\n889–898 (Association for Computational Linguistics, 2018).\\n51. Speaks, J. in The Stanford Encyclopedia of Philosophy (ed. Zalta, E. N.) (Metaphysics \\nResearch Lab, Stanford Univ., 2021).\\n52. Culicover, P. W. Paraphrase generation and information retrieval from stored text. Mech. \\nTransl. Comput. Linguist. 11, 78–88 (1968).\\n53. Padó, S., Cer, D., Galley, M., Jurafsky, D. & Manning, C. D. Measuring machine translation \\nquality as semantic equivalence: a metric based on entailment features. Mach. Transl. 23, \\n181–193 (2009).\\n54. Androutsopoulos, I. & Malakasiotis, P. A survey of paraphrasing and textual entailment \\nmethods. J. Artif. Intell. Res. 38, 135–187 (2010).\\n55. MacCartney, B. Natural Language Inference (Stanford Univ., 2009).\\n56. He, P., Liu, X., Gao, J. & Chen, W. Deberta: decoding-enhanced BERT with disentangled \\nattention. In International Conference on Learning Representations https://openreview.\\nnet/forum?id=XPZIaotutsD (2021).\\n57. Brown, T. et\\xa0al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, \\n1877–1901 (2020).\\n58. Williams, A., Nangia, N. & Bowman, S. R. A broad-coverage challenge corpus for sentence \\nunderstanding through inference. In Proc. 2018 Conference of the North American \\nChapter of the Association for Computational Linguistics: Human Language Technologies \\n(eds Walker, M. et\\xa0al.) 1112–1122 (Assoc. Comp. Linguistics, 2018).\\n59. Yu, L., Hermann, K. M., Blunsom, P. & Pulman, S. Deep learning for answer sentence \\nselection. Preprint at https://arxiv.org/abs/1412.1632 (2014).\\n60. Socher, R., Huang, E., Pennin, J., Manning, C. D. & Ng, A. Dynamic pooling and unfolding \\nrecursive autoencoders for paraphrase detection. In Proceedings of the 24th Conference \\non Neural Information Processing Systems (eds Shawe-Taylor, J. et\\xa0al.) (2011)\\n61. He, R., Ravula, A., Kanagal, B. & Ainslie, J. Realformer: Transformer likes residual attention. \\nIn Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021  \\n(eds Zhong, C., et\\xa0al.) 929–943 (Assoc. Comp. Linguistics, 2021).\\n62. Tay, Y. et\\xa0al. Charformer: fast character transformers via gradient-based subword \\ntokenization. In Proceedings of the International Conference on Learning Representations \\nhttps://openreview.net/forum?id=JtBRnrlOEFN (2022).\\n63. Kane, H., Kocyigit, Y., Abdalla, A., Ajanoh, P. & Coulibali, M. Towards neural similarity \\nevaluators. In Workshop on Document Intelligence at the 32nd conference on Neural \\nInformation Processing (2019).\\n64. Lebret, R., Grangier, D. & Auli, M. Neural text generation from structured data with \\napplication to the biography domain. In Proc. 2016 Conference on Empirical Methods in \\nNatural Language Processing (eds Su, J. et\\xa0al.) 1203–1213 (Association for Computational \\nLinguistics, 2016).\\n65. Kossen, J., jlko/semantic_uncertainty: Initial release v.1.0.0. Zenodo https://doi.org/ \\n10.5281/zenodo.10964366 (2024).\\nAcknowledgements We thank G. Irving, K. Perlin, J. Richens, L. Rimell and M. Turpin for their \\ncomments or discussion related to this work. We thank K. Handa for his help with the human \\nevaluation of our automated accuracy assessment. We thank F. Bickford Smith and L. Melo for \\ntheir code review.\\xa0Y.G.\\xa0is supported by a Turing AI Fellowship funded by the UK government’s \\nOffice for AI, through UK Research and Innovation (grant reference EP/V030302/1), and \\ndelivered by the Alan Turing Institute.\\nAuthor contributions S.F. led the work from conception to completion and proposed using \\nbidirectional entailment to cluster generations as a way of computing entropy in LLMs.  \\nHe wrote the main text, most of the Methods and Supplementary Information and prepared \\nmost of the figures. J.K. improved the mathematical formalization of semantic entropy; led  \\nthe extension of semantic entropy to sentence- and paragraph-length generations; wrote the \\ncode for, and carried out, all the experiments and evaluations; wrote much of the Methods and \\nSupplementary Information and prepared drafts of many figures; and gave critical feedback \\non the main text. L.K. developed the initial mathematical formalization of semantic entropy; \\nwrote code for, and carried out, the initial experiments around semantic entropy and its \\nvariants which demonstrated the promise of the idea and helped narrow down possible \\nresearch avenues to explore; and gave critical feedback on the main text. Y.G. ideated the \\nproject, proposing the idea to differentiate semantic and syntactic diversity as a tool for \\ndetecting hallucinations, provided high-level guidance on the research and gave critical \\nfeedback on the main text; he runs the research laboratory in which the work was carried out.\\nCompeting interests S.F. is currently employed by Google DeepMind and L.K. by OpenAI. For \\nboth, this paper was written under their University of Oxford affiliation. The remaining authors \\ndeclare no competing interests.\\nAdditional information\\nSupplementary information The online version contains supplementary material available at \\nhttps://doi.org/10.1038/s41586-024-07421-0.\\nCorrespondence and requests for materials should be addressed to Sebastian Farquhar.\\nPeer review information Nature thanks Mirella Lapata and the other, anonymous, reviewer(s) \\nfor their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-06-08T12:34:01+05:30', 'author': 'Sebastian Farquhar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'keywords': '', 'moddate': '2024-06-08T12:35:01+05:30', 'subject': 'Nature, doi:10.1038/s41586-024-07421-0', 'title': 'Detecting hallucinations in large language models using semantic entropy', 'doi': '10.1038/s41586-024-07421-0', 'source': '/content/drive/My Drive/documents_RAG/s41586-024-07421-0.pdf', 'total_pages': 12, 'page': 11, 'page_label': '636'}, page_content='Article\\nExtended Data Fig. 1 | Algorithm outline for bidirectional entailment clustering. Given a set of outputs in response to a context, the bidirectional entailment \\nanswer returns a set of sets of outputs which have been classified as sharing a meaning.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1163'}, page_content='Investigating Hallucinations in Pruned\\nLarge Language Models for Abstractive Summarization\\nGeorge Chrysostomou∗♣† Zhixue Zhao∗♦ Miles Williams∗♦ Nikolaos Aletras♦\\n♦University of Sheffield, UK ♣AstraZeneca, UK\\n{zhixue.zhao, mwilliams15, n.aletras}@sheffield.ac.uk\\nAbstract\\nDespite the remarkable performance of gen-\\nerative large language models (LLMs) on\\nabstractive summarization, they face two sig-\\nnificant challenges: their considerable size\\nand tendency to hallucinate. Hallucinations\\nare concerning because they erode reliability\\nand raise safety issues. Pruning is a technique\\nthat reduces model size by removing redun-\\ndant weights, enabling more efficient sparse\\ninference. Pruned models yield downstream\\ntask performance comparable to the original,\\nmaking them ideal alternatives when operat-\\ning on a limited budget. However, the effect\\nthat pruning has upon hallucinations in ab-\\nstractive summarization with LLMs has yet\\nto be explored. In this paper, we provide an\\nextensive empirical study across five summa-\\nrization datasets, two state-of-the-art pruning\\nmethods, and five instruction-tuned LLMs.\\nSurprisingly, we find that hallucinations are\\nless prevalent from pruned LLMs than the orig-\\ninal models. Our analysis suggests that pruned\\nmodels tend to depend more on the source\\ndocument for summary generation. This leads\\nto a higher lexical overlap between the gen-\\nerated summary and the source document,\\nwhich could be a reason for the reduction in\\nhallucination risk.1\\n1 Introduction\\nAbstractive summarization is the task of distill-\\ning the key information from a document into a\\nsummary that may contain novel text not present\\nin the original document (Cohn and Lapata, 2008;\\nSaggion and Poibeau, 2013; Lin and Ng, 2019).\\nGenerative large language models (LLMs) have\\ndemonstrated strong performance on abstractive\\nsummarization (Ouyang et al., 2022; Touvron\\netal., 2023; Almazrouei et al., 2023; OpenAI\\net al., 2024; Zhang et al., 2024). However, they\\nface two significant challenges: Their substantial\\n∗ Equal contribution.\\n† Work done independently of AstraZeneca.\\n1https://github.com/casszhao/PruneHall.\\nsize requires extensive computational resources\\nfor training and inference; and they tend to hal-\\nlucinate, i.e., generate nonfactual contents not\\nsupported by the source document (Zhao et al.,\\n2020; Xu et al., 2023). Figure 1 shows an il-\\nlustrative example of hallucinated content in a\\ngenerated summary.\\nOn the one hand, hallucinations not only under-\\nmine the performance of models but also introduce\\ncritical safety risks, ultimately eroding the trust of\\nend users (Milintsevich and Agarwal, 2023; Tang\\net al., 2023a; Narayan et al., 2023; Zhao and Shan,\\n2024). For example, LLM-generated summaries\\nin the legal or health domain can contain inaccu-\\nrate information that poses real-life harms (Zhao\\net al., 2022a; Weidinger et al., 2022).\\nOn the other hand, LLMs such as GPT-3.5\\n(Ouyang et al., 2022), GPT-4 (OpenAI et al.,\\n2024), and Llama-2 (Touvron et al., 2023) demand\\nsubstantial hardware resources. As an indication,\\nGPT-3 (175B) requires at least five NVIDIA\\nA100 GPUs with 80GB of memory each for\\nhalf-precision inference (Frantar and Alistarh,\\n2023). This creates barriers for those without\\naccess to costly computational resources, ulti-\\nmately hindering inclusivity in NLP (Schwartz\\net al., 2020; Weidinger et al., 2022). To tackle\\nthis issue, pruning techniques enable efficient\\nsparse inference by removing redundant weights,\\nwhile maintaining comparable performance (Sun\\net al., 2024). Pruned models therefore appear as at-\\ntractive alternatives for abstractive summarization\\nwhen computational resources are constrained.\\nIn abstractive summarization, model halluci-\\nnations are a thoroughly studied subject (Cao\\net al., 2020; Durmus et al., 2020; Raunak et al.,\\n2021; Narayan et al., 2023; Laban et al., 2023).\\nSimilarly, the effect of pruning on model\\nperformance in abstractive summarization bench-\\nmarks was also explored more recently (Dun\\net al., 2023; Jaiswal et al., 2024). However, the\\nrelationship between pruning and hallucination\\n1163\\nTransactions of the Association for Computational Linguistics, vol. 12, pp. 1163–1181, 2024. https://doi.org/10.1162/tacl a 00695\\nAction Editor: Wenjie (Maggie) Li. Submission batch: 3/2024; Revision batch: 4/2024; Published 9/2024.\\nc⃝ 2024 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 1, 'page_label': '1164'}, page_content='Figure 1: An example of a hallucination ( highlighted\\ntext) in abstractive summarization.\\nrisk has yet to be explored. Given the appeal of\\ngreater efficiency with comparable downstream\\nperformance it is important to establish how trust-\\nworthy summaries generated from pruned models\\nare. Therefore, we seek to answer the follow-\\ning question: Are hallucinations more or less\\nprevalent in LLMs after pruning?\\nTo this end, we empirically investigate the risk\\nof generating hallucinated content in pruned mod-\\nels across five LLMs, two state-of-the-art pruning\\nmethods, and five summarization datasets. Sur-\\nprisingly, our results show that pruned models are\\nless prevalent in hallucinations compared to the\\noriginal LLM. To understand this phenomenon,\\nwe further investigate the impact of different spar-\\nsity levels on hallucination patterns. Our analysis\\nshows that hallucination risk decreases as spar-\\nsity increases, regardless of the pruning methods\\ntested. Furthermore, our results suggest that prun-\\ning encourages the model to rely more on the\\nsource document during generation, resulting in\\nsummaries that are lexically more similar to the\\nsource document.\\n2 Related Work\\n2.1 Hallucinations in Summarization\\nIn abstractive summarization, a model is expected\\nto generate a concise summary of the source doc-\\nument. However, prior work observed that models\\ntend to generate hallucinatory content that is not\\nbased on or cannot be entailed from the source\\ndocument (Vinyals and Le, 2015; Rohrbach et al.,\\n2018; Cao et al., 2018; Maynez et al., 2020;\\nRaunak et al., 2021; Falke et al., 2019; Maynez\\net al., 2020; Zhao et al., 2022b; Chen et al.,\\n2022). For example, Falke et al. (2019) found that\\n25% of the model generated summaries contain\\nhallucinated content. On the other hand, auto-\\nmatic summary quality evaluation metrics such\\nas ROUGE (Lin, 2004) and BERTScore (Zhang\\net al., 2020) do not correlate with the degree\\nof hallucinations appearing in summaries (Zhou\\net al., 2021). For instance, Zhou et al. (2021) show\\nthat even if a summary contains a large amount\\nof hallucinatory content, it can still achieve a high\\nROUGE score. This has opened up new research\\ndirections that develop approaches to detect and\\nevaluate hallucinations (Zhou et al., 2021; Durmus\\net al., 2020; Guerreiro et al., 2023; Ji et al., 2023),\\nas well as mitigate them (Xiao and Wang, 2021;\\nChoubey et al., 2023; King et al., 2022).\\n2.2 Measuring Hallucination Risk\\nEvaluation metrics for measuring hallucina-\\ntion risk can be broadly categorized as: (a)\\nentailment-based, (b) question-answering (QA),\\nand (c) text-generation based. Entailment-based\\nmethods (Kryscinski et al., 2020; Laban et al.,\\n2022) use pre-trained language models to com-\\npute the entailment score between the source and\\nthe generated summary. The higher the entail-\\nment score, the more consistent a summary is\\nwith respect to the source. QA methods decom-\\npose the task to a question answering problem\\n(Wang et al., 2020; Deutsch et al., 2021; Durmus\\net al., 2020). Finally, text-generation based meth-\\nods use off-the-shelf models to quantify the risk\\nof hallucinations (Yuan et al., 2021; Son et al.,\\n2022). A representative approach is the Halluci-\\nnation Risk Measurement (HaRiM+), which uses\\nthe log-likelihoods from a reference-free decoder\\nmodel to estimate hallucination risk in a summary\\nat the token level (Son et al., 2022). More recently,\\nLaban et al. (2023) examined instruction-tuned\\nLLMs as reasoners for factual assessments (i.e.,\\nassessors of hallucination prevalence) in abstrac-\\ntive text summarization. They demonstrated that\\nmany of these LLMs struggle to compete with\\nprevious entailment-based methods.\\n2.3 Pruning Large Language Models\\nModel compression is the task of reducing the\\nmemory footprint of a model (Ganesh et al., 2021).\\nPruning is a popular technique that removes re-\\ndundant weights from the model (LeCun et al.,\\n1989). Weights may be removed individually (un-\\nstructured pruning), according to defined blocks\\n1164\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 2, 'page_label': '1165'}, page_content='(semi-structured pruning), or in relation to model\\ncomponents (structured pruning) (Blalock et al.,\\n2020; Mishra et al., 2021; Ma et al., 2023).\\nAs the size of LLMs surpasses billions of param-\\neters, pruning techniques that require re-training\\nbecome impractical. Instead, post-training com-\\npression aims to reduce model size using only\\na small calibration dataset (Nagel et al., 2020;\\nWilliams and Aletras, 2023). In this setting,\\nFrantar and Alistarh (2022) define the layer-wise\\ncompression problem, with the aim of creat-\\ning a compressed version of a given layer that\\nfunctions as closely as possible to the original.\\nState-of-the-art post-training pruning techniques,\\nsuch as SparseGPT (Frantar and Alistarh, 2023)\\nand Wanda (Sun et al., 2024), build upon this,\\noffering layer-wise solutions. SparseGPT in-\\ntroduces an efficient approximation that relies\\nupon an iterative weight update process us-\\ning Hessian inverses, inspired by Optimal Brain\\nSurgeon (Hassibi et al., 1993). Wanda further\\nimproves upon efficiency by avoiding a weight\\nupdate procedure, enabling pruning in a single\\nforward pass.\\nIn practice, the sparsity induced by pruning\\nenables substantial improvements in inference\\nperformance across a variety of hardware. On\\na CPU, Frantar and Alistarh (2023) demonstrate\\na1 . 8 2× speedup with 50% unstructured spar-\\nsity, using the DeepSparse engine (Neural Magic,\\n2021). Separately, they observe a 1.54-1.79 ×\\nspeedup for feed-forward layers on an NVIDIA\\nAmpere GPU, using 2:4 semi-structured sparsity\\n(Mishra et al., 2021).\\nRecent pruning approaches (such as SparseGPT\\nand Wanda) can be applied to decoder-only\\nLLMs with minimal impact upon common-sense\\nreasoning (Sun et al., 2024) or summarization\\nperformance (Jaiswal et al., 2024). Interestingly,\\nrelated studies suggest that pruning can reduce\\nsocial bias and toxicity (Xu and Hu, 2022) and im-\\nprove resilience to ‘jailbreaking’ attacks (Hasan\\net al., 2024). However, it remains unclear how\\npruning affects hallucination risk in LLMs.\\n3 Methodology\\n3.1 Models\\nWe experiment with the following publicly avail-\\nable LLMs: (1) the Llama-2 (Touvron et al.,\\n2023) model family (7B, 13B, and 70B); (2) Mis-\\ntral 7B (v0.1) (Jiang et al., 2023); (3) Falcon 7B\\n(Almazrouei et al., 2023); and (4) the OPT-IML\\n(Iyer et al., 2023) model family (1.3B and 30B).\\nWe opt for decoder-only instruction-tuned mod-\\nels due to their efficacy in zero-shot abstractive\\nsummarization tasks (Tang et al., 2023b; Adams\\net al., 2023; Laskar et al., 2023).\\n3.2 Pruning Methods\\nWe consider three different pruning methods:\\none standard baseline (layer-wise magnitude) and\\ntwo state-of-the-art techniques (SparseGPT and\\nWanda). Formally, these pruning methods pro-\\nvide a saliency score Sij for each element of the\\nweight matrix Wij in a given layer. The elements\\ncorresponding to the k smallest saliency scores\\nare the target weights to be pruned, where k is\\ndetermined by the sparsity ratio. The primary dis-\\ntinction between our selected pruning methods\\nlies in their saliency score calculation metrics. In\\na post-training setting, pruning metrics can ad-\\nditionally incorporate layer activations,X.T h e\\nactivations for each layer of the model are com-\\nputed through performing a forward pass with the\\ncalibration data. We follow Sun et al. (2024) in\\nusing the same calibration data for each model,\\nspecifically 128 examples randomly sampled\\nfrom C4 (Raffel et al., 2020).\\nMagnitude (Hagiwara, 1994; Han et al., 2015)\\nTo offer a lower bound for the performance of\\npruned models, we employ layer-wise weight\\nmagnitude pruning. Here, the saliency score is\\nsimply the magnitude of each weight:\\nSij = |Wij|\\nSparseGPT (Frantar and Alistarh, 2023) The\\nSparseGPT algorithm is an iterative procedure that\\noffers an efficient approximation to the exact layer\\nreconstruction. The effective saliency criterion is\\nSij =\\n[\\n|W|2/diag\\n(\\n(XXT + λI)−1)]\\nij\\nwhere λ is a dampening parameter to enable\\ninversion of the Hessian, XXT + λI.2\\nWanda (Sun et al., 2024) In contrast, Wanda\\navoids a computationally expensive weight update\\nprocedure, instead relying upon only the weight\\nmagnitudes and norm of the input activations:\\nSij = |Wij|·|| X||2\\nThis approximates SparseGPT when considering\\nonly diagonal elements of the Hessian for λ =0 .\\n2We follow Frantar and Alistarh (2023) in usingλ =0 .01.\\n1165\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 3, 'page_label': '1166'}, page_content='# Prompt Template\\nA Summarize in a single short paragraph the context below:\\n[document]\\nThe summary is:[summary]\\nB Summarize in a couple of sentences the document below:\\n[document]\\nThe summary is:[summary]\\nC Give me a short summary of the below:\\n[document]\\nThe summary is:[summary]\\nTable 1: Each prompt template consists of the task\\ninstructions (italic) and the source[document].\\nThe LLM then generates the [summary].\\nSparsity Level Following previous work (Fantar\\nand Alistarh 2023; Sun et al., 2024), we evaluate\\nour pruning methods across both semi-structured\\nand unstructured settings:\\n• 2:4 semi-structured sparsity: Two weights\\nin every contiguous block of four must be\\nzero, providing a total of 50% sparsity. This\\nsparsity pattern is required to enable hard-\\nware acceleration on GPUs (Mishra et al.,\\n2021).\\n• 50% unstructured sparsity: To enable com-\\nparison, we use a sparsity level of 50%\\nfor unstructured pruning, unless otherwise\\nstated.\\nWe do not explore pruning above 50% sparsity as\\nlanguage modeling performance collapses shortly\\nbeyond this threshold (Frantar and Alistarh, 2023;\\nSun et al., 2024). Maintaining language model-\\ning performance is essential for the generation\\nof high-quality summaries, enabling comparison\\nbetween the models and their pruned counterparts.\\n3.3 Prompting\\nLLMs are known to be sensitive to prompt design\\n(Petroni et al., 2019; Elazar et al., 2021; Fierro\\nand Søgaard, 2022). To mitigate the effect of\\nprompt variability, we summarize each document\\nusing three distinct prompt templates (Table 1).\\nEach template instructs the model to summarize\\na given document in a slightly different manner,\\noffering three summaries for each document. We\\nthen evaluate all three summaries by averaging\\nthe scores.\\nFor each model family, we follow the prompt\\nformatting used in the original work. In the case\\nof Llama-2 and Mistral, this includes the use of\\nSource Reference\\nDataset # Mean Max Mean Max\\nFactCC 311 634.2 1838 17.4 63\\nPolytope 634 575.1 1781 64.6 128\\nSummEval 100 407.8 589 65.1 101\\nLegal Contracts 85 237.8 1106 21.6 61\\nRCT 53 307.5 447 68.7 174\\nTable 2: The number of source documents in\\neach dataset (#), and the mean and maximum\\nlength (in words) for the documents and reference\\nsummaries.\\n[INST] and [/INST] tokens to delimit user\\ninstructions. For the Falcon and OPT-IML model\\nfamilies, which were not trained with a specific\\nprompt format, we use the prompts as is (Table 1).\\n3.4 Summarization Datasets\\nWe include the following summarization datasets:\\n(1) FactCC (Kryscinski et al., 2020); (2) Poly-\\ntope (Huang et al., 2020); (3) SummEval (Fabbri\\net al., 2021); (4) Legal Contracts(Manor and Li,\\n2019); and (5) RCT summaries (Wallace et al.,\\n2021). FactCC, Polytope, and SummEval are all\\ndifferent subsets of the CNN/DailyMail news ar-\\nticle dataset (Nallapati et al., 2016), covering a\\nvariety of topics. Legal Contracts consists of legal\\ntext snippets from the terms of service for various\\nproducts and services. Finally, RCT combines the\\nabstracts from randomized control trials with their\\ncorresponding human-written conclusions from\\nsystematic reviews, i.e., the conclusions are used\\nas the target summary. For simplicity, we select\\ninstances in RCT where there is a one-to-one\\nmapping between abstract and target summary.\\nWe use the test set from each dataset and re-\\nmove any duplicates if any exist. Table 2 provides\\ndetailed dataset statistics.\\n3.5 Evaluation of Summarization Quality\\nWe evaluate the quality of generated summaries\\nagainst the corresponding reference summary, us-\\ning a subset of the ROUGE family of metrics\\n(Lin, 2004) and BERTScore (Zhang et al., 2020).3\\nFrom ROUGE, we use two n-gram overlap met-\\nrics (ROUGE-1 and ROUGE-2) and the longest\\nsequence overlap metric (ROUGE-L).\\n3.6 Hallucination Risk Metrics\\nTo automatically evaluate the hallucination risk\\nin the generated summaries, we use standard\\n3For FactCC, we use the extracted claim as the reference.\\n1166\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 4, 'page_label': '1167'}, page_content='automatic metrics that compare directly the\\nsource document and the corresponding generated\\nsummary.\\nHaRiM+ (Son et al., 2022) HaRiM is based on\\nthe idea that over-reliance on decoder context dur-\\ning generation leads to hallucinations. Given a\\nsummary and a reference document, HaRiM+ first\\nuses a pre-trained sequence-to-sequence model\\n(S2S, an encoder-decoder model) to calculate\\nthe token probabilities in the summary given\\nthe reference document as input. A pre-trained\\ndecoder-only model is used as a secondary model\\n(Aux) to compute summary token probabilities,\\ni.e., no input document is provided to summarize.\\nHaRiM+ therefore uses Aux token probabilities\\nto regularize S2S token probabilities and detect\\nhallucinations by:\\nHaRiM = 1\\nL\\nL∑\\ni=0\\n(1 −pS2S)(1 −(pS2S −pAux))\\nwhere L is the sequence length, pS2S the pre-\\ndicted probability of a token generated by the\\nmodel given the source document, and pAux the\\nprobability of the same generated token from\\nthe auxiliary model.\\nHaRiM+ extends HaRiM through adding the\\nS2S log-likelihood of tokens, and applying a\\nscaling hyperparameterλH :4\\nHaRiM+ = 1\\nL\\nL∑\\ni\\nlog(p(yi | y<i; X))−λH HaRiM\\nIntuitively, a higher HaRiM + score indicates that\\nthe summary is more likely to be faithful to the\\nsource document, i.e., less likely to contain hallu-\\ncinations. Son et al. (2022) also showed that the\\nfirst sequence-to-sequence model can also act as\\na secondary model, with equivalent performance.\\nSummaC (Laban et al., 2022) This metric uses\\nan off-the-shelf entailment model to assess the\\nconsistency between a source document and a\\ngenerated summary. First, the document and sum-\\nmary are split into sentences, with the document\\nsentences (N) being the hypothesis and the gen-\\nerated summary sentences (K) being the premise.\\nThe second step is to create an K ×N matrix of\\nentailment scores from the pre-trained model. A\\ngenerated sentence with a low entailment score\\n4We follow Son et al. (2022) in using λH =7 .\\nto any of the document sentences is a potential\\nhallucination.\\nSummaCZS obtains the row-wise maximum en-\\ntailment score, which leads to a vector E of size\\nK. SummaCConv obtains vector E by using a\\nconvolutional model over each row K, to obtain\\na single score. In both metrics, each element in\\nE can be interpreted as the consistency score for\\neach sentence in the summary. E is averaged to\\nobtain a single summary consistency score.\\nHallucination Risk Ratio (HRR) To compare\\nthe hallucination risk of pruned models relative\\nto the original, we compute a ratio using any one\\nof the hallucination risk metrics:\\nHRR = Hallucination RiskOriginal\\nHallucination RiskPruned\\nA lower HRR indicates that the pruned model\\nhas a lower hallucination risk than the origi-\\nnal. This contrasts the hallucination risk metrics,\\nwhere a higher score indicates a lower risk for a\\ngiven model.\\n3.7 Human Evaluation\\nWe also conduct a human evaluation task to com-\\npare the hallucination prevalence between the\\noriginal and pruned models. For this purpose,\\nwe randomly sample 100 distinct source docu-\\nments from FactCC, Polytope, and SummEval.\\nWe selected these datasets because they consist\\nof news articles, making them suitable for human\\nevaluation without requiring extensive domain ex-\\npertise. We recruited three participants who are\\nnative speakers or proficiently fluent in English.\\nFollowing Lango and Dusek (2023), we ask them\\nto answer the following questions for compar-\\ning the summaries generated by the original and\\npruned models:\\nQ1. Hallucinations: Which summary contains\\nmore hallucinations (i.e., content that is not\\nsupported by the source document)?\\nQ2. Omission: Which summary is missing more\\ncrucial information from the document?\\nQ3. Repetition: Which summary contains more\\nrepetitive information?\\nQ4. Alignment: Which summary is more seman-\\ntically aligned with the source document?\\nIdentifying hallucinations in text is challenging\\nand requires careful reading and attention to nu-\\nanced facts (Laban et al., 2023). Therefore, we\\n1167\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 5, 'page_label': '1168'}, page_content='Magnitude SparseGPT Wanda\\nModel – 2:4 50% 2:4 50% 2:4 50%\\nFalcon 7B 19.93 303.22 482.11 52.11 37.10 85.68 38.93\\nLlama-2 7B 6.49 78.29 19.07 10.79 7.94 12.46 7.93\\nLlama-2 13B 5.71 10.73 7.98 8.68 6.80 9.58 6.94\\nLlama-2 70B 4.30 6.89 5.61 6.51 5.18 6.45 5.23\\nMistral 7B 6.32 9.55 7.96 9.21 7.18 9.85 7.26\\nOPT-IML 1.3B 14.68 166.09 1391.46 24.92 18.03 25.11 17.94\\nOPT-IML 30B 10.56 246.42 57.88 11.61 10.74 12.44 10.74\\nTable 3: Perplexity ( ↓) of original and pruned\\nmodels on the held-out set of WikiText.\\nfirst perform a calibration run on a held-out set\\nof ten documents and their generated summaries.\\nTwo of the participants are then presented with\\nthe set of 100 original documents, alongside two\\ngenerated summaries: one from a pruned model\\nand the other from the original model. The or-\\nder of the documents is shuffled and information\\nabout which model generated the summary is not\\ndisclosed to the participants. Similar to Xu et al.\\n(2023), we use the third participant as an ad-\\njudicator for disagreements. The inter-annotator\\nagreement is computed using Cohen’s kappa IAA\\n(κ), as the average between the two participants\\nand the adjudicator.\\n3.8 Implementation Details\\nWe use the model implementation and weights\\navailable from Hugging Face (Wolf et al., 2020).\\nWe perform experiments using either one or two\\nNVIDIA A100 (SXM 80GB) GPUs. For the prun-\\ning methods, we use the hyperparameters from\\nFrantar and Alistarh (2023) and Sun et al. (2024).\\nFor summary generation we use greedy de-\\ncoding (i.e., sampling the token with the highest\\nprobability) for better reproducibility. We con-\\ntinue to sample tokens until we reach either (a)\\nthe end of sequence token, or (b) the maximum\\nsequence length of the model.\\n4 Results\\n4.1 Language Modeling\\nWe first compare language modeling performance\\nbetween the original and pruned models. Follow-\\ning Frantar and Alistarh (2023) and Sun et al.\\n(2024), we compute perplexity on the WikiText\\ntest set (Merity et al., 2017), shown in Table 3.\\nOverall, pruned models consistently generate\\ntext with higher perplexity than their original\\ncounterparts. Unsurprisingly, magnitude pruning\\nroutinely produces the highest perplexity. In many\\ncases, the increase over the original model (de-\\nnoted by ‘-’) is substantial. For example, we\\nobserve more than a twentyfold increase for\\nOPT-IML 30B, from 10.56 to 246.42. In con-\\ntrast, SparseGPT and Wanda achieve perplexity\\nclose to the original for the majority of the models.\\nSurprisingly, Falcon 7B records higher perplex-\\nity across all pruning methods, e.g., 85.68 when\\napplying Wanda from 19.93 without pruning.\\nDue to the substantial degradation in language\\nmodeling performance, we omit magnitude prun-\\ning from further analysis. For the same reason, we\\nalso exclude the Falcon 7B and OPT-IML 1.3B\\nmodels.\\n4.2 Summarization\\nTable 4 shows summarization performance\\n(ROUGE-1/2/L & BERTScore) across all\\ndatasets.5 We first observe that the original mod-\\nels perform comparably for BERTScore across\\nmost datasets. For example, in Legal Contracts,\\nLlama-2 13B records a BERTScore of 84.75\\ncompared to 84.90 from OPT-IML 30B. We only\\nobserve larger performance deviations in the case\\nof RCT, with the original Mistral 7B obtaining the\\nhighest BERTScore (88.46) and OPT-IML 30B\\nthe lowest (83.12). This suggests that all LLMs\\ngenerate summaries that are equally semantically\\nsimilar to the reference summary. Compared to\\nBERTScore, the scores of the original models\\nin lexical overlap metrics (ROUGE-1/2/L) differ\\nlargely not only across models, but also across\\ndatasets. For example, Llama-2 7B achieves\\nthe second highest ROUGE-L score in RCT\\n(33.50) and the lowest score in FactCC (11.51).\\nSimilarly, in RCT, Mistral 7B records an increase\\nof 34.65 (46.16) for ROUGE-L, making it the\\nbest performing original model for this metric.\\nComparing the performance between original\\nand pruned models, we find that they perform com-\\nparably in the majority of cases. For SparseGPT,\\nthe summaries score significantly higher (across\\nall metrics) than those from the original model\\nin 19 out of 100 comparisons, while they score\\nsignificantly lower in 11 out of 100 ( bold scores;\\npaired t-test; p< 0.05). The results are similar\\nfor Wanda, where pruned models perform sig-\\nnificantly higher in 20 out of 100 comparisons\\nand significantly lower (underlined scores) in 26\\n5We obtain comparable results using 50% unstructured\\nsparsity, which are omitted for brevity.\\n1168\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 6, 'page_label': '1169'}, page_content='Llama-2 7B Llama-2 13B Llama-2 70B Mistral 7B OPT-IML 30B\\nDataset Method ROUGE-1/2/L BS ROUGE-1/2/L BS ROUGE-1/2/L BS ROUGE-1/2/L BS ROUGE-1/2/L BS\\nFactCC\\n– 13.99 / 6.41 / 11.51 84.60 15.14 / 6.39 / 12.30 84.39 15.04 / 6.29 / 12.11 84.75 14.83 / 8.21 / 12.70 84.78 23.51 / 12.68 / 20.48 85.71\\nSpGPT 12.46 / 6.07 / 10.55 84.15 15.34 / 6.62 / 12.75 84.76 14.78 / 6.80 / 12.29 84.68 14.43 / 8.52 / 12.62 84.45 18.52 / 12.05 / 16.89 85.04\\nWanda 11.04 / 5.94 / 9.53 80.57 15.64 / 7.32 / 13.09 84.78 15.09 / 6.88 / 12.47 84.72 13.67 / 8.30 / 12.02 84.34 17.91 / 11.68 / 16.38 83.94\\nPolytope\\n– 38.92 / 18.19 / 25.86 85.41 38.63 / 17.51 / 25.34 84.91 39.28 / 17.48 / 25.78 85.48 40.27 / 22.69 / 28.65 85.63 33.06 / 22.81 / 27.74 86.54\\nSpGPT 33.98 / 18.14 / 24.45 84.88 35.99 / 16.74 / 25.01 85.01 38.16 / 18.51 / 25.89 85.31 39.07 / 24.21 / 29.54 85.58 33.39 / 26.32 / 29.02 87.01\\nWanda 30.88 / 15.39 / 21.77 83.09 37.33 / 19.29 / 26.68 85.23 38.74 / 18.80 / 26.58 85.42 37.08 / 23.78 / 28.76 85.34 30.14 / 22.72 / 25.85 86.03\\nSummEval\\n– 40.39 / 18.73 / 26.61 85.42 40.36 / 18.00 / 25.88 84.78 41.52 / 18.78 / 26.82 85.58 43.94 / 26.34 / 32.04 86.05 51.93 / 36.55 / 41.38 86.94\\nSpGPT 38.77 / 23.04 / 27.81 85.36 40.55 / 18.42 / 27.15 85.33 41.58 / 19.69 / 27.65 85.61 43.77 / 28.00 / 33.33 86.03 50.00 / 37.16 / 41.64 86.73\\nWanda 37.78 / 23.95 / 28.82 85.12 44.31 / 23.51 / 31.58 86.03 41.57 / 19.44 / 27.67 85.57 45.11 / 29.95 / 34.84 86.22 44.48 / 33.57 / 36.90 86.12\\nLegal\\nContracts\\n– 18.75 / 6.20 / 13.93 84.73 21.12 / 6.90 / 15.41 84.75 21.66 / 7.07 / 16.19 85.60 17.52 / 6.21 / 13.70 84.78 22.96 / 7.45 / 18.30 84.90\\nSpGPT 16.84 / 5.98 / 12.80 84.17 18.99 / 6.11 / 14.41 84.90 21.74 / 7.42 / 16.73 85.33 18.56 / 6.90 / 14.51 84.76 21.18 / 7.22 / 17.15 84.49\\nWanda 14.22 / 4.94 / 11.14 81.52 18.80 / 6.37 / 14.53 84.41 22.13 / 7.51 / 16.72 85.55 18.14 / 6.37 / 13.83 84.79 19.10 / 6.79 / 15.36 81.86\\nRCT\\n– 45.29 / 26.89 / 33.50 86.97 39.87 / 22.01 / 28.56 86.43 37.79 / 20.98 / 28.05 86.25 53.66 / 40.66 / 46.16 88.46 24.62 / 18.20 / 21.33 83.12\\nSpGPT 50.57 / 37.40 / 43.12 87.89 37.81 / 22.40 / 29.37 86.26 40.19 / 25.35 / 31.97 86.57 56.93 / 47.79 / 52.45 89.17 25.22 / 21.50 / 23.61 77.39\\nWanda 38.79 / 28.59 / 33.12 86.06 36.90 / 23.07 / 28.82 86.11 39.61 / 24.79 / 31.60 86.49 59.29 / 50.02 / 54.83 89.40 31.59 / 28.84 / 30.49 70.64\\nTable 4: ROUGE-1/2/L ( ↑) and BERTScore (BS; ↑) for the original models (–) and their pruned\\ncounterparts (SparseGPT and Wanda). Values inbold indicate that the pruned model scores significantly\\nhigher than the original while underlined values denote a significantly lower score (paired t-test;\\np< 0.05).\\nLlama-2 7B Llama-2 13B Llama-2 70B Mistral 7B OPT-IML 30B\\nSparseGPT Wanda SparseGPT Wanda SparseGPT Wanda SparseGPT Wanda SparseGPT Wanda\\nDataset Metric 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50% 2:4 50%\\nFactCC\\nHaRiM+ 0.98 0.95 0.94 0.95 0.77 0.95 0.69 0.91 0.93 0.96 0.93 0.96 0.93 0.94 0.91 0.94 0.83 0.87 0.87 0.85\\nSummaCconv 0.64 0.82 0.56 0.81 0.76 0.83 0.64 0.84 0.76 0.92 0.77 0.90 0.79 0.88 0.74 0.86 0.80 0.86 0.84 0.83\\nSummaCzs 0.47 0.65 0.39 0.65 0.50 0.61 0.41 0.61 0.63 0.86 0.63 0.83 0.76 0.85 0.68 0.82 0.80 0.87 0.85 0.83\\nPolytope\\nHaRiM+ 0.97 0.97 0.97 0.97 0.78 0.93 0.71 0.85 0.94 0.96 0.95 1.00 0.95 0.95 0.94 0.96 0.87 0.93 0.92 0.88\\nSummaCconv 0.67 0.83 0.69 0.83 0.70 0.78 0.65 0.79 0.77 0.93 0.78 0.92 0.78 0.82 0.76 0.84 0.86 0.95 0.91 0.92\\nSummaCzs 0.64 0.85 0.64 0.75 0.58 0.69 0.56 0.69 0.75 0.88 0.74 0.83 0.76 0.81 0.75 0.84 0.88 0.95 0.92 0.93\\nSummEval\\nHaRiM+ 0.88 0.93 0.81 0.93 0.80 0.97 0.69 0.96 0.95 0.98 0.95 0.98 0.93 0.94 0.92 0.95 0.91 0.92 0.90 0.89\\nSummaCconv 0.55 0.81 0.46 0.76 0.67 0.81 0.59 0.81 0.78 0.96 0.79 0.93 0.79 0.85 0.77 0.87 0.86 0.88 0.83 0.85\\nSummaCzs 0.49 0.75 0.4 0.68 0.56 0.71 0.49 0.66 0.70 0.92 0.70 0.88 0.79 0.84 0.76 0.88 0.86 0.89 0.85 0.86\\nLegal\\nContracts\\nHaRiM+ 0.99 0.85 0.90 0.85 0.83 0.88 0.76 0.88 0.87 0.92 0.89 0.95 0.85 0.94 0.89 0.93 0.85 0.89 0.81 0.83\\nSummaCconv 0.98 0.85 0.93 0.94 0.82 0.81 0.76 0.81 0.79 0.88 0.83 0.91 0.83 0.92 0.92 0.89 0.85 0.88 0.81 0.86\\nSummaCzs 1.01 0.86 0.96 0.90 0.93 0.86 0.88 0.88 0.85 0.93 0.88 0.95 0.88 0.92 0.93 0.92 0.93 0.96 0.94 1.00\\nRCT\\nHaRiM+ 0.92 0.96 0.87 0.92 0.86 0.99 0.80 0.97 0.93 0.96 0.93 0.97 0.93 0.96 0.93 0.95 0.85 0.88 0.83 0.87\\nSummaCconv 0.69 0.86 0.70 0.88 0.78 0.89 0.79 0.88 0.82 0.92 0.82 0.93 0.82 0.88 0.81 0.87 0.83 0.88 0.79 0.88\\nSummaCzs 0.71 0.83 0.71 0.82 0.69 0.81 0.70 0.82 0.79 0.90 0.79 0.90 0.84 0.89 0.82 0.89 0.77 0.80 0.77 0.83\\nAverage\\nHaRiM+ 0.95 0.93 0.90 0.92 0.81 0.95 0.73 0.91 0.92 0.96 0.93 0.97 0.92 0.95 0.92 0.95 0.87 0.90 0.87 0.87\\nSummaCconv 0.70 0.83 0.67 0.85 0.74 0.82 0.68 0.83 0.78 0.92 0.80 0.92 0.80 0.87 0.80 0.87 0.84 0.89 0.84 0.87\\nSummaCzs 0.67 0.79 0.62 0.76 0.65 0.74 0.61 0.73 0.74 0.90 0.75 0.88 0.81 0.86 0.79 0.87 0.85 0.90 0.86 0.89\\nTable 5: Hallucination risk ratio (HRR) between the original and the pruned model (values less than one\\nare highlighted, indicating that the pruned model has a lower hallucination risk than the original model),\\naveraged across all data points over the three prompts for each dataset. Bold values denote significant\\ndifferences between the pruned and the original model (paired t-test; p< 0.05).\\nout of 100. We also find that models pruned\\nwith SparseGPT perform more consistently com-\\npared to those pruned using Wanda. For example,\\nLlama-2 7B pruned with SparseGPT records a\\nBERTScore of 84.17 for Legal Contracts, com-\\npared to 81.52 with Wanda, and 84.73 from\\nthe original.\\nComparing across model sizes for Llama-2,\\npruning seems to be less impactful as model size\\nincreases. For SparseGPT, we find that the pruned\\nmodel is comparable (by any metric) in 15 out of\\n20 comparisons for Llama-2 7B, 18 out of 20 for\\nLlama-2 13B, and in all 20 for Llama-2 70B.\\nThese findings suggest that the summarization\\nperformance between pruned and original models\\nis at least comparable.\\n4.3 Hallucination Risk\\nTable 5 shows the HRR (Section 3.6) for all\\nmodels and datasets, using each hallucination risk\\nmetric.6\\nPruning Reduces Hallucination Risk. In al-\\nmost all cases, irrespective of the pruning method\\nor sparsity pattern (i.e., 2:4 or 50%), the results\\nshow that pruned models have a lower hallu-\\ncination risk (i.e., values lower than 1.0). We\\nfind only a single exception, Llama-2 7B pruned\\nwith SparseGPT (2:4) for Legal Contracts, with\\na SummaC ZS ratio of 1.01. More importantly,\\n6For reproducibility and transparency, we include the\\nfull results (i.e., absolute hallucination risk scores) in this\\nlinkdue to space constraints.\\n1169\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 7, 'page_label': '1170'}, page_content='pruned models record significantly lower HRRs\\n(paired t-test; p< 0.05). This applies to 284 out\\nof 300 total comparisons across datasets, models,\\npruning methods, and sparsity patterns. For exam-\\nple, we observe significantly lower scores across\\nall metrics for Llama-2 7B with SummEval. In\\nparticular, SummaCZS scores more than halve for\\n2:4 semi-structured SparseGPT (0.55) and 2:4\\nsemi-structured Wanda (0.49).\\nThese findings seem counter intuitive, con-\\nsidering that pruned models typically perform\\ncomparably to original models in summarization\\n(Table 4). As both language modeling and sum-\\nmarization performance remains comparable, we\\nhypothesize that the parametric knowledge re-\\nmoved by pruning (Namburi et al., 2023) ‘‘forces’’\\nthe model to rely more on the source docu-\\nment during generation and in turn reducing\\nhallucination risk. We examine this further in\\nSection 5.\\nSemi-structured Pruning Mitigates Hallucina-\\ntion Risk. We observe consistently lower HRRs\\nwhen pruning with semi-structured sparsity (2:4\\npattern), versus unstructured pruning at the same\\nsparsity level (50%). Semi-structured pruning\\nrecords a lower HRR across all three metrics in 59\\nout of 65 cases with SparseGPT, and in 55 out of 65\\ncases with Wanda. We note that semi-structured\\npruning sometimes produces a substantially lower\\nHRR than unstructured pruning. For example,\\nsemi-structured pruning for Llama-2 13B with\\nWanda records an average SummaC ZS HRR of\\n0.61 versus 0.73 with unstructured pruning.\\nUnstructured pruning allows weights to be re-\\nmoved in any pattern, enabling pruning according\\nto the optimal layer-wise solution. In contrast,\\nsemi-structured pruning constrains the solution\\nspace to only the subset that satisfies the de-\\nsired sparsity pattern (e.g., 2:4, removing two\\nweights in every contiguous block of four). In-\\nevitably, even influential weights with relatively\\nhigh layer-wise saliency scores may be removed.\\nAs semi-structured pruning deviates from the op-\\ntimal layer-wise solution, a higher proportion of\\nimportant weights are therefore removed. This\\nlikely includes relevant parametric knowledge\\n(Namburi et al., 2023), potentially requiring such\\nmodels to rely more on the source document for\\ngeneration.\\nTo investigate this, we compute lexical overlap\\n(using ROUGE-1/2/L) between summaries and\\ntheir source documents across all models, datasets\\nand pruning methods. We find that summaries\\nfrom models pruned with 2:4 sparsity result in\\nhigher lexical overlaps in 114 out of 150 compar-\\nisons (three ROUGE metrics, five datasets, five\\nmodels, two pruning methods) compared to mod-\\nels with 50% unstructured pruning, supporting\\nour hypothesis.\\nSummaC and HaRiM + Moderately Agree.\\nConsidering the average results across datasets,\\nwe observe mixed signals from SummaC-based\\nHRRs versus HaRiM + HRRs. For example,\\nSummaCConv with SparseGPT (2:4) shows that on\\naverage, Llama-2 7B benefits most over the origi-\\nnal (0.70), followed by Llama-2 13B (0.74). On the\\ncontrary, for HaRiM+with 2:4 sparsity, summaries\\nfrom Llama-2 13B appear to yield the largest re-\\nductions in hallucination risk on average (0.81\\nwith SparseGPT and 0.73 with Wanda), followed\\nby OPT-IML 30B (0.86 with both SparseGPT\\nand Wanda). As the results between hallucina-\\ntion risk metrics differ, we want to shed light\\non how well they agree with each other. There-\\nfore, we compute Pearson’s correlation coefficient\\nbetween all HRR metrics, across all datasets, mod-\\nels and pruning methods. Unsurprisingly, both\\nSummaC-based metrics show a strong correlation\\nbetween them (0.82 averaged across all datasets,\\nmodels and pruning methods). We also find mod-\\nerate correlations between HaRiM+ and SummaC\\nmetrics (0.45 between HaRiM + and SummaCZS;\\n0.53 between HaRiM+and SummaCConv).\\nThis is expected, as each metric group com-\\nputes hallucination risk with different motivations\\n(SummaC-based metrics use entailment methods\\nover the summary and document, while HaRiM +\\nuses token-level predictive likelihood). This ex-\\nplains partly the moderate correlation between\\nthem, also highlighting that it can be beneficial to\\nuse HaRiM+ and SummaC in conjunction.\\n4.4 Human Evaluation\\nTable 6 shows human evaluation results for\\nthe questions presented in Section 3. To offer\\na fair selection of models, we use summaries\\ngenerated by the pair that benefited the most\\n(Llama-2 7B) and the least (Mistral 7B) in\\nterms of hallucination risk (i.e., the largest and\\nsmallest improvements in Table 5). We then\\nselect the corresponding summaries from the\\npruned counterpart, specifically SparseGPT (2:4)\\n1170\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 8, 'page_label': '1171'}, page_content='Halluc. Omiss. Repet. Align.\\nModel Q1 ( ↓)Q 2 ( ↓)Q 3 ( ↓)Q 4 ( ↑)\\nLlama-2 7B 31 50 2 8\\nw/ SparseGPT 14 18 9 21\\nIAA (κ) 0.82 0.63 0.62 0.53\\nMistral 7B 12 90 3 1\\nw/ SparseGPT 10 13 5 23\\nIAA (κ) 0.87 0.61 0.67 0.59\\nTable 6: Human evaluation results. Values denote\\nthe number (out of 100) of summary preferences\\nby participants for the corresponding category.\\nBold denotes the best performing model per\\nquestion.\\nwhich obtained the most consistent summarization\\nperformance (Section 4.2).\\nOriginal Models Hallucinate More. Sum-\\nmaries generated by the original Llama-2 7B\\nmodel contain hallucinations in 31 cases (out\\nof 100) compared to 14 with SparseGPT applied.\\nIn comparison, the results for Mistral 7B also\\nsuggest that 10 (out of 100) summaries from\\nMistral 7B pruned with SparseGPT contain hal-\\nlucinations, compared to 12 summaries generated\\nusing the original model (i.e., a smaller difference\\ncompared to Llama-2 7B).\\nThis aligns well with our initial expectations and\\nHRR results (Table 5), as Mistral 7B benefits less\\nfrom pruning in terms of hallucination risk com-\\npared to Llama-2 7B. For example, considering\\nSummaCZS for SummEval, Llama-2 7B pruned\\nwith SparseGPT approximately halves the hallu-\\ncination risk (0.49) compared to 0.79 with Mistral\\n7B. From analyzing human evaluation results, we\\nfound that the large difference between pruned\\nand original Llama-2 7B is predominantly driven\\nby major factual errors (discussed in Section 6).\\nOriginal Models Omit and Repeat Slightly\\nLess. With substantial (0.61–0.80) agreement\\nbetween participants, the results agree that both\\noriginal models had no repetitions in their sum-\\nmaries and omitted less important information\\ncompared to pruned model summaries (e.g., nine\\ninstances with Mistral 7B compared to 13 with its\\npruned version with SparseGPT).\\nComparing how well the summaries semanti-\\ncally align with the source document, the results\\nshow a preference towards the original mod-\\nels (with moderate agreement; 0.40–0.60). For\\nexample, 28 (out of 100) summaries of the\\noriginal Llama-2 7B were selected as more\\naligned compared to 21 summaries when pruned\\nwith SparseGPT.\\n5 Impact of Pruning Sparsity on\\nHallucination Risk\\nTo better understand previous observations and\\ntest our hypothesis (i.e., sparsity likely encour-\\nages models to focus more on the source document\\nduring generation), we analyze hallucination risk\\nacross different sparsity levels. We additionally\\ntrack the lexical overlap (using ROUGE-1/2/L)\\nand semantic overlap (using BERTScore) between\\nthe generated summary and the source document.\\nOur hypothesis is:If lexical overlap positively cor-\\nrelates with sparsity levels, it suggests that pruned\\nmodels may rely more on the source document for\\ngeneration.\\nFigure 2 shows the summarization performance\\nratio (ROUGE-1/2/L and BERTScore; ratio com-\\nputed as pruned over original) and HRR ( ↓) for\\nfive LLMs and two pruning methods, across in-\\ncreasing levels of unstructured sparsity (10% to\\n50%). We only consider unstructured sparsity,\\nsince the 2:4 semi-structured pattern enforces a\\nfixed sparsity level of 50%. The ratio for each\\nmetric is averaged across datasets for brevity,\\nwith error bars indicating standard deviation. For\\nsummarization performance, a ratio higher than\\n1.0 indicate that the pruned model performs better\\nthan the original, whereas a HRR lower than 1.0\\nindicates that summaries from the pruned model\\nhave a lower hallucination risk.\\nHallucination Risk Reduces as Sparsity In-\\ncreases. Results consistently show that hallu-\\ncination risk reduces as sparsity levels increase,\\nacross all models and pruning methods. For ex-\\nample, with Llama-2 13B and Wanda, SummaCZS\\nHRR reduces from 0.98 at 10% sparsity, to 0.90 at\\n30% to finally 0.73 at 50%. Moreover, OPT-IML\\n30B displays a remarkably linear improvement\\n(i.e., with SparseGPT the HRR is 1.00 at 10%\\nsparsity, 0.95 at 30% and 0.90 at 50%, for all hal-\\nlucination risk metrics). These findings suggest\\nthat increasing sparsity to moderate levels (up to\\n50%) does indeed appear to reduce hallucination\\nrisk in generated summaries.\\nSemantic and Lexical Overlaps Differ. Ob-\\nserving the lexical (ROUGE) and semantic\\n1171\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 9, 'page_label': '1172'}, page_content='Figure 2: Ratio between a pruned model and the original across five sparsity levels, three hallucination risk metrics\\n(lines with circled markers; lower means pruned is better) and four summary generation performance metrics\\n(gray dotted lines; higher means pruned is better). The ratio for each metric is averaged across all datasets, with\\nerror bars indicating standard deviation.\\n(BERTScore) similarity ratios between document\\nand generated summary across sparsity levels,\\nthe outcomes are mixed. In almost all cases for\\nboth pruning methods, BERTScore results remain\\ncomparable to the original model (close to 1.0)\\nup to 50% sparsity, with minimal deviation across\\ndatasets. This shows that summaries from pruned\\nmodels are as semantically similar to the source\\ndocument as those from original models, across\\nall sparsity levels.\\nHowever, there is a stark contrast with\\nROUGE-1/2/L. For Llama-2 models, ROUGE-\\nbased ratios appear to decrease until 30% sparsity,\\nthen increase substantially and peak above 1.0\\n(the original model baseline) at 50% sparsity. For\\nMistral 7B and OPT-IML 30B, we observe that\\nROUGE-based ratios increase above 1.0 (higher\\nthan original) from a lower sparsity (20%). As\\nsummaries from pruned models remain as se-\\nmantically similar to the source document as\\nthose from original models, their higher lexical\\noverlap with the source document indicates that\\npruned models focus more on the input document\\nto generate a summary.\\nHigher Lexical Overlap, Lower Hallucination\\nRisk. Surprisingly, we observe an inversely\\nproportional relationship between ROUGE-based\\nratios and HRRs. We hypothesize that a higher\\nlexical overlap with the source document is a pos-\\nsible reason for the lower hallucination risk. To\\nROUGE-1/2/L\\nModel SparseGPT Wanda\\nLlama-2 7B −0.69 / −0.89 / −0.90 −0.45 / −0.86 / −0.79\\nLlama-2 13B −0.70 / −0.77 / −0.84 −0.72 / −0.78 / −0.85\\nLlama-2 70B −0.39 / −0.86 / −0.84 −0.69 / −0.86 / −0.86\\nMistral 7B −0.91 / −0.97 / −0.97 −0.88 / −0.96 / −0.97\\nOPT-IML 30B −0.70 / −0.93 / −0.89 −0.93 / −0.94 / −0.93\\nTable 7: Averaged Pearson’s correlation co-\\nefficient ( r) between hallucination risk and\\nROUGE-based metrics (calculated between the\\ngenerated summaries and the source documents)\\nacross sparsity levels. Bold values indicate\\nsignificant correlations (p< 0.05).\\nassess this, we calculate Pearson’s correlation co-\\nefficient, averaged across sparsity levels between\\nall HRR and ROUGE-based metrics (Table 7,\\nsignificant correlations in bold; p< 0.05).\\nWe note a strong significant inverse correlation\\n(Pearson’s r< −0.8) for both pruning methods\\nfor ROUGE-2/L across almost all models (exclud-\\ning Llama-2 13B) and r< −0.4 for ROUGE-1.\\nThis suggests that a higher lexical overlap could\\nbe responsible for the reduced hallucination risk,\\nwhile increasing sparsity appears responsible for\\nan increasing lexical overlap. In particular, we\\nfind an almost perfect negative relationship be-\\ntween ROUGE-based ratios and HRRs ( −0.97\\nwith SparseGPT) for Mistral 7B. This corrob-\\norates findings from the study by Durmus et al.\\n(2020), which shows that summaries with a higher\\n1172\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 10, 'page_label': '1173'}, page_content='Source Document Generated Summary\\nFactCC #205 Hallucination (Q1)\\nWhat do we have for the contestant on ‘‘The Price Is Right’’?\\nA brand-new car! Whoops. [ ...] model Manuela Arbelaez\\naccidentally revealed the correct answer [...]\\nThe contestant on ‘‘The Price Is Right’’ accidentally revealed the\\ncorrect answer to a guessing game for a new Hyundai Sonata\\n[ ...]\\nSummEval #86 Omission (Q2)\\n[ ...]P a t r i c kK l u i v e r th a sm a d eaw i n n i n gs t a r ti nWorld Cup\\nqualifying as coach [ ...]T h eC a r i b b e a ni s l a n dteam advanced\\nto the second qualifying round in CONCACAF for the 2018\\ntournam e n t[ ...]\\nP a t r i c kK l u i v e r t[ ...]h a ss uccessfully led Curacao to the second\\nqualifying round in CONCACAF for the 2018 tournam e n t .[ ...]\\nSummEval #33 Repetition (Q3)\\n[ ...]a u ction at Sotheby’s on Wednesday. While the 13 letters\\nwere expected to bring in anywhere from $300,000 to $600,000,\\n[ ...] estimated to bring in anywhere from $300,000 to $600,000,\\nb u t[ ...]\\n[ ...] The 13 letters were expected to bring in anywhere from\\n$300,000 to $600,000, but received no b i d s . [ ...]The letters were\\nestimated to be worth between $300,000 and $600,000. [. . . ]\\nFactCC #136 Alignment (Q4)\\nDeion Sanders is such a dad. The NFL legend called out Deion\\nSanders Jr. on Twitter for saying he only eats ‘‘hood doughnuts.’’\\nIn response, the elder Sanders [ ...]r e m i n d e dh i ss o nh eh a sa\\ntrust fund, a condo and his own clothing line called ‘‘Well Off’’\\n[ ...]\\nDeion Sanders Jr. is a Southern Methodist University wide re-\\nceiver and aspiring entrepreneur who loves ‘‘hood doughnuts’’\\nand has a trust fund, condo, and clothing line called ‘‘Well Off.’’\\n[ ...]\\nTable 8: Examples of errors in generated summaries corresponding to the human evaluation questions\\nand the context from the source document.\\nlexical similarity to the source document are less\\nlikely to contain hallucinations.\\n6 Qualitative Analysis\\nFollowing the human evaluation (see Sections 3.7\\nand 4.4), we review specific cases, highlighting\\nissues with the summaries generated by pruned\\nmodels in Table 8.\\nHallucinations. Our analysis of the human eval-\\nuation task results suggests that hallucinations in\\nthe summaries from both Llama-2 7B and Mistral\\n7B are either: (a) additional information not sup-\\nported by the source document, or (b) modified or\\nmisplaced information from the source document\\n(e.g., FactCC #205).\\nOmissions. Omission is a category where we\\nfound a few instances of disagreement be-\\ntween the participants. In general, participants\\nagree in clear cases like SummEval #86 (e.g.,\\n‘‘2018 tournament’’ should be ‘‘2018 World\\nCup’’). Comparatively in disagreements, omit-\\nted information is more nuanced and difficult to\\ndetect, such as important details from the source\\ndocument (e.g., missing dates).\\nRepetitions. Interestingly, we find that sum-\\nmaries containing repetitions occur when the\\nsource document also contains repeating informa-\\ntion (e.g., the price range ‘‘$300,000 to $600,000’’\\nduplicated in SummEval #33).\\nAlignment. The generated summaries that are\\nless aligned to the source document do not nec-\\nessarily contain any hallucinations, omissions,\\nor repetitions. However, we found that they do\\nnot entirely convey the original meaning of the\\nsource document. For example in FactCC #136,\\nthe source describes Deion Sanders Jr. being\\npublicly scolded by his father for downplaying\\nhis wealthy lifestyle. However, this particular\\npiece of information is not conveyed in the\\ngenerated summary.\\n7C o n c l u s i o n\\nWe conducted an extensive study to assess the\\nhallucination risk of LLMs after pruning. We\\nexperimented with two state-of-the-art pruning\\nmethods applied to five instruction-tuned LLMs.\\nWe measured the hallucination risk using three\\nestablished automatic metrics, in addition to a\\nhuman evaluation. Our results show that as\\nmodels are pruned to moderately high sparsity\\nlevels, the risk of generating hallucinating con-\\ntent decreases. Our analysis suggests that pruned\\nmodels tend to generate summaries that have a\\ngreater lexical overlap with the source docu-\\nment, offering a possible explanation for the lower\\nhallucination risk.\\nIn future work, we plan to explore the rela-\\ntionship between hallucination risk and model\\nquantization (Dettmers et al., 2022; Frantar et al.,\\n2023) and also expand to tasks such as open-book\\n1173\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 11, 'page_label': '1174'}, page_content='question answering (Ciosici et al., 2021) and\\nmachine translation (Guzm ´an et al., 2019). Fi-\\nnally, an interesting direction is to investigate the\\nrelationship between hallucination risk and expla-\\nnation faithfulness (Chrysostomou and Aletras,\\n2022; Zhao and Aletras, 2023).\\nAcknowledgments\\nWe would like to thank the anonymous review-\\ners and action editor for their invaluable feedback.\\nMW is supported by the Centre for Doctoral Train-\\ning in Speech and Language Technologies (SLT)\\nand their Applications funded by UK Research\\nand Innovation grant EP/S023062/1. ZZ and NA\\nare supported by EPSRC grant EP/V055712/1,\\npart of the European Commission CHIST-ERA\\nprogramme, call 2019 XAI: Explainable Machine\\nLearning-based Artificial Intelligence. NA is also\\nsupported by EPSRC grant EP/Y009800/1, part of\\nthe RAI UK Keystone projects.\\nReferences\\nGriffin Adams, Alex Fabbri, Faisal Ladhak,\\nEric Lehman, and No ´emie Elhadad. 2023.\\nFrom sparse to dense: GPT-4 summariza-\\ntion with chain of density prompting. In\\nProceedings of the 4th New Frontiers in\\nSummarization Workshop, pages 68–74, Singa-\\npore. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2023.newsum-1.7\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz\\nAlshamsi, Alessandro Cappelli, Ruxandra\\nCojocaru, M´erouane Debbah, ´Etienne Goffinet,\\nDaniel Hesslow, Julien Launay, Quentin\\nMalartic, Daniele Mazzotta, Badreddine Noune,\\nBaptiste Pannier, and Guilherme Penedo. 2023.\\nThe Falcon series of open language models.\\narXiv preprint, arXiv:2311.16867.\\nDavis Blalock, Jose Javier Gonzalez Ortiz,\\nJonathan Frankle, and John Guttag. 2020. What\\nis the state of neural network pruning? In Pro-\\nceedings of Machine Learning and Systems ,\\nvolume 2, pages 129–146.\\nMeng Cao, Yue Dong, Jiapeng Wu, and\\nJackie Chi Kit Cheung. 2020. Factual error\\ncorrection for abstractive summarization mod-\\nels. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 6251–6258, On-\\nline. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2020.emnlp-main.506\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li.\\n2018. Faithful to the original: Fact-aware neural\\nabstractive summarization. In Proceedings\\nof the Thirty-Second AAAI Conference on\\nArtificial Intelligence and Thirtieth Innova-\\ntive Applications of Artificial Intelligence\\nConference and Eighth AAAI Symposium on\\nEducational Advances in Artificial Intelligence,\\nAAAI’18/IAAI’18/EAAI’18. AAAI Press.\\nhttps://doi.org/10.1609/aaai.v32i1\\n.11912\\nXiuying Chen, Mingzhe Li, Xin Gao, and\\nXiangliang Zhang. 2022. Towards improv-\\ning faithfulness in abstractive summarization.\\nIn Advances in Neural Information Process-\\ning Systems, volume 35, pages 24516–24528.\\nCurran Associates, Inc.\\nPrafulla Kumar Choubey, Alex Fabbri, Jesse\\nVig, Chien-Sheng Wu, Wenhao Liu, and\\nNazneen Rajani. 2023. CaPE: Contrastive\\nparameter ensembling for reducing hallucina-\\ntion in abstractive summarization. In Findings\\nof the Association for Computational Lin-\\nguistics: ACL 2023 , pages 10755–10773,\\nToronto, Canada. Association for Computa-\\ntional Linguistics. https://doi.org/10\\n.18653/v1/2023.findings-acl.685\\nGeorge Chrysostomou and Nikolaos Aletras.\\n2022. An empirical study on explanations\\nin out-of-domain settings. In Proceedings of\\nthe 60th Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1:\\nLong Papers), pages 6920–6938, Dublin, Ire-\\nland. Association for Computational Linguis-\\ntics.https://doi.org/10.18653/v1\\n/2022.acl-long.477\\nManuel Ciosici, Joe Cecil, Dong-Ho Lee,\\nAlex Hedges, Marjorie Freedman, and Ralph\\nWeischedel. 2021. Perhaps PTLMs should\\ngo to school – a task to assess open\\nbook and closed book QA. In Proceed-\\nings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing ,\\npages 6104–6111, Online and Punta Cana,\\n1174\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 12, 'page_label': '1175'}, page_content='Dominican Republic. Association for Compu-\\ntational Linguistics.https://doi.org/10\\n.18653/v1/2021.emnlp-main.493\\nTrevor Cohn and Mirella Lapata. 2008. Sentence\\ncompression beyond word deletion. In Pro-\\nceedings of the 22nd International Conference\\non Computational Linguistics (Coling 2008),\\npages 137–144, Manchester, UK. Coling 2008\\nOrganizing Committee. https://doi.org\\n/10.3115/1599081.1599099\\nTim Dettmers, Mike Lewis, Younes Belkada, and\\nLuke Zettlemoyer. 2022. GPT3.int8(): 8-bit ma-\\ntrix multiplication for transformers at scale.\\nIn Advances in Neural Information Processing\\nSystems.\\nDaniel Deutsch, Tania Bedrax-Weiss, and Dan\\nRoth. 2021. Towards question-answering as\\nan automatic metric for evaluating the con-\\ntent quality of a summary. Transactions of\\nthe Association for Computational Linguistics,\\n9:774–789. https://doi.org/10.1162\\n/tacl_a_00397\\nChen Dun, Mirian Hipolito Garcia, Guoqing\\nZheng, Ahmed Hassan Awadallah, Anastasios\\nKyrillidis, and Robert Sim. 2023. Sweep-\\ning heterogeneity with smart mops: Mixture\\nof prompts for LLM task adaptation. arXiv\\npreprint, arXiv:2310.02842.\\nEsin Durmus, He He, and Mona Diab. 2020.\\nFEQA: A question answering evaluation\\nframework for faithfulness assessment in ab-\\nstractive summarization. In Proceedings of the\\n58th Annual Meeting of the Association for\\nComputational Linguistics, pages 5055–5070,\\nOnline. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2020.acl-main.454\\nYanai Elazar, Nora Kassner, Shauli Ravfogel,\\nAbhilasha Ravichander, Eduard Hovy,\\nHinrich Sch ¨utze, and Yoav Goldberg. 2021.\\nMeasuring and improving consistency in pre-\\ntrained language models. Transactions of\\nthe Association for Computational Linguis-\\ntics, 9:1012–1031. https://doi.org/10\\n.1162/tacl_a_00410\\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan\\nMcCann, Caiming Xiong, Richard Socher,\\nand Dragomir Radev. 2021. SummEval: Re-\\nevaluating summarization evaluation. Trans-\\nactions of the Association for Computational\\nLinguistics, 9:391–409. https://doi.org\\n/10.1162/tacla 00373\\nTobias Falke, Leonardo F. R. Ribeiro,\\nPrasetya Ajie Utama, Ido Dagan, and Iryna\\nGurevych. 2019. Ranking generated summaries\\nby correctness: An interesting but challenging\\napplication for natural language inference. In\\nProceedings of the 57th Annual Meeting of\\nthe Association for Computational Linguistics,\\npages 2214–2220, Florence, Italy. Associa-\\ntion for Computational Linguistics. https://\\ndoi.org/10.18653/v1/P19-1213\\nConstanza Fierro and Anders Søgaard. 2022.\\nFactual consistency of multilingual pre-\\ntrained language models. In Findings of the\\nAssociation for Computational Linguistics:\\nACL 2022 , pages 3046–3052, Dublin, Ire-\\nland. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2022.findings-acl.240\\nElias Frantar and Dan Alistarh. 2022. Optimal\\nbrain compression: A framework for accu-\\nrate post-training quantization and pruning. In\\nAdvances in Neural Information Processing\\nSystems, volume 35, pages 4475–4488. Curran\\nAssociates, Inc.\\nElias Frantar and Dan Alistarh. 2023. SparseGPT:\\nMassive language models can be accurately\\npruned in one-shot. In Proceedings of the\\n40th International Conference on Machine\\nLearning, volume 202 of Proceedings of Ma-\\nchine Learning Research, pages 10323–10337.\\nPMLR.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler,\\nand Dan Alistarh. 2023. OPTQ: Accurate quan-\\ntization for generative pre-trained transformers.\\nInThe Eleventh International Conference on\\nLearning Representations.\\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad\\nAli Khan, Yin Yang, Hassan Sajjad, Preslav\\nNakov, Deming Chen, and Marianne Winslett.\\n2021. Compressing large-scale transformer-\\nbased models: A case study on BERT.\\nTransactions of the Association for Compu-\\ntational Linguistics, 9:1061–1080. https://\\ndoi.org/10.1162/tacla 00413\\nNuno M. Guerreiro, Pierre Colombo, Pablo\\nPiantanida, and Andr ´e Martins. 2023. Opti-\\nmal transport for unsupervised hallucination\\n1175\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 13, 'page_label': '1176'}, page_content='detection in neural machine translation. In\\nProceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 13766–13784,\\nToronto, Canada. Association for Computa-\\ntional Linguistics. https://doi.org/10\\n.18653/v1/2023.acl-long.770\\nFrancisco Guzm ´an, Peng-Jen Chen, Myle Ott,\\nJuan Pino, Guillaume Lample, Philipp Koehn,\\nVishrav Chaudhary, and Marc’Aurelio\\nRanzato. 2019. The FLORES evaluation\\ndatasets for low-resource machine translation:\\nNepali–English and Sinhala–English. In Pro-\\nceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and\\nthe 9th International Joint Conference on Nat-\\nural Language Processing (EMNLP-IJCNLP),\\npages 6098–6111, Hong Kong, China. Associ-\\nation for Computational Linguistics. https://\\ndoi.org/10.18653/v1/D19-1632\\nMasafumi Hagiwara. 1994. A simple and effective\\nmethod for removal of hidden units and weights.\\nNeurocomputing, 6(2):207–218. Backpropa-\\ngation, Part IV. https://doi.org/10\\n.1016/0925-2312(94)90055-8\\nSong Han, Jeff Pool, John Tran, and William\\nDally. 2015. Learning both weights and connec-\\ntions for efficient neural network. In Advances\\nin Neural Information Processing Systems ,\\nvolume 28. Curran Associates, Inc.\\nAdib Hasan, Ileana Rugina, and Alex Wang. 2024.\\nPruning for protection: Increasing jailbreak re-\\nsistance in aligned LLMs without fine-tuning.\\narXiv preprint, arXiv:2401.10862.\\nB. Hassibi, D. G. Stork, and G. J. Wolff. 1993.\\nOptimal brain surgeon and general net-\\nwork pruning. In IEEE International Con-\\nference on Neural Networks , pages 293–299\\nvol. 1. https://doi.org/10.1109/ICNN\\n.1993.298572\\nDandan Huang, Leyang Cui, Sen Yang,\\nGuangsheng Bao, Kun Wang, Jun Xie, and\\nYue Zhang. 2020. What have we achieved\\non text summarization? InProceedings of\\nthe 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP) ,\\npages 446–469, Online. Association for Com-\\nputational Linguistics. https://doi.org\\n/10.18653/v1/2020.emnlp-main.33\\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth\\nPasunuru, Todor Mihaylov, Daniel Simig, Ping\\nYu, Kurt Shuster, Tianlu Wang, Qing Liu,\\nPunit Singh Koura, Xian Li, Brian O’Horo,\\nGabriel Pereyra, Jeff Wang, Christopher\\nDewan, Asli Celikyilmaz, Luke Zettlemoyer,\\nand Ves Stoyanov. 2023. OPT-IML: Scal-\\ning language model instruction meta learning\\nthrough the lens of generalization. arXiv\\npreprint, arXiv:2212.12017.\\nAjay Kumar Jaiswal, Zhe Gan, Xianzhi Du,\\nBowen Zhang, Zhangyang Wang, and Yinfei\\nYang. 2024. Compressing LLMs: The truth\\nis rarely pure and never simple. In The\\nTwelfth International Conference on Learning\\nRepresentations.\\nZiwei Ji, Tiezheng Yu, Yan Xu, Nayeon\\nLee, Etsuko Ishii, and Pascale Fung. 2023.\\nTowards mitigating LLM hallucination via\\nself reflection. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2023,\\npages 1827–1843, Singapore. Association for\\nComputational Linguistics.https://doi.org\\n/10.18653/v1/2023.findings-emnlp.123\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur\\nMensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand,\\nGianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, L ´elio Renard Lavaud, Marie-Anne\\nLachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timoth ´ee Lacroix, and\\nWilliam El Sayed. 2023. Mistral 7B. arXiv\\npreprint, arXiv:2310.06825.\\nDaniel King, Zejiang Shen, Nishant Subramani,\\nDaniel S. Weld, Iz Beltagy, and Doug Downey.\\n2022. Don’t say what you don’t know:\\nImproving the consistency of abstractive sum-\\nmarization by constraining beam search. In\\nProceedings of the 2nd Workshop on Natural\\nLanguage Generation, Evaluation, and Metrics\\n(GEM), pages 555–571, Abu Dhabi, United\\nArab Emirates (Hybrid). Association for Com-\\nputational Linguistics. https://doi.org\\n/10.18653/v1/2022.gem-1.51\\nWojciech Kryscinski, Bryan McCann, Caiming\\nXiong, and Richard Socher. 2020. Evaluating\\nthe factual consistency of abstractive text\\nsummarization. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP), pages 9332–9346,\\n1176\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 14, 'page_label': '1177'}, page_content='Online. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2020.emnlp-main.750\\nPhilippe Laban, Wojciech Kryscinski, Divyansh\\nAgarwal, Alexander Fabbri, Caiming Xiong,\\nShafiq Joty, and Chien-Sheng Wu. 2023.\\nSummEdits: Measuring LLM ability at factual\\nreasoning through the lens of summarization.\\nIn Proceedings of the 2023 Conference on Em-\\npirical Methods in Natural Language Process-\\ning, pages 9662–9676, Singapore. Association\\nfor Computational Linguistics. https://doi\\n.org/10.18653/v1/2023.emnlp-main.600\\nPhilippe Laban, Tobias Schnabel, Paul N.\\nBennett, and Marti A. Hearst. 2022. SummaC:\\nRe-visiting NLI-based models for inconsistency\\ndetection in summarization. Transactions of\\nthe Association for Computational Linguis-\\ntics, 10:163–177. https://doi.org/10\\n.1162/tacl_a_00453\\nMateusz Lango and Ondrej Dusek. 2023.\\nCritic-driven decoding for mitigating hal-\\nlucinations in data-to-text generation. In\\nProceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language\\nProcessing, pages 2853–2862, Singapore.\\nAssociation for Computational Linguistics.\\nhttps://doi.org/10.18653/v1/2023\\n.emnlp-main.172\\nMd Tahmid Rahman Laskar, Xue-Yong Fu,\\nCheng Chen, and Shashi Bhushan TN. 2023.\\nBuilding real-world meeting summarization\\nsystems using large language models: A prac-\\ntical perspective. In Proceedings of the 2023\\nConference on Empirical Methods in Nat-\\nural Language Processing: Industry Track ,\\npages 343–352, Singapore. Association for\\nComputational Linguistics.https://doi.org\\n/10.18653/v1/2023.emnlp-industry.33\\nYann LeCun, John Denker, and Sara Solla. 1989.\\nOptimal brain damage. In Advances in Neu-\\nral Information Processing Systems, volume 2.\\nMorgan-Kaufmann.\\nChin-Yew Lin. 2004. ROUGE: A package for\\nautomatic evaluation of summaries. In Text\\nSummarization Branches Out , pages 74–81,\\nBarcelona, Spain. Association for Computa-\\ntional Linguistics.\\nHui Lin and Vincent Ng. 2019. Abstractive\\nsummarization: A survey of the state of the\\nart. Proceedings of the AAAI Conference\\non Artificial Intelligence , 33(01):9815–9822.\\nhttps://doi.org/10.1609/aaai.v33i01\\n.33019815\\nXinyin Ma, Gongfan Fang, and Xinchao Wang.\\n2023. LLM-Pruner: On the structural pruning\\nof large language models. In Thirty-seventh\\nConference on Neural Information Processing\\nSystems.\\nLaura Manor and Junyi Jessy Li. 2019. Plain\\nEnglish summarization of contracts. In Pro-\\nceedings of the Natural Legal Language\\nProcessing Workshop 2019, pages 1–11, Min-\\nneapolis, Minnesota. Association for Computa-\\ntional Linguistics. https://doi.org/10\\n.18653/v1/W19-2201\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithful-\\nness and factuality in abstractive summa-\\nrization. In Proceedings of the 58th Annual\\nMeeting of the Association for Computa-\\ntional Linguistics , pages 1906–1919, On-\\nline. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2020.acl-main.173\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2017. Pointer sentinel mix-\\nture models. In International Conference on\\nLearning Representations.\\nKirill Milintsevich and Navneet Agarwal. 2023.\\nCalvados at MEDIQA-chat 2023: Improv-\\ning clinical note generation with multi-task\\ninstruction finetuning. In Proceedings of\\nthe 5th Clinical Natural Language Pro-\\ncessing Workshop , pages 529–535, Toronto,\\nCanada. Association for Computational Lin-\\nguistics. https://doi.org/10.18653\\n/v1/2023.clinicalnlp-1.56\\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool,\\nDarko Stosic, Dusan Stosic, Ganesh Venkatesh,\\nChong Yu, and Paulius Micikevicius. 2021. Ac-\\ncelerating sparse deep neural networks. arXiv\\npreprint, arXiv:2104.08378.\\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\\nChristos Louizos, and Tijmen Blankevoort.\\n2020. Up or down? Adaptive rounding for\\npost-training quantization. In Proceedings of\\n1177\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 15, 'page_label': '1178'}, page_content='the 37th International Conference on Machine\\nLearning, volume 119 of Proceedings of Ma-\\nchine Learning Research , pages 7197–7206.\\nPMLR.\\nRamesh Nallapati, Bowen Zhou, Cicero dos\\nSantos, C¸a ˘glar Gulc¸ehre, and Bing Xiang.\\n2016. Abstractive text summarization using\\nsequence-to-sequence RNNs and beyond. In\\nProceedings of the 20th SIGNLL Conference\\non Computational Natural Language Learning,\\npages 280–290, Berlin, Germany. Associa-\\ntion for Computational Linguistics. https://\\ndoi.org/10.18653/v1/K16-1028\\nSatya Sai Srinath Namburi, Makesh Sreedhar, Sri-\\nnath Srinivasan, and Frederic Sala. 2023. The\\ncost of compression: Investigating the impact\\nof compression on parametric knowledge in\\nlanguage models. InFindings of the Association\\nfor Computational Linguistics: EMNLP 2023,\\npages 5255–5273, Singapore. Association for\\nComputational Linguistics.https://doi.org\\n/10.18653/v1/2023.findings-emnlp.349\\nShashi Narayan, Joshua Maynez, Reinald Kim\\nAmplayo, Kuzman Ganchev, Annie Louis,\\nFantine Huot, Anders Sandholm, Dipanjan Das,\\nand Mirella Lapata. 2023. Conditional gen-\\neration with a question-answering blueprint.\\nTransactions of the Association for Compu-\\ntational Linguistics , 11:974–996. https://\\ndoi.org/10.1162/tacla 00583\\nNeural Magic. 2021. DeepSparse.\\nOpenAI, Josh Achiam, Steven Adler, Sandhini\\nAgarwal, Lama Ahmad, Ilge Akkaya,\\nFlorencia Leoni Aleman, Diogo Almeida, Janko\\nAltenschmidt, Sam Altman, Shyamal Anadkat,\\nRed Avila, Igor Babuschkin, Suchir Balaji,\\nValerie Balcom, Paul Baltescu, Haiming Bao,\\nMohammad Bavarian, Jeff Belgum, Irwan\\nBello, Jake Berdine, Gabriel Bernadett-Shapiro,\\nChristopher Berner, Lenny Bogdonoff, Oleg\\nBoiko, Madelaine Boyd, Anna-Luisa Brakman,\\nGreg Brockman, Tim Brooks, Miles Brundage,\\nKevin Button, Trevor Cai, Rosie Campbell,\\nAndrew Cann, Brittany Carey, Chelsea Carlson,\\nRory Carmichael, Brooke Chan, Che Chang,\\nFotis Chantzis, Derek Chen, Sully Chen, Ruby\\nChen, Jason Chen, Mark Chen, Ben Chess,\\nChester Cho, Casey Chu, Hyung Won Chung,\\nDave Cummings, Jeremiah Currier, Yunxing\\nDai, Cory Decareaux, Thomas Degry, Noah\\nDeutsch, Damien Deville, Arka Dhar, David\\nDohan, Steve Dowling, Sheila Dunning, Adrien\\nEcoffet, Atty Eleti, Tyna Eloundou, David\\nFarhi, Liam Fedus, Niko Felix, Sim ´on Posada\\nFishman, Juston Forte, Isabella Fulford, Leo\\nGao, Elie Georges, Christian Gibson, Vik\\nGoel, Tarun Gogineni, Gabriel Goh, Rapha\\nGontijo-Lopes, Jonathan Gordon, Morgan\\nGrafstein, Scott Gray, Ryan Greene, Joshua\\nGross, Shixiang Shane Gu, Yufei Guo, Chris\\nHallacy, Jesse Han, Jeff Harris, Yuchen He,\\nMike Heaton, Johannes Heidecke, Chris Hesse,\\nAlan Hickey, Wade Hickey, Peter Hoeschele,\\nBrandon Houghton, Kenny Hsu, Shengli Hu,\\nXin Hu, Joost Huizinga, Shantanu Jain, Shawn\\nJain, Joanne Jang, Angela Jiang, Roger Jiang,\\nHaozhun Jin, Denny Jin, Shino Jomoto, Billie\\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz\\nKaiser, Ali Kamali, Ingmar Kanitscheider,\\nNitish Shirish Keskar, Tabarak Khan, Logan\\nKilpatrick, Jong Wook Kim, Christina\\nKim, Yongjik Kim, Jan Hendrik Kirchner,\\nJamie Kiros, Matt Knight, Daniel Kokotajlo,\\nŁukasz Kondraciuk, Andrew Kondrich, Aris\\nKonstantinidis, Kyle Kosic, Gretchen Krueger,\\nVishal Kuo, Michael Lampe, Ikai Lan, Teddy\\nLee, Jan Leike, Jade Leung, Daniel Levy,\\nChak Ming Li, Rachel Lim, Molly Lin,\\nStephanie Lin, Mateusz Litwin, Theresa Lopez,\\nRyan Lowe, Patricia Lue, Anna Makanju,\\nKim Malfacini, Sam Manning, Todor Markov,\\nYaniv Markovski, Bianca Martin, Katie Mayer,\\nAndrew Mayne, Bob McGrew, Scott Mayer\\nMcKinney, Christine McLeavey, Paul McMillan,\\nJake McNeil, David Medina, Aalok Mehta,\\nJacob Menick, Luke Metz, Andrey Mishchenko,\\nPamela Mishkin, Vinnie Monaco, Evan\\nMorikawa, Daniel Mossing, Tong Mu, Mira\\nMurati, Oleg Murk, David M ´ely, Ashvin\\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind\\nNeelakantan, Richard Ngo, Hyeonwoo Noh,\\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki,\\nAlex Paino, Joe Palermo, Ashley Pantuliano,\\nGiambattista Parascandolo, Joel Parish, Emy\\nParparita, Alex Passos, Mikhail Pavlov, Andrew\\nPeng, Adam Perelman, Filipe de Avila Belbute\\nPeres, Michael Petrov, Henrique Ponde de\\nOliveira Pinto, Michael Pokorny, Michelle\\nPokrass, Vitchyr H. Pong, Tolly Powell,\\nAlethea Power, Boris Power, Elizabeth Proehl,\\nRaul Puri, Alec Radford, Jack Rae, Aditya\\n1178\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 16, 'page_label': '1179'}, page_content='Ramesh, Cameron Raymond, Francis Real,\\nKendra Rimbach, Carl Ross, Bob Rotsted,\\nHenri Roussez, Nick Ryder, Mario Saltarelli,\\nTed Sanders, Shibani Santurkar, Girish Sastry,\\nHeather Schmidt, David Schnurr, John\\nSchulman, Daniel Selsam, Kyla Sheppard,\\nToki Sherbakov, Jessica Shieh, Sarah Shoker,\\nPranav Shyam, Szymon Sidor, Eric Sigler,\\nMaddie Simens, Jordan Sitkin, Katarina Slama,\\nIan Sohl, Benjamin Sokolowsky, Yang Song,\\nNatalie Staudacher, Felipe Petroski Such,\\nNatalie Summers, Ilya Sutskever, Jie Tang,\\nNikolas Tezak, Madeleine B. Thompson, Phil\\nTillet, Amin Tootoonchian, Elizabeth Tseng,\\nPreston Tuggle, Nick Turley, Jerry Tworek,\\nJuan Felipe Cer´on Uribe, Andrea Vallone, Arun\\nVijayvergiya, Chelsea Voss, Carroll Wainwright,\\nJustin Jay Wang, Alvin Wang, Ben Wang,\\nJonathan Ward, Jason Wei, C. J. Weinmann,\\nAkila Welihinda, Peter Welinder, Jiayi Weng,\\nLilian Weng, Matt Wiethoff, Dave Willner,\\nClemens Winter, Samuel Wolrich, Hannah\\nWong, Lauren Workman, Sherwin Wu, Jeff\\nWu, Michael Wu, Kai Xiao, Tao Xu, Sarah\\nYoo, Kevin Yu, Qiming Yuan, Wojciech\\nZaremba, Rowan Zellers, Chong Zhang, Marvin\\nZhang, Shengjia Zhao, Tianhao Zheng, Juntang\\nZhuang, William Zhuk, and Barret Zoph.\\n2024. GPT-4 technical report. arXiv preprint,\\narXiv:2303.08774.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo\\nAlmeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina\\nSlama, Alex Ray, John Schulman, Jacob Hilton,\\nFraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul\\nChristiano, Jan Leike, Ryan Lowe. 2022. Train-\\ning language models to follow instructions with\\nhuman feedback. Advances in Neural Informa-\\ntion Processing Systems, 35:27730–27744.\\nFabio Petroni, Tim Rockt¨aschel, Sebastian Riedel,\\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. 2019. Language models\\nas knowledge bases? InProceedings of the 2019\\nConference on Empirical Methods in Natural\\nLanguage Processing and the 9th International\\nJoint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 2463–2473,\\nHong Kong, China. Association for Computa-\\ntional Linguistics. https://doi.org/10\\n.18653/v1/D19-1250\\nColin Raffel, Noam Shazeer, Adam Roberts,\\nKatherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J.\\nLiu. 2020. Exploring the limits of transfer\\nlearning with a unified text-to-text transformer.\\nThe Journal of Machine Learning Research ,\\n21(1):5485–5551.\\nVikas Raunak, Arul Menezes, and Marcin\\nJunczys-Dowmunt. 2021. The curious case of\\nhallucinations in neural machine translation.\\nIn Proceedings of the 2021 Conference of\\nthe North American Chapter of the Associ-\\nation for Computational Linguistics: Human\\nLanguage Technologies , pages 1172–1183,\\nOnline. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2021.naacl-main.92\\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee\\nBurns, Trevor Darrell, and Kate Saenko. 2018.\\nObject hallucination in image captioning. In\\nProceedings of the 2018 Conference on Empir-\\nical Methods in Natural Language Processing,\\npages 4035–4045, Brussels, Belgium. Associa-\\ntion for Computational Linguistics. https://\\ndoi.org/10.18653/v1/D18-1437\\nHoracio Saggion and Thierry Poibeau. 2013.\\nAutomatic Text Summarization: Past, Present\\nand Future, pages 3–21. Springer Berlin Heidel-\\nberg, Berlin, Heidelberg. https://doi.org\\n/10.1007/978-3-642-28569-11\\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and\\nOren Etzioni. 2020. Green AI.Communications\\nof the ACM , 63(12):54–63. https://doi\\n.org/10.1145/3381831\\nSeonil (Simon) Son, Junsoo Park, Jeong-in\\nHwang, Junghwa Lee, Hyungjong Noh, and\\nYeonsoo Lee. 2022. HaRiM +: Evaluating\\nsummary quality with hallucination risk. In\\nProceedings of the 2nd Conference of the\\nAsia-Pacific Chapter of the Association for\\nComputational Linguistics and the 12th In-\\nternational Joint Conference on Natural Lan-\\nguage Processing (Volume 1: Long Papers) ,\\npages 895–924, Online only. Association for\\nComputational Linguistics.\\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico\\nKolter. 2024. A simple and effective pruning\\napproach for large language models. In The\\n1179\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 17, 'page_label': '1180'}, page_content='Twelfth International Conference on Learning\\nRepresentations.\\nLiyan Tang, Tanya Goyal, Alex Fabbri, Philippe\\nLaban, Jiacheng Xu, Semih Yavuz, Wojciech\\nKryscinski, Justin Rousseau, and Greg Durrett.\\n2023a. Understanding factual errors in sum-\\nmarization: Errors, summarizers, datasets,\\nerror detectors. In Proceedings of the\\n61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1:\\nLong Papers), pages 11626–11644, Toronto,\\nCanada. Association for Computational Lin-\\nguistics. https://doi.org/10.18653\\n/v1/2023.acl-long.650\\nYuting Tang, Ratish Puduppully, Zhengyuan Liu,\\nand Nancy Chen. 2023b. In-context learning\\nof large language models for controlled di-\\nalogue summarization: A holistic benchmark\\nand empirical analysis. In Proceedings of the\\n4th New Frontiers in Summarization Workshop,\\npages 56–67, Singapore. Association for Com-\\nputational Linguistics. https://doi.org\\n/10.18653/v1/2023.newsum-1.6\\nHugo Touvron, Louis Martin, Kevin Stone,\\nPeter Albert, Amjad Almahairi, Yasmine\\nBabaei, Nikolay Bashlykov, Soumya Batra,\\nPrajjwal Bhargava, Shruti Bhosale, Dan Bikel,\\nLukas Blecher, Cristian Canton Ferrer, Moya\\nChen, Guillem Cucurull, David Esiobu, Jude\\nFernandes, Jeremy Fu, Wenyin Fu, Brian\\nFuller, Cynthia Gao, Vedanuj Goswami,\\nNaman Goyal, Anthony Hartshorn, Saghar\\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas,\\nViktor Kerkez, Madian Khabsa, Isabel\\nKloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya\\nLee, Diana Liskovich, Yinghai Lu, Yuning\\nMao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie,\\nAndrew Poulton, Jeremy Reizenstein, Rashi\\nRungta, Kalyan Saladi, Alan Schelten, Ruan\\nSilva, Eric Michael Smith, Ranjan Subramanian,\\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor,\\nAdina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang,\\nAngela Fan, Melanie Kambadur, Sharan\\nNarang, Aurelien Rodriguez, Robert Stojnic,\\nSergey Edunov, and Thomas Scialom. 2023.\\nLlama 2: Open foundation and fine-tuned chat\\nmodels. arXiv preprint, arXiv:2307.09288.\\nOriol Vinyals and Quoc Le. 2015. A neu-\\nral conversational model. arXiv preprint ,\\narXiv:1506.05869.\\nByron C. Wallace, Sayantan Saha, Frank\\nSoboczenski, and Iain J. Marshall. 2021. Gen-\\nerating (factual?) narrative summaries of RCTs:\\nExperiments with neural multi-document sum-\\nmarization. AMIA Summits on Translational\\nScience Proceedings, 2021:605.\\nAlex Wang, Kyunghyun Cho, and Mike Lewis.\\n2020. Asking and answering questions to\\nevaluate the factual consistency of sum-\\nmaries. In Proceedings of the 58th Annual\\nMeeting of the Association for Computa-\\ntional Linguistics, pages 5008–5020, Online.\\nAssociation for Computational Linguistics.\\nhttps://doi.org/10.18653/v1/2020\\n.acl-main.450\\nLaura Weidinger, Jonathan Uesato, Maribeth\\nRauh, Conor Griffin, Po-Sen Huang, John\\nMellor, Amelia Glaese, Myra Cheng, Borja\\nBalle, Atoosa Kasirzadeh, Courtney Biles,\\nSasha Brown, Zac Kenton, Will Hawkins,\\nTom Stepleton, Abeba Birhane, Lisa Anne\\nHendricks, Laura Rimell, William Isaac, Julia\\nHaas, Sean Legassick, Geoffrey Irving, and\\nIason Gabriel. 2022. Taxonomy of risks posed\\nby language models. InProceedings of the 2022\\nACM Conference on Fairness, Accountability,\\nand Transparency, FAccT ’22, pages 214–229,\\nNew York, NY, USA. Association for Com-\\nputing Machinery. https://doi.org/10\\n.1145/3531146.3533088\\nMiles Williams and Nikolaos Aletras. 2023. How\\ndoes calibration data affect the post-training\\npruning and quantization of large language\\nmodels? arXiv preprint, arXiv:2311.09755.\\nThomas Wolf, Lysandre Debut, Victor Sanh,\\nJulien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Remi Louf,\\nMorgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite,\\nJulien Plu, Canwen Xu, Teven Le Scao,\\nSylvain Gugger, Mariama Drame, Quentin\\nLhoest, and Alexander Rush. 2020. Trans-\\nformers: State-of-the-art natural language\\nprocessing. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural\\nLanguage Processing: System Demonstrations,\\n1180\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iTextSharp 4.1.6 by 1T3XT', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-09-16T21:29:22+08:00', 'title': '', 'keywords': '', 'author': '', 'subject': '', 'moddate': '2025-01-26T02:11:59+00:00', 'source': '/content/drive/My Drive/documents_RAG/tacl_a_00695.pdf', 'total_pages': 19, 'page': 18, 'page_label': '1181'}, page_content='pages 38–45, Online. Association for Compu-\\ntational Linguistics.https://doi.org/10\\n.18653/v1/2020.emnlp-demos.6\\nYijun Xiao and William Yang Wang. 2021.\\nOn hallucination and predictive uncertainty in\\nconditional language generation. In Proceed-\\nings of the 16th Conference of the European\\nChapter of the Association for Computational\\nLinguistics: Main Volume, pages 2734–2744,\\nOnline. Association for Computational Linguis-\\ntics. https://doi.org/10.18653/v1\\n/2021.eacl-main.236\\nGuangxuan Xu and Qingyuan Hu. 2022. Can\\nmodel compression improve NLP fairness.\\narXiv preprint, arXiv:2201.08542.\\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou,\\nMarianna J. Martindale, and Marine Carpuat.\\n2023. Understanding and detecting hal-\\nlucinations in neural machine translation\\nvia model introspection. Transactions of\\nthe Association for Computational Linguis-\\ntics, 11:546–564. https://doi.org/10\\n.1162/tacl_a_00563\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu.\\n2021. BARTScore: Evaluating generated text\\nas text generation. In Advances in Neural\\nInformation Processing Systems , volume 34,\\npages 27263–27277. Curran Associates, Inc.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian\\nQ. Weinberger, and Yoav Artzi. 2020.\\nBERTScore: Evaluating text generation with\\nBERT. In International Conference on Learn-\\ning Representations.\\nTianyi Zhang, Faisal Ladhak, Esin Durmus,\\nPercy Liang, Kathleen McKeown, and\\nTatsunori B. Hashimoto. 2024. Benchmarking\\nLarge Language Models for News Summariza-\\ntion. Transactions of the Association for Com-\\nputational Linguistics, 12:39–57. https://\\ndoi.org/10.1162/tacl a 00632\\nZhixue Zhao and Nikolaos Aletras. 2023.\\nIncorporating attribution importance for im-\\nproving faithfulness metrics. In Proceedings\\nof the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1:\\nLong Papers) , pages 4732–4745, Toronto,\\nCanada. Association for Computational Lin-\\nguistics. https://doi.org/10.18653\\n/v1/2023.acl-long.261\\nZhixue Zhao, George Chrysostomou, Kalina\\nBontcheva, and Nikolaos Aletras. 2022a. On\\nthe impact of temporal concept drift on model\\nexplanations. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2022,\\npages 4039–4054, Abu Dhabi, United Arab\\nEmirates. Association for Computational Lin-\\nguistics. https://doi.org/10.18653\\n/v1/2022.findings-emnlp.298\\nZheng Zhao, Shay B. Cohen, and Bonnie Webber.\\n2020. Reducing quantity hallucinations in\\nabstractive summarization. In Findings of the\\nAssociation for Computational Linguistics:\\nEMNLP 2020 , pages 2237–2249, Online.\\nAssociation for Computational Linguistics.\\nhttps://doi.org/10.18653/v1/2020\\n.findings-emnlp.203\\nZhixue Zhao and Boxuan Shan. 2024. Reagent:\\nA model-agnostic feature attribution method\\nfor generative language models.arXiv preprint,\\narXiv:2402.00794. https://doi.org/10\\n.22541/au.170709121.16176681/v1\\nZhixue Zhao, Ziqi Zhang, and Frank Hopfgartner.\\n2022b. Utilizing subjectivity level to mitigate\\nidentity term bias in toxic comments classi-\\nfication. Online Social Networks and Media ,\\n29:100205. https://doi.org/10.1016\\n/j.osnem.2022.100205\\nChunting Zhou, Graham Neubig, Jiatao Gu,\\nMona Diab, Francisco Guzm ´an, Luke\\nZettlemoyer, and Marjan Ghazvininejad. 2021.\\nDetecting hallucinated content in conditional\\nneural sequence generation. In Findings of\\nthe Association for Computational Linguis-\\ntics: ACL-IJCNLP 2021 , pages 1393–1404,\\nOnline. Association for Computational Lin-\\nguistics. https://doi.org/10.18653/v1\\n/2021.findings-acl.120\\n1181\\nDownloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00695/2470787/tacl_a_00695.pdf by guest on 26 January 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 0, 'page_label': '2758'}, page_content='Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2758–2774\\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\\nSources of Hallucination by Large Language Models on Inference Tasks\\nNick McKenna†* Tianyi Li †*\\nLiang Cheng† Mohammad Javad Hosseini‡ Mark Johnson§ Mark Steedman†\\n†University of Edinburgh ‡Google Research §Macquarie University\\n{nick.mckenna, tianyi.li}@ed.ac.uk\\nAbstract\\nLarge Language Models (LLMs) are claimed\\nto be capable of Natural Language Inference\\n(NLI), necessary for applied tasks like question\\nanswering and summarization. We present a\\nseries of behavioral studies on several LLM\\nfamilies (LLaMA, GPT-3.5, and PaLM) which\\nprobe their behavior using controlled exper-\\niments. We establish two biases originating\\nfrom pretraining which predict much of their\\nbehavior, and show that these are major sources\\nof hallucination in generative LLMs. First,\\nmemorization at the level of sentences: we\\nshow that, regardless of the premise, models\\nfalsely label NLI test samples as entailing when\\nthe hypothesis is attested in training data, and\\nthat entities are used as “indices” to access the\\nmemorized data. Second, statistical patterns\\nof usage learned at the level of corpora: we\\nfurther show a similar effect when the premise\\npredicate is less frequent than that of the hy-\\npothesis in the training data, a bias following\\nfrom previous studies. We demonstrate that\\nLLMs perform significantly worse on NLI test\\nsamples which do not conform to these biases\\nthan those which do, and we offer these as valu-\\nable controls for future LLM evaluation.1\\n1 Introduction\\nLarge Language Models (LLMs) such as LLaMA,\\nGPT-3/4, PaLM, and more (Touvron et al., 2023;\\nBrown et al., 2020; Chowdhery et al., 2022), have\\nbeen trusted by many to perform language un-\\nderstanding in downstream tasks such as summa-\\nrization, question answering, and fact verification,\\namong others (Zhang et al., 2023). However, due\\nto the large-scale nature of LLM training on vast,\\noften proprietary data, and the inherent opacity\\nof LLM parameters, it is difficult to explain their\\n*Equal contribution.\\n1Code and LLM outputs (LLaMA and GPT-3.5)\\nare available at https://github.com/Teddy-Li/\\nLLM-NLI-Analysis.\\nbehavior when answering user queries and the cor-\\nresponding risks in terms of bias and robustness.\\nIn particular, one LLM behavior poses a signifi-\\ncant challenge: “hallucination,” the phenomenon\\nin which LLMs provide information which is in-\\ncorrect or inappropriate, presented as fact.\\nThis paper investigates two biases driving LLM\\nperformance in natural language inference, some-\\ntimes called textual entailment. This is a basic com-\\nponent of language understanding which is critical\\nin applied tasks, and we offer these two biases as\\nexplanations of general false positive hallucination\\nin everyday use. We examine broader NLI, and\\nespecially directional entailments, which hold in\\none direction, but not both. For example, DEFEAT\\nentails PLAY but PLAY does not entail DEFEAT . In-\\nferring directional entailment is more difficult than\\nthat of symmetric paraphrase, so it more deeply\\nprobes understanding.\\nOur approach is a behavioral study of prompted\\nLLM decision-making. We alter existing NLI\\ndatasets in targeted ways while measuring how pre-\\ndictions change, across several major LLM families\\n(LLaMA, GPT-3.5, and PaLM). We demonstrate\\ntwo sources of LLM performance on the NLI task,\\nwhich we offer as explanations of general false pos-\\nitive hallucination: (1) LLM bias toward affirming\\nentailment when the hypothesis may be attested in\\nthe training text, including reliance on named entity\\nidentifiers; and (2) a corpus-frequency bias, affirm-\\ning entailment when the premise is less frequent\\nthan the hypothesis.\\nWe establish that these biases originate from\\nthe LLM pretraining objective, in which statisti-\\ncal modeling of the natural distribution of human-\\ngenerated text leads to (at the level of sentences)\\nmemorizing individual statements, and (at the\\nlevel of corpora) learning typical patterns of us-\\nage. Though they are superficially performant, our\\nexperiments show that even powerful LLMs still\\nuse unsatisfactory tools instead of robust reasoning.\\n2758'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2759'}, page_content='We present three contributions, the demonstra-\\ntions of both factors and an analysis of their impact:\\n(1) In a prompting scenario, LLMs respond to\\nentailment samples according to anattestation bias,\\naffirming entailments more readily if the hypoth-\\nesis is attested by the pretraining text. We find\\nthat LLaMA-65B, GPT-3.5, and PaLM-540B are\\nrespectively 1.9, 2.2, and 2.0 times more likely\\nto wrongly predict Entail when the model al-\\nready asserts the hypothesis is attested, compared\\nto when not attested. Further, LLMs recall from\\ntheir propositional memory using named entities as\\nidentifying “indices,” even though they are irrele-\\nvant to the logic of the predicate inference task.\\n(2) LLMs also rely on a simple corpus-statistic\\nbias using relative term-frequencies, especially\\nwhen propositional memory is not available. The\\nthree LLMs are 1.6, 1.8 and 2.0 times more likely to\\nwrongly affirm entailments if the premise has lower\\nterm frequency than the hypothesis, than when not.\\n(3) For the NLI test samples consistent with\\nthese factors, LLM scores are misleadingly high;\\nfor NLI samples adversarial to them, LLM perfor-\\nmance is severely degraded. We show that when\\nlabels go against the attestation bias, LLMs can be\\npoor or even near-random classifiers; for the rela-\\ntive frequency bias, we similarly show a substantial\\nperformance decrease across all LLMs.\\n2 Related Work\\nAddressing task robustness, Poliak et al. (2018)\\nfound a range of NLI datasets contain artifacts\\nwhich are learned by supervised models trained\\non only sample hypotheses. In our work we design\\na similar hypothesis-only test with LLMs, but we\\nuse it to probe model memory without any training.\\nBy conditioning on the attestation of hypotheses,\\nwe show that LLMs are inherently sensitive to attes-\\ntation, separate from the statistical idiosyncrasies\\nof NLI datasets.2\\nAdditionally, Talman and Chatzikyriakidis\\n(2019) report generalization failure among many\\nmodels supervised for NLI — models fail to gen-\\neralize between NLI datasets, even if the task is\\nformatted the same. On smaller Language Models\\nsuch as RoBERTa (Liu et al., 2019; 355M params),\\nLi et al. (2022) also observed a reliance on dataset\\n2We speculate that a similar attestation effect could even\\nbe present in the supervised models studied in Poliak et al.\\n(2018), which could contribute to those models’ performance.\\nWe leave the investigation of this to future work.\\nartifacts when performing directional NLI on pred-\\nicates. We now study the behavior of much larger\\nLMs, which have demonstrated more robust perfor-\\nmance across NLP tasks.\\nRecent work has also explored LLM memoriza-\\ntion and generalization. Carlini et al. (2023) estab-\\nlish that LLMs are able to memorize more data than\\nsmall LMs, whereas Tirumala et al. (2022) further\\ndemonstrate that LLMs pay special attention early\\nin training to numbers and nouns, which act as\\nunique identifiers for individual training sentences.\\nWe also show that memories used in language infer-\\nence are tied to specific named entities. And while\\nWeller et al. (2023) and Kandpal et al. (2022) find\\nthat entity frequency in training data is correlated\\nwith performance in factual recall about them, we\\nfind that entity frequency is anti-correlated with\\nhypothetical generalization performance (§6).\\nBubeck et al. (2023) argue that GPT-4 under-\\nstands language “beyond memorization”. We do\\nnot disprove generalization, but we show that GPT-\\n4 shows the same hallucinations in Appendix F.\\n3 Experimental Design\\nWe design behavioral experiments on LLMs by\\nmodifying NLI datasets with rigorous controls. We\\nobserve large behavior changes across three major\\nLLM families due to propositional-memory effects\\nin §5 and §6, and corpus frequency in §7. Finally,\\nwe show the impact on real performance in §8.\\n3.1 Two Biases in Inference Predictions\\nWe claim that the pretraining objective to fit the\\ndistribution of natural text leads to biases in LLM\\ngenerations. We explore two such biases.\\nThe Attestation Bias (Λ) is the over-reliance of\\nan LLM on its propositional memory about a query\\nstatement. We claim that when a statement is likely\\nto be attested in some way by an LLM’s training\\ndata, it is more likely to affirm it as a conclusion in\\nNLI tasks, regardless of any premise it is presented\\nwith. We measure the attestedness of a sample\\nby prompting the LLM to ask if the hypothesis in\\nquestion is true, false, or unknown. 3 Attestation\\npredictions are denoted by Λ.\\nA biased model will appear to perform well on\\ndataset samples with entailment labels that happen\\nto align with the bias. Table 1 shows examples\\nfrom the Levy/Holt dev set.\\n3Alternatively, LLM perplexity for a statement could be\\nused; however, this is not always available, e.g. with GPT-3.\\n2759'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 2, 'page_label': '2760'}, page_content='Dev Sample Query: [premise] ⇒ [hypothesis] Dataset Label Bias Prediction\\nGeysers are common to New Zealand ⇒Geysers are found in New Zealand Entail Λ = hypothesis Attested\\nGeysers are found in New Zealand ⇒Geysers are common to New Zealand No-Entail Λ = hypothesis Not-Attested\\nWhiskey consists chiefly of alcohol ⇒Whiskey contains alcohol Entail Φ = f(consists chiefly of ) < f(contains)\\nWhiskey contains alcohol ⇒Whiskey consists chiefly of alcohol No-Entail Φ = f(contains) > f(consists chiefly of )\\nTable 1: Two pairs of samples are consistent with a respective bias. Model predictions made on the basis of the bias\\nwill appear to predict the direction of entailment for each sample. f(·) maps a term to its corpus frequency.\\nAs discussed in §2, we draw inspiration from the\\nhypothesis-only baseline (Poliak et al., 2018), but\\nour test only queries model memory about the hy-\\npothesis without any training. We describe prompt\\ngeneration in detail in §4.2, with an example in\\nappendix Table 13.\\nDasgupta et al. (2022) show a similar effect in\\nLLMs on abstract reasoning tests, related to sen-\\ntential content, and equate it to human tendencies.\\nIn contrast, we examine the risks of propositional\\nmemory on more realistic inference tasks.\\nThe Relative Frequency Bias (Φ) is the use by\\nLLMs of a simple rule for deciding entailment,\\ncalculable from corpus statistics. Entailment is\\naffirmed if, ignoring named entities, the eventuality\\nin premise P is less frequent in training than that\\nin hypothesis H.\\nThis bias is reflected in natural text: it is known\\nthat nouns follow a trend of becoming more specific\\nas corpus-frequency decreases (Rosch et al., 1976;\\nCaraballo and Charniak, 1999) and the same is\\nobserved for predicates (McKenna et al., 2023).\\nSince entailments may carry from a specific term\\nto a more general one, e.g. SPRINT entails RUN,\\nrelative frequency can often indicate direction of\\nentailment. However, this is an artifact of natural\\ntext and has no direct relationship with meaning.\\nTest samples are labeled for agreement with this\\nbias separately from models, with examples shown\\nin Table 1. Since LLM pre-train corpora are imprac-\\ntically large or proprietary, we instead use Google\\nN-grams4 as a proxy of the natural distribution of\\ntext, and thus the distributions of these corpora.\\nWe average frequencies between the years 1950-\\n2019, and compare between P and H. To acquire\\nthe generic eventualities in each sample, we ex-\\nclude any extracted entities and lemmatize predi-\\ncate phrases; further, we reduce the effect of noise\\nand sparsity by requiring a wide margin of differ-\\nence between P and H frequency estimates. Fre-\\nquency decisions are denoted by Φ.\\n4https://books.google.com/ngrams\\n3.2 Datasets\\nLevy/Holt consists of premise-hypothesis pairs,\\nwith a task formatted: “Given [premise P], is it\\ntrue that [hypothesis H]?” (Levy and Dagan, 2016;\\nHolt, 2019). Each P- and H-statement has the\\nproperty of containing one predicate with two en-\\ntity arguments, (where the same entities appear in\\nboth P and H) as shown in Table 2. This targeted\\ndataset is ideal for precisely measuring model un-\\nderstanding of predicates, because entailment be-\\ntween statements is decidable purely on the basis\\nof the predicates and their attributes. We study the\\nchallenging directional subset, where entailments\\nhold in one direction but not both.\\nRTE-1 is one of the original and most difficult\\ntests of NLI (Dagan et al., 2006). It is not purely\\ndirectional on the basis of predicates or consistently\\nstructured like Levy/Holt, so we leave it out of the\\nbehavioral experiments. However, it is a widely\\nunderstood dataset, and we use it to demonstrate\\nthe impact of the two biases in general NLI in §8.\\nExclusions are made of NLI datasets relating\\nto knowledge of the world, since we aim to test\\nLLMs on their capability to reason purely about\\nthe semantics of natural language predicates with-\\nout relying on memorized facts. We explicitly\\navoid datasets such as MMLU (Hendrycks et al.,\\n2021), Natural Questions (Kwiatkowski et al.,\\n2019), OpenBookQA (Mihaylov et al., 2018) etc.\\n3.3 Dataset Transformations\\nThe Standard Inference Task (I) is on original\\nNLI datasets, in which entailment is determinable\\nby using general language inference on sentences.\\nIn Levy/Holt, it is determinable just by predicates.\\nWe define three dataset transformations to study\\nthe change in model responses as targeted informa-\\ntion is removed. These are the randomized premise\\npredicate setting IRandPrem , and two argument\\ntransformations: generic arguments IGenArg, and\\ntype-constrained randomized arguments IRandArg.\\n2760'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 3, 'page_label': '2761'}, page_content='Task Label Dev Sample Query: [premise] ⇒ [hypothesis]\\nI Entail George Bush was the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor of Texas⇒ George Bush is a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician from Texas\\nIRandPrem No-Entail George Bush resided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided in Texas ⇒ George Bush is a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician from Texas\\nTable 2: From the original dataset task ( I) we derive the Random Premise task ( IRandPrem ), respecting type-\\nconstraints. A random premise is highly unlikely to entail the hypothesis, so samples are relabeled No-Entail.\\nTask Label Dev Sample Query: [premise] ⇒ [hypothesis]\\nI Entail IndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndia exports tons of ricericericericericericericericericericericericericericericericerice ⇒ IndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndia exports ricericericericericericericericericericericericericericericericerice\\nIGenArg Entail location Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation X exports tons of food Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Y ⇒ location Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation X exports food Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Y\\nIRandArg↓ Entail SloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijk exports tons of oatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookies ⇒ SloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijk exports oatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookies\\nIRandArg↑ Entail HelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinki exports tons of Granny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny Smith ⇒ HelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinki exports Granny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny Smith\\nTable 3: An original dev sample (I) is transformed by insertion of entity types (IGenArg); by real entities sampled\\nfrom the 5% least frequent in NewsCrawl (IRandArg↓); and also from the 5% most frequent (IRandArg↑).\\nTransformations involve first identifying the\\ntypes of entities in statements, in order to con-\\nstrain entity or predicate replacements. We type\\neach entity with one of the 48 FIGER types (Ling\\nand Weld, 2012), such as “person,” “location,” etc.\\nFirst, an entity linker (Nguyen et al., 2014) identi-\\nfies the Freebase ID (Bollacker et al., 2008) for an\\nentity, from which we then obtain its FIGER type;\\nwe assign a default type “thing” in failure cases.\\nThe Random Premise Task ( IRandPrem ) re-\\nplaces the original premise predicate with a ran-\\ndom predicate, while maintaining the same entity\\narguments. This manipulation produces a dataset in\\nwhich all samples are labeled No-Entail, since\\ntwo randomly paired predicates are very unlikely to\\nbe related by entailment. Thus, positive decisions\\nby the model are false positive hallucinations.5\\nTo maintain naturalness and grammaticality, we\\nconstrain a new predicate to have argument slots\\nof the same types as the original premise. For ex-\\nample, “[medicine] is indicated for patients with\\n[disease]” is swapped for “[medicine] does not\\ncure [disease]”. We source candidates from dev\\nset premises satisfying the target type-constraints,\\nand sample uniform randomly. We map the original\\nentities to their respective slots in the new premise.\\nExamples are shown in Table 2. IRandPrem is a\\ngood test of model reliance on propositional mem-\\nory, since we prevent entailments while maintain-\\ning the attestedness of conclusions (hypotheses).\\nThe Generic Argument Task ( IGenArg) re-\\nplaces original entities with unique FIGER-typed\\n5We manually inspected the generated random premise\\nentries for the Levy/Holt dataset to verify this: we found\\n86.6% of entries are successfully non-entailing, 3.8% unde-\\ncided cases, and only 9.6% are unintended true entailments.\\nidentifiers, e.g. “location X” and “food Y .” By\\nmasking the identities of entities, this test is de-\\nsigned to remove entity information while main-\\ntaining the same entailment label, as a baseline\\ncontrol setting. We append unique identifiers (e.g.\\n“X,” “Y”) to allow tracking of entity slots across\\nthe premise and the hypothesis.\\nThe Random Argument Task ( IRandArg) re-\\nplaces original entities with other real, random en-\\ntities of the same FIGER-type. Like IGenArg, this\\ntest is designed to create novel strings by modifying\\nstatements without changing entailment labels. But\\nnow we test model sensitivity to added extraneous\\ninformation. Examples are shown in Table 3.\\nWe use entity type constraints here to ensure\\npolysemous predicates maintain the same sense.\\nFor example, a different sense of run is used\\nin “[person] runs [organization]” vs. “[person]\\nruns [software]”, but between different entities of\\nthe same type, the same senses are used, so the\\nexact entity IDs do not affect entailment labels\\n(Yarowsky, 1993). We source new entities from\\nNewsCrawl (Barrault et al., 2019), a decade-long\\nspan of multi-source news text, in which entities\\nare typed as above. We sample new entities uni-\\nform randomly from the 5% least common entities\\nin NewsCrawl (IRandArg↓), and the 5% most com-\\nmon (IRandArg↑). We insert the sampled entities\\nwhile preserving the rest of each statement.\\n4 Querying Models with Prompts\\n4.1 Models\\nLLaMA is a recent LLM model family which\\nrivals or surpasses GPT-3 performance while being\\nopen to scientific study. A range of model sizes\\n2761'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 4, 'page_label': '2762'}, page_content='are provided, and we test the largest LLaMA-65B\\nmodel. LLaMA is not fine-tuned. In preliminary\\nexperiments on the Levy/Holt dataset, we found\\ntwo popular fine-tuned LLaMA variants, Alpaca\\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\\nperform similarly to LLaMA base models and un-\\nderperform LLaMA-65B, so we leave them out of\\nfurther experiments.\\nGPT-3 Series models are closed to deep scien-\\ntific review (Brown et al., 2020), though they are a\\nwidely-used comparison for their performance, and\\nhave been reasonably well-studied. We evaluate on\\ntext-davinci-003 (GPT-3.5), as it is the largest, and\\nhas undergone instruction- and RLHF-finetuning,\\nenabling interesting comparisons.\\nPaLM is larger than GPT-3, which often claims\\nstate-of-the-art on evaluation datasets. We use the\\nlargest PaLM-540B base model, which is also only\\npretrained, so it serves as a further comparison\\npoint to LLaMA.\\nLater GPT models (like text-davinci-003 in our\\nexperiments) have been pre-trained and fine-tuned,\\nwhile base LLaMA and PaLM have only under-\\ngone pre-training, so their contrast indicates what\\nstage of training is responsible for the phenomena\\nwe study. Our aim is not to judge which LLM\\nis superior, but to show the common sources of\\nhallucination they share.\\nWe also omit models superseded in performance\\nby LLaMA (e.g. OPT, GPT-J, etc.), as well as\\nproducts that are closed to scientific review (e.g.\\nGPT-4, Bard, etc.)6.\\n4.2 Prompt Design and Evaluation\\nFormatting of test samples is done by inserting\\nthe premise and hypothesis into a prompt template,\\nwhich is used to query the model in natural lan-\\nguage. Following this, we append a three-way an-\\nswer choice: A) Entailment, B) Neutral, C) Contra-\\ndiction, following the typical format in NLI (Bow-\\nman et al., 2015).\\nSelection of the prompt template used in test is\\ndecided by the highest AUC obtained on the respec-\\ntive dev set. We try 8 promising templates includ-\\ning 5 from Schmitt and Schütze (2021), also used\\nin other NLI work7 (Webson and Pavlick, 2022).\\n6We include an analysis of GPT-4 in Appendix F.\\n7See Appendix A for details on prompt template selection.\\nIdeally, an LLM with advanced language under-\\nstanding ability could perform inference in zero-\\nshot without annotated examples, which would\\nraise confidence that this faculty is ready for down-\\nstream tasks. To this end, we examine each LLM in\\nzero-shot (detailed in Appendix A), but they exhibit\\nseverely degraded, even near-random performance.\\nWe turn to few-shot, and hand-annotate a mini-\\nmal 4 examples in the style of the template, with\\nadded explanations about why the given answer\\nis correct for each example. These examples are\\nprepended before the query (see Appendix A for\\nan example). Our goal is to study model behavior\\nas conditions change, not to maximize the score on\\nany particular dataset. Therefore, we use a mini-\\nmal 4-example setup, which we find is capable of\\nevoking positive responses from all three LLMs on\\neach dev set, across most templates.\\nScoring is done by converting choice A into\\nEntail and collapsing both B and C choices into\\nNo-Entail to align with Levy/Holt and RTE-1\\nannotation. For behavioral experiments in §5, §6,\\nand §7, we score the model solely based on its tex-\\ntual response. All models successfully choose one\\nof A/B/C on all dev questions, showing compatibil-\\nity with the QA format.\\nFor the analysis in §8 which measures model per-\\nformance across confidence thresholds, we convert\\nthe letter choice to a probability with the mapping:\\nSent = 0.5 + 0.5 ∗I[tok = A] ∗Stok\\n−0.5 ∗I[tok ∈{B, C}] ∗Stok\\nWhere I is the indicator function, and Sent esti-\\nmates the probability of Entail from a textual\\noutput (0 ≤Sent ≤1) with token probability Stok.\\nThe linear transformation preserves the ordering of\\nmodel confidences, which is sufficient for calculat-\\ning a precision-recall curve.\\n5 Experiment 1: Attestation Bias\\nWe begin our experiments by assessing LLMs’ re-\\nliance on their propositional memory of training\\ntext by conditioning each model’s entailment task\\npredictions I on its own predictions of attestation\\nΛ. We do this by comparing estimated probabilities\\nof predicting Entail conditioned on whether the\\nhypothesis is predicted Attested or not.\\nFurther, we test a setting which controls for the\\npossibility that original Levy/Holt entailments may\\ncoincidentally refer to attested facts, which could\\n2762'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 5, 'page_label': '2763'}, page_content='Figure 1: Exp-1. Estimated probability of predicting\\nEntail for original entries in Levy/Holt, conditioned\\non LLMs’ attestation of hypotheses (Λ). This setting is\\nintuitive but may be subject to spurious correlations.\\nFigure 2: Exp-1. Estimated probability of predicting\\nEntail for Random-Premise entries in Levy/Holt,\\nconditioned on LLMs’ attestation of hypotheses ( Λ).\\nNow, predicting Entail is false positive hallucination\\n(lower is better). Models are sensitive to attestation, and\\nhallucinate more when the hypothesis is attested.\\nlead to spurious correlation between inference and\\nattestation scores without clearly demonstrating use\\nof memory versus true entailment. This controlled\\nsetting is the random premise task IRandPrem ,\\nwhich converts entailments into non-entailments\\nwithout altering the hypothesis. An ideal model\\ncapable of drawing inferences from information in\\ncontext should detect that in the IRandPrem task it\\nis no longer possible to infer the hypothesis based\\non the premise (even if the hypothesis is itself\\nattested in training), and never predict Entail.\\nThus, in IRandPrem , all Entail predictions are\\nassumed to be false positive hallucinations.\\n5.1 Results\\nWith I, IRandPrem and Λ predictions acquired as\\ndescribed in §3.1, we present the conditional proba-\\nbilities in Figures 1 and 2. It is clear that a model’s\\nmemory about the hypothesis plays a part in its pre-\\ndictions of the hypothesis given a premise, either\\nrelated or random.\\nFor I, we observe significantly higher proba-\\nbility of predicting Entail when the hypothe-\\nsis is Attested. In the random premise task\\nIRandPrem , this trend continues. LLaMA, GPT-\\n3.5, and PaLM, respectively, show a 1.9x, 2.2x,\\nand 2.0x higher chance of falsely predicting that a\\nrandom premise Entails the hypothesis if it al-\\nready predicts the hypothesis is Attested. This\\nfalse positive hallucination and its impact on NLI\\nperformance is investigated further in §8.\\nThis behavior is observed across model families\\n(LLaMA, GPT, and PaLM), establishing that it is\\ndue to pretraining rather than Instruction-tuning\\nor RLHF, since LLaMA and PaLM have only un-\\ndergone pretraining. This behavior is undesirable,\\nbecause model predictions on NLI tasks should be\\nbased solely on general language understanding,\\nnot prior knowledge. We may conclude that mem-\\nory of training data is a significant contributor in\\nLLM inference, and may be an important source of\\nhallucination.\\n5.2 Implications for Real Applications\\nUsing prior knowledge as part of language infer-\\nence has bad implications for the use of LLMs in\\nreal applications. We offer an example scenario\\nof a question-answering task where user questions\\nare answered from a Knowledge Base (KB). In\\ntypical formulations of this task, if a statement in\\nthe KB (premise) entails a user query (hypothe-\\nsis), the premise may be formulated into an answer.\\nConsider a KB such as a legal document or HR rule-\\nbook. Assume that the text is prepended to the user\\nquery and presented to the LLM, as in other works\\n(Srinivasan et al., 2022). Given our findings, we\\nmight observe the LLM hallucinating answers to\\nquestions using information which is not presented\\nin the KB, but may have been read by the LLM in\\ntext from other sources during pretraining. These\\nanswers could be illogical, contradictory, and could\\nmisrepresent the views of the KB, or other harms.\\nSuch poor use of in-context learning has already\\nbeen observed in specific domains like medicine\\n(Jimenez Gutierrez et al., 2022).\\nIn general, this is a risk for LLMs which (a) are\\ndeployed for tasks like QA by feeding novel text\\n(e.g. a legal document) in-context as part of the\\nuser query, and (b) are trained on datasets which are\\n2763'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 6, 'page_label': '2764'}, page_content='Levy/Holt (Directional)\\nModel Task Precision Recall ∆-Recall\\nLLaMA\\nI 67.0 68.4 0\\nIGenArg 69.0 66.9 -1.5\\nIRandArg↓ 64.0 63.8 -4.6\\nIRandArg↑ 67.2 53.7 -14.7\\nGPT-3.5\\nI 62.4 92.3 0\\nIGenArg 65.1 75.7 -16.6\\nIRandArg↓ 65.5 66.5 -25.8\\nIRandArg↑ 68.8 55.3 -37.0\\nPaLM\\nI 72.8 76.2 0\\nIGenArg 79.8 50.8 -25.4\\nIRandArg↓ 69.5 58.7 -17.5\\nIRandArg↑ 70.8 52.4 -23.8\\nTable 4: Exp-2. Scoring model outputs in different\\nargument-replacement tasks. We indicate the highest\\nand lowest recall score across replacement settings. Re-\\ncall decreases sharply across settings in all models.\\nprivate or otherwise infeasibly large to read man-\\nually, containing many facts and human opinions\\nunknowable to both the user and modeler.\\n6 Experiment 2:\\nEntities are Indices to Memory\\nIn §5, we have established that propositional mem-\\nory explains a significant portion of false positives\\nin LLM inference predictions. In this section, we\\ncontinue by showing the importance of named enti-\\nties in the process of LLMs’ memory recall.\\nAs described in §3.3, we manipulate the enti-\\nties with the IGenArg generic argument replace-\\nment, and two random entity replacements, one\\nwith infrequent-entities IRandArg↓ and one with\\nfrequent-entities IRandArg↑(examples in Table 3).\\nBy replacing arguments constrained by type, en-\\ntailment labels are maintained; however, new sam-\\nples should contain novel strings not attested in pre-\\ntrain corpora. We expect that an ideal, generalizing\\nmodel would maintain its predictions across all\\nconditions; a flawed model utilizing the attestation\\nbias would predict fewer Entail, since entities\\nno longer identify these statements in training.\\n6.1 Results\\nWe report results across conditions in Table 4. We\\nobserve two phenomena across all three models,\\naligning with the above conjecture of “flaws.”\\nFirst, we observe that all models’ behavior signif-\\nicantly changes in the same way when original en-\\ntities are replaced by either entity types or random\\nreal entities. Despite similar (or marginally increas-\\ning) precision across conditions, recall degrades\\nsharply from original entities (I) (GPT-3.5 @92.3)\\nto random frequent entities (IRandArg↑) (GPT-3.5\\n@55.3). Generic-argument IGenArg performance\\nalso degrades in this way, showing that this is not a\\nmatter of poorly selected real entities, but rather a\\nloss of information from the original dataset which\\nmodels were using to answer questions.\\nSecond, across the 3 models, we observe a sig-\\nnificant difference in recall between the two real\\nentity conditions IRandArg↓and IRandArg↑, which\\nare both composed of unattested statements, but in-\\nvolve entities that differ in typical corpus frequency.\\nInfrequent entities (IRandArg↓) yield better gener-\\nalization and a higher recall (GPT-3.5 @66.5) than\\nfrequent entities (IRandArg↑) (GPT-3.5 @55.3).\\nThese findings corroborate those from §5, that\\nLLMs use memory as part of language inference,\\nand additionally show that these memories are re-\\ncalled using named entities acting as indices. These\\nexperiments demonstrate that too much prior expo-\\nsure to an entity may impede model generalization\\nwhen that entity is discussed in novel inferences:\\nthe more a model has read about an entity during\\npretraining, the less capable it is of drawing novel\\nnatural language inferences involving it. This is\\nthe case even though the inferences do not require\\ndetailed knowledge of the entity.\\nLike §5, the effect is consistent across models,\\nindicating LLM pretraining is responsible.\\nWe show similar results on RTE-1 in Appendix\\nB. Further, instructing LLMs to ignore proposi-\\ntional memory in Appendix C shows little change.\\n7 Experiment 3: Relative Frequency Bias\\nWe continue the conditioning experiments from §5,\\nnow exploring the relative frequency bias. Sam-\\nple labels for this bias are denoted by the model-\\nagnostic Φ as described in §3.1. Φ labels the con-\\nformance of sample predicates to the bias: Φ<\\nmeans P is less corpus-frequent than H by a mar-\\ngin (positive class), Φ> means P more frequent\\nthan H by the margin (negative class). To control\\nfor differences between datasets, the margin is set\\nso that 1/3 of samples are classed as “roughly equal”\\n(Φ≈), which we discard.\\nFollowing the observations in §6, we further ap-\\nply a generic-argument transformation to control\\nfor attestation, yielding IGenArg\\nRandPrem . With the en-\\ntities masked, models cannot recall propositional\\nmemory for this task: by re-calculating the Λ mea-\\n2764'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 7, 'page_label': '2765'}, page_content='Figure 3: Exp-3. Estimated probability of predicting\\nEntail for random-premise Levy/Holt conditioned\\non relative frequencies (Φ), with original (IRandPrem )\\nor generic ( IGenArg\\nRandPrem ) entities. Predicting Entail\\nis false positive hallucination (lower is better). Models\\nhallucinate more often when test samples conform to\\nthe relative frequency bias (Φ<) than when not (Φ>).\\nsure with generic arguments, only 2 hypotheses are\\nstill predicted as Attested by GPT-3.5, whereas\\nfor LLaMA and PaLM, the numbers are also only\\n6.2% and 3.9%. Additionally, as with IRandPrem ,\\nhere the entailment label of each sample remains\\nNo-Entail, so any Entail prediction is false\\npositive hallucination.\\n7.1 Results\\nWe estimate the probabilities of models predict-\\ning Entail conditioned on the frequency label Φ,\\nbetween IRandPrem and IGenArg\\nRandPrem settings, and\\npresent the results in Figure 3. We observe a clear\\nand consistent rise of hallucination when samples\\nconform to the bias. Namely, in case ofΦ<, models\\nare more likely to predict Entail, even though\\nno semantic relation exists between P and H.\\nBetween the two settings, withIRandPrem , when\\nentities are available, this effect is moderate. On\\nthe other hand, with IGenArg\\nRandPrem when entity-based\\nmemory is blocked, we observe a decrease in the\\noverall level of hallucination, but the separation be-\\ntween Φ< and Φ> becomes more drastic, to 1.6x,\\n1.8x and 2.0x for LLaMA, GPT-3.5 and PaLM\\nrespectively. This indicates a tension between Λ\\nand Φ: propositional memory may be used when\\navailable, and if not, the predicate pairing may be\\nattended to more closely. Again, the Φ effect is\\nobserved across the three model families, reveal-\\ning its root in the large-scale pre-training process,\\nrather than model peculiarities or fine-tuning.\\n8 Impact of Bias on Performance\\nWe have demonstrated two sources of hallucination\\nby LLMs on inference tasks. We now assess their\\nimpact on model performance to quantify their risk.\\nWe compare LLMs’ performance between NLI\\nsubsets that are consistent or adversarial to each\\nbias. A sample P ⊨ H? is consistent with a bias\\nwhen the prediction by the bias agrees with the\\ngold entailment label; conversely, it is adversarial\\nto a bias when the prediction by the bias disagrees\\nwith the label.\\nFor example, “Google bought YouTube ⊨\\nGoogle owns YouTube” isconsistent with the attes-\\ntation bias of every model, because the conclusion\\nGoogle owns YouTubeis attested in every LLM’s\\ntraining data, and the sample label is Entail;\\n“Apple owns Samsung ⊭ Apple bought Samsung”\\nis also consistent, because its conclusion is not at-\\ntested and the sample label is No-Entail. The\\nreverses of these two samples areadversarial, since\\ntheir respective attestedness (unchanged) does not\\nagree with the entailment labels (now flipped). For\\neach subset, there is substantial representation in\\nboth Levy/Holt and RTE-1 (see appendix Table 9).\\nWhile earlier experiments inspected model tex-\\ntual responses to characterize behavior change,\\nwe now use area under the precision-recall curve\\n(AUC) to summarize model performance over a\\ntunable confidence threshold (scoring described in\\n§4.2), which is better for measuring practical dis-\\ncriminative power. Following Li et al. (2022), we\\nre-scale AUC values to normalize over the label\\ndistribution, yielding AUCnorm values that assign\\nrandom classifiers 0% and perfect classifiers 100%.\\nWe report results in Table 5. Under the stan-\\ndard inference task I, the performance drop from\\nΛCONSISTENT to ΛADVERSARIAL is severe for all 3\\nLLMs: they deteriorate from very good classi-\\nfiers to poor or even near-random ones. 8 This\\nfragility from the attestation bias can be alleviated\\nby masking entities with type-identifiers (condition\\nIGenArg), which reduces the performance drop.\\nOn the other hand, with the generic arguments\\nin IGenArg, LLMs are forced to focus on the predi-\\ncates in each proposition. As a result, the impact\\nof the relative frequency bias is intensified. From\\nthe standard inference task I to IGenArg, the av-\\nerage performance drop from the cons. to adv.\\n8We note Λ predictions could possibly be influenced by\\nmodel-specific idiosyncrasies in prompt format. We provide\\nan analysis in Appendix E, where we find no significant effect.\\n2765'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 8, 'page_label': '2766'}, page_content='Levy/Holt RTE-1\\nAttestation (Λ) Rel. Frequency ( Φ) Attestation ( Λ) Rel. Frequency ( Φ)\\nModel Task cons. adv. diff. cons. adv. diff. cons. adv. diff. cons. adv. diff.\\nLLaMA I 65.5 8.1 -57.4 42.1 32.3 -9.8 62.1 37.4 -24.7 55.5 51.7 -3.8\\nGPT-3.5 I 85.0 10.8 -74.2 53.5 43.2 -10.3 84.6 47.5 -37.1 77.6 43.4 -34.2\\nPaLM I 79.1 31.5 -47.6 63.3 53.0 -10.3 87.1 83.4 -3.7 87.5 81.0 -6.5\\nLLaMA IGenArg 52.1 34.4 -17.7 55.3 34.9 -20.4 59.2 30.4 -28.8 51.7 39.4 -12.3\\nGPT-3.5 IGenArg 67.1 18.8 -48.3 50.4 35.0 -15.4 80.1 56.4 -23.7 79.6 49.1 -30.5\\nPaLM IGenArg 58.1 46.6 -11.5 59.9 47.3 -12.6 78.1 84.4 +6.3 85.4 78.7 -6.7\\nTable 5: LLM performance on subsets where Λ/Φ is consistent/adversarial to entailment labels, measured with\\nAUCnorm (0% = random chance performance). Decrease from cons to adv subsets are shown in the diff. columns.\\nsubsets w.r.t. Φ is widened from 10.1% to 16.1%\\nfor Levy/Holt and from 14.8% to 16.5% for RTE-\\n1. The differences for Φ-consistency subsets are\\ngenerally narrower thanΛ-consistency subsets, pos-\\nsibly because the relative frequencies require gen-\\neralizing from instances, and may be more difficult\\nto capture, and potentially because frequency mea-\\nsures with Google N-gram are a crude estimate of\\nthe actual frequencies in LLM pre-train corpora.\\n9 Conclusion\\nAcross several major LLM families and experimen-\\ntal settings, we demonstrate two important biases\\nin the performance of LLMs on natural language\\ninference tasks, which may also manifest in applied\\ntasks as hallucination. Contrary to claims of LLM\\ngeneral reasoning capabilities, we show that much\\nof this performance is achieved by (1) recall of rel-\\nevant memorizations and (2) corpus-based biases\\nlike term frequency. Since these factors are repro-\\nduced in all models, we establish that they originate\\nin LLM pre-training, and are not corrected during\\nGPT-3.5 fine-tuning.\\nWe conclude that LLMs, though powerful, use\\nunsatisfactory tools for the basic tasks of language\\nunderstanding and inference. We propose several\\napproaches to control for these biases in evaluation,\\nand ultimately conclude that further attention on\\nalleviating these biases are needed, before LLMs\\nmay be trusted to reason robustly about language.\\nLimitations\\nIn this paper, we have discussed two prominent\\nsources of hallucination for LLMs in natural lan-\\nguage inference tasks. We acknowledge that this is\\nnot an exhaustive search of all the sources, where\\nfurther exploration should be done in future work.\\nWe also note that after controlling for the factors\\ndiscussed in this paper, there remains residual, un-\\nexplained performance on NLI tasks. This residual\\nmight be due to other undiscovered biases or pos-\\nsibly generalising inference capability. We leave\\nfurther exploration of this residual to future work.\\nAs discussed in Appendix A, we compared a\\nrange of popular LLM prompting techniques and\\nselected the most promising approach. We ac-\\nknowledge that there could potentially be other\\nnovel prompting techniques that could help the\\nLLMs resist the influence of the biases discussed\\nin this paper. We identify this as an open question\\nand advocate for future research.\\nEthical Considerations\\nThis paper discusses two major sources of hallu-\\ncination in LLM output when asked to perform\\nnatural language inference, which we note is a ca-\\npability required of many downstream tasks such\\nas summarization, question answering, etc. We\\nshow that users of LLMs may be subjected to faulty\\njudgements if the content of their request overlaps\\nwith data in pretraining. However, it is difficult to\\nascertain for both a user or modeler exactly what\\nis contained in pretraining data, or how this will\\ninteract with a user’s query. Our proposed attes-\\ntation query shows promise in detecting potential\\noverlaps, but model responses in applications of\\nthese cases are not explored. Further, the relative\\nfrequency bias demonstrates a much more subtle\\nproblem of corpus distribution that is naturally in-\\nherent to model pretraining on human generated\\ntext.\\nIn light of these, the potential harms of LLM use\\nfor drawing natural language inferences may in-\\nclude: offering inaccurate or irrelevant information\\nto a user’s query or contradiction of information\\nprovided in-context with a user’s query.\\n2766'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 9, 'page_label': '2767'}, page_content='Acknowledgements\\nThis research was supported by ERC Advanced\\nFellowship GA 742137 SEMANTAX and the Uni-\\nversity of Edinburgh Huawei Laboratory.\\nReferences\\nLoïc Barrault, Ond ˇrej Bojar, Marta R. Costa-jussà,\\nChristian Federmann, Mark Fishel, Yvette Gra-\\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\\nShervin Malmasi, Christof Monz, Mathias Müller,\\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\\nFindings of the 2019 Conference on Machine Trans-\\nlation (WMT19). In Proceedings of the Fourth Con-\\nference on Machine Translation (Volume 2: Shared\\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\\nsociation for Computational Linguistics.\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\\nSturge, and Jamie Taylor. 2008. Freebase: A col-\\nlaboratively created graph database for structuring\\nhuman knowledge. In Proceedings of the 2008 ACM\\nSIGMOD International Conference on Management\\nof Data, SIGMOD ’08, page 1247–1250, New York,\\nNY , USA. Association for Computing Machinery.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn Proceedings of the 2015 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n632–642, Lisbon, Portugal. Association for Compu-\\ntational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, Harsha Nori, Hamid Palangi, Marco Tulio\\nRibeiro, and Yi Zhang. 2023. Sparks of Artificial\\nGeneral Intelligence: Early experiments with GPT-4.\\nArXiv:2303.12712 [cs].\\nSharon A. Caraballo and Eugene Charniak. 1999. De-\\ntermining the specificity of nouns from text. In 1999\\nJoint SIGDAT Conference on Empirical Methods in\\nNatural Language Processing and Very Large Cor-\\npora.\\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\\n2023. Quantifying Memorization Across Neural Lan-\\nguage Models. ArXiv:2202.07646 [cs].\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\n2006. The pascal recognising textual entailment chal-\\nlenge. In Machine Learning Challenges. Evaluating\\nPredictive Uncertainty, Visual Object Classification,\\nand Recognising Tectual Entailment, pages 177–190,\\nBerlin, Heidelberg. Springer Berlin Heidelberg.\\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\\nC. Y . Chan, Antonia Creswell, Dharshan Kumaran,\\nJames L. McClelland, and Felix Hill. 2022. Lan-\\nguage models show human-like content effects on\\nreasoning. ArXiv:2207.07051 [cs].\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\nhardt. 2021. Measuring massive multitask language\\nunderstanding. Proceedings of the International Con-\\nference on Learning Representations (ICLR).\\nXavier Holt. 2019. Probabilistic Models of Relational\\nImplication. arXiv:1907.12048 [cs, stat] . ArXiv:\\n1907.12048.\\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\\nWashington, You Chen, Lang Li, Huan Sun, and\\nYu Su. 2022. Thinking about GPT-3 in-context learn-\\ning for biomedical IE? think again. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2022, pages 4497–4512, Abu Dhabi, United Arab\\nEmirates. Association for Computational Linguistics.\\n2767'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 10, 'page_label': '2768'}, page_content='Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2022. Large language\\nmodels struggle to learn long-tail knowledge.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Transactions of the Association of Compu-\\ntational Linguistics.\\nOmer Levy and Ido Dagan. 2016. Annotating Rela-\\ntion Inference in Context via Question Answering.\\nIn Proceedings of the 54th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n2: Short Papers), pages 249–255, Berlin, Germany.\\nAssociation for Computational Linguistics.\\nTianyi Li, Mohammad Javad Hosseini, Sabine Weber,\\nand Mark Steedman. 2022. Language Models Are\\nPoor Learners of Directional Inference. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2022, pages 903–921, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nXiao Ling and Daniel S. Weld. 2012. Fine-grained en-\\ntity recognition. In Proceedings of the Twenty-Sixth\\nAAAI Conference on Artificial Intelligence, AAAI’12,\\npage 94–100. AAAI Press.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\\n1907.11692.\\nNick McKenna, Tianyi Li, Mark Johnson, and Mark\\nSteedman. 2023. Smoothing Entailment Graphs with\\nLanguage Models. In Proceedings of the 3rd Confer-\\nence of the Asia-Pacific Chapter of the Association\\nfor Computational Linguistics and the 13th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing. Association for Computational Linguistics.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018. Can a suit of armor conduct elec-\\ntricity? a new dataset for open book question answer-\\ning. In Conference on Empirical Methods in Natural\\nLanguage Processing.\\nDat Ba Nguyen, Johannes Hoffart, Martin Theobald,\\nand Gerhard Weikum. 2014. Aida-light: High-\\nthroughput named-entity disambiguation. In LDOW.\\nOpenAI. 2023. GPT-4 Technical Report.\\nArXiv:2303.08774 [cs].\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. ArXiv:2203.02155 [cs].\\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\\nRachel Rudinger, and Benjamin Van Durme. 2018.\\nHypothesis only baselines in natural language infer-\\nence. In Proceedings of the Seventh Joint Confer-\\nence on Lexical and Computational Semantics, pages\\n180–191, New Orleans, Louisiana. Association for\\nComputational Linguistics.\\nEleanor Rosch, Carolyn B Mervis, Wayne D Gray,\\nDavid M Johnson, and Penny Boyes-Braem. 1976.\\nBasic objects in natural categories. Cognitive Psy-\\nchology, 8(3):382–439.\\nMartin Schmitt and Hinrich Schütze. 2021. Language\\nModels for Lexical Inference in Context. In Proceed-\\nings of the 16th Conference of the European Chap-\\nter of the Association for Computational Linguistics:\\nMain Volume, pages 1267–1280, Online. Association\\nfor Computational Linguistics.\\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\\nLingrui Liao, Luca Bertelli, and Michael Bendersky.\\n2022. QUILL: Query intent with large language mod-\\nels using retrieval augmentation and multi-stage dis-\\ntillation. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing:\\nIndustry Track, pages 492–501, Abu Dhabi, UAE.\\nAssociation for Computational Linguistics.\\nAarne Talman and Stergios Chatzikyriakidis. 2019.\\nTesting the Generalization Power of Neural Network\\nModels across NLI Benchmarks. In Proceedings of\\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\\nand Interpreting Neural Networks for NLP, pages 85–\\n94, Florence, Italy. Association for Computational\\nLinguistics.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca.\\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\\nand Armen Aghajanyan. 2022. Memorization with-\\nout overfitting: Analyzing the training dynamics of\\nlarge language models. In Advances in Neural Infor-\\nmation Processing Systems, volume 35, pages 38274–\\n38290. Curran Associates, Inc.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models.\\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\\nbased models really understand the meaning of their\\n2768'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 11, 'page_label': '2769'}, page_content='prompts? In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, pages 2300–2344, Seattle, United States.\\nAssociation for Computational Linguistics.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems,\\nvolume 35, pages 24824–24837. Curran Associates,\\nInc.\\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn\\nLawrie, Daniel Khashabi, and Benjamin Van Durme.\\n2023. \"according to ...\" prompting language models\\nimproves quoting from pre-training data.\\nDavid Yarowsky. 1993. One sense per collocation.\\nIn Human Language Technology: Proceedings of\\na Workshop Held at Plainsboro, New Jersey, March\\n21-24, 1993.\\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\\nKathleen McKeown, and Tatsunori B. Hashimoto.\\n2023. Benchmarking large language models for news\\nsummarization.\\nA Prompt Format Selection\\nIn prompt-based interactions with the LLMs, sev-\\neral types of context information could be added\\nto help models produce accurate and robust predic-\\ntions. We attend to two design choices in prompt\\nengineering: prompt templates and in-context ex-\\namples.\\nPrompt templates are known to have a direct\\nand sometimes decisive impact on LLM behavior.\\nAs such, we carefully select a range of clear and\\nconcise templates as promising candidates. As\\ndiscussed in §4.2, we run each template through\\nthe dev sets of each dataset, and select the template\\nwith the best discriminative power according to\\nAUC scores (similarly to §8). The candidate set of\\ntemplates includes 3 concise templates we wrote:\\n1. If [PREMISE ], then [HYPOTHESIS ].\\n2. [PREMISE ], so [HYPOTHESIS ].\\n3. [PREMISE ] entails [HYPOTHESIS ].\\nWe also considered the 5 prompt templates\\nused in bias work on LMs for textual entailments\\n(Schmitt and Schütze, 2021):\\n4. [PREMISE ], which means that [HYPOTHESIS ].\\n5. [HYPOTHESIS ], because [PREMISE ].\\n6. It is not the case that [HYPOTHESIS ], let alone\\nthat [PREMISE ].\\n7. [HYPOTHESIS ]NEG , which means that\\n[PREMISE ]NEG .\\n8. [PREMISE ]NEG , because [HYPOTHESIS ]NEG .\\nIn preliminary experiments with GPT-3.5, we ob-\\nserved that LLMs are not responsive to the 3 contra-\\npositive prompts from Schmitt and Schütze (2021)\\n(colored gray), performing at random. We also\\nobserved that prompt number 5 from Schmitt and\\nSchütze (2021) also consistently underperforms the\\nother 4 templates, so we use the remaining 4 tem-\\nplates (namely, template no. 1, 2, 3, 4) as our final\\ncandidate set.\\nIn-Context Examples have been widely used for\\ninteractions with LLMs since Brown et al. (2020).\\nFurther, Wei et al. (2022) has demonstrated that\\nincluding chain-of-thought explanation, namely\\nstep-by-step explanations, in the in-context exam-\\nples, helps LLMs perform reasoning tasks. On the\\nother hand, Ouyang et al. (2022) has suggested\\nthat instruction-tuned LLMs are also capable of\\nperforming tasks in zero-shot, without exposure to\\nany in-context examples.\\nWe compared zero-shot and few-shot in our pre-\\nliminary experiments with LLaMA and GPT-3.5 on\\nLevy/Holt directional dev set. Following Touvron\\net al. (2023), for zero-shot, we prepend a textual\\ndescription of the task to each test sample; for few-\\nshot, we prepend a minimal 4 examples with ex-\\nplanations. Instantiated prompts in the two settings\\nare demonstrated in Table 13. Here we report the\\ndev set results with the best-performing templates.\\nWe found that for the two pre-trained LLMs,\\nnamely, LLaMA and PaLM, zero-shot performance\\non the Levy/Holt directional dev set is near-random,\\nat 56.6% and 61.5% AUC respectively (random is\\n50%); with 4 in-context examples, the models be-\\ngin to exhibit non-trivial behavior, with 65.0% and\\n80.2% AUC , respectively. This is not surprising,\\nsince pre-trained LLMs without instruction fine-\\ntuning should not be expected to perform complex\\ntasks zero-shot. For GPT-3.5, the performance is\\nstill much lower in zero-shot, at 64.5%, compared\\nto 74.6% in few-shot.\\nAs discussed in §4.2, ideally we would like\\nLLMs to have zero-shot natural language abili-\\nties readily available for downstream tasks. How-\\never, in light of this observation, our primary ex-\\nperiments are conducted in the few-shot setting\\n2769'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 12, 'page_label': '2770'}, page_content='Model Task Precision Recall ∆-Recall\\nLLaMA\\nI 74.5 52.5 0\\nIGenArg 70.9 57.3 +4.8\\nIRandArg↓ 66.9 60.5 +8.0\\nIRandArg↑ 70.6 51.5 -1.0\\nGPT-3.5\\nI 80.6 96.5 0\\nIGenArg 79.7 91.3 -5.2\\nIRandArg↓ 80.1 82.5 -14.0\\nIRandArg↑ 81.9 80.5 -16.0\\nPaLM\\nI 90.3 84.0 0\\nIGenArg 92.3 71.5 -12.5\\nIRandArg↓ 87.8 82.5 -1.5\\nIRandArg↑ 88.2 82.0 -2.0\\nTable 6: Scoring model outputs in different conditions\\nof RTE-1. We indicate the highest and lowest recall\\nscore across replacement settings.\\nthroughout, in order to better explore the abilities\\nof these LLMs.\\nB RTE-1 Results For Experiment 2:\\nEntities are Indices to Memory\\nThe RTE-1 dataset contains complex natural lan-\\nguage statements with varied linguistic features,\\nso predictions about entailment are not decidable\\nonly on the basis of contained predicates. However,\\nRTE-1 is a difficult challenge set for models, and\\ninteresting to compare to in the broader domain of\\nNLI. Though the sentences are much more com-\\nplex, we are able to conduct an analogous experi-\\nment as in §6 by first identifying spans of named\\nentities and their respective entity types, then re-\\nplacing the entities with new ones. As before, we\\ncompare model scores on the original dataset to\\nthree test conditions: generic arguments (“location\\nX”, “person Y”, etc.), sampled low-frequency en-\\ntities constrained to the same type, and the same\\nfor high-frequency sampled entities. Since only\\nthe entities in each statement have been altered,\\nthe entailment labels between premise/hypothesis\\npairs remain unchanged, and an ideal model capa-\\nble of generalizing inference would make the same\\npredictions across dataset conditions. Results are\\nshown in Table 6.\\nWe observe similar trends to those reported on\\nLevy/Holt. GPT-3.5 performs very consistently be-\\ntween Levy/Holt and RTE-1 in terms of degrading\\nrecall when information is changed in each sample.\\nWe observe that model performance is worse than\\nthe original dataset when using generic arguments,\\nand worse still using type-constrained random ar-\\nguments. We further observe that across all three\\nLLMs across both datasets, models consistently\\nachieve worse recall using high-frequency entities\\nthan low-frequency entities, supporting the claim\\nthat increasing the frequency of entity occurrence\\nin training data impedes generalization.\\nDifferent from in Levy/Holt, we observe some\\nnoise in LLaMA’s predictions; the recall on the\\noriginal task is actually lower than the generic argu-\\nment condition and the low-frequency entity condi-\\ntion. We note that overall, LLaMA is the weakest\\nLLM tested in this experiment on both Levy/Holt\\nand RTE-1, and that its performance on RTE-1 is\\nparticularly low. We suggest that the increased dif-\\nficulty of RTE-1 over Levy/Holt (due to having\\nmuch more linguistic variation) is simply too com-\\nplex for LLaMA, which is neither the largest LLM\\ntested, nor instruction-finetuned.\\nWe also observe a smaller gap between PaLM’s\\nrecall rates across dataset conditions, though the\\ngaps are consistent with our claims. While the\\nmodel appears able to generalize to conditions in\\nwhich random real arguments are inserted, recall\\non the generic argument condition is significantly\\ndegraded. Failure on this control condition indi-\\ncates that the model may not be generalizing as\\nwell as the other conditions would imply.\\nC The Ineffectiveness of Instructing\\nLLMs to Stop Conditioning on\\nAttested Information\\nIn §5 and §6, we showed that entailment predic-\\ntions from LLMs are strongly biased by their pre-\\ndictions on the attestation of hypotheses. We won-\\ndered whether there are intuitive prompt engineer-\\ning techniques to steer its behavior away from at-\\ntending to attestation.\\nTowards this goal, we experimented with\\nprepending a brief task description to the few-shot\\nprompts in part B of Table 13, explicitly instructing\\nthe models to ignore the attestedness of individual\\nstatements: Please check the entailments between\\nthe following hypothetical statements. Ignore the\\nveracity of these statements.\\nWe replicated the experiments in §5 and §6 with\\nGPT-3.5, since GPT-3.5 is an instruction-finetuned\\nmodel trained to be responsive to prompts, where\\nthe other two LLM families are only pre-trained.\\nDespite having been instruction-finetuned, the re-\\nsults with GPT-3.5 show only marginal improve-\\nments in model behavior.\\n2770'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 13, 'page_label': '2771'}, page_content='task GPT-3.5 Instructed to Ignore Attestedness Not Instructed\\nI P (Entail | Attested) 74.3 77.6\\nI P (Entail | ¬Attested) 57.8 63.6\\nIRandPrem P(Entail | Attested) 39.0 41.3\\nIRandPrem P(Entail | ¬Attested) 17.6 18.8\\nTable 7: We estimate the probability of positive predictions of Entail in I and IRandPrem tasks respectively\\ngiven that the hypothesis is attested, namely Λ = Attested. Not instructed results are copied from Figure 2 and\\nlisted here for ease of comparison; also note that all IRandPrem = Entail predictions are false positives.\\nLevy/Holt (Directional)\\nGPT-3.5 Condition Task Precision Recall ∆-Recall\\nFew-shot, instructed to ignore attestedness.\\nI 64.9 90.8 0\\nIGenArg 73.5 69.3 -21.5\\nIRandArg↓ 64.6 68.4 -22.4\\nIRandArg↑ 67.5 58.1 -32.7\\nFew-shot, no instructions.\\nI 62.4 92.3 0\\nIGenArg 65.1 75.7 -16.6\\nIRandArg↓ 65.5 66.5 -25.8\\nIRandArg↑ 68.8 55.3 -37.0\\nTable 8: GPT-3.5 predictions when models are explicitly instructed to avoid taking the attestedness of individual\\nstatements into account. In the upper half are the instructed behavior, and in the lower half are the regular few-shot\\nbehavior as in Table 4. Differences in recalls remain at a similar scale, with precision again stable, where the benefit\\nfrom the explicit instruction is marginal.\\nIn Table 7, we show that instructing GPT-3.5\\nto ignore attestation does not help narrow the gap\\nbetween Λ = Attested and Λ = ¬Attested;\\ninstead, probabilities of predicting Entail went\\ndown by similar amounts, indicating that the model\\nis becoming slightly more conservative in predict-\\ning positives when instructed to ignore attestation,\\nbut not in a principled manner.\\nFurther, as shown in Table 8, despite the ex-\\nplicit instruction, recall still drops at similar scales\\nwhen arguments are randomly replaced with the\\nsame sets of frequent/infrequent replacement enti-\\nties as before. Since GPT-3.5 has been instruction-\\nfinetuned to respond to prompts, its failure means\\neradicating such biases from model outputs is a\\ndifficult task, one that needs further research atten-\\ntion.\\nD Statistics of Consistency Subsets\\nThe statistics of consistency subsets are presented\\nin Table 9.\\nE The Reliability of Λ Measure and Its\\nRelation to Consensus of Attestation\\nThe Λ-consistency subsets most directly capture\\nthe impacts of the attestation bias. However, these\\nsubset separations are based on Λ predictions from\\nindividual models, which can be noisy, subject to\\nmodel-specific idiosyncracies such as trigger words\\nor certain syntactic structures in the prompt, etc.\\nTo verify that the performance gaps in Λ-\\nconsistency subsets that we observe in §8 comes\\nfrom predicted attestedness and not some idiosyn-\\ncrasy, we experiment with another pair of subsets\\nbased on consensus attestation instead of individu-\\nally predicted attestation.\\nWe use a majority vote among the three\\nindependently-trained LLMs to approximate con-\\nsensus attestation. The approximation is denoted\\nas ˜Λ. This is because any model-specific idiosyn-\\ncrasies should not be shared between LLMs in-\\ndependently trained from different source corpora\\nin general. Therefore, with the majority vote, we\\nreduce this noise and acquire predictions on the\\nconsensus attestation of statements.\\nPerformances of LLMs between ˜Λ-consistency\\nsubsets are listed in Table 10. Gaps between\\nthe ˜Λ-consistency subsets that are larger than Λ-\\nconsistency gaps are colored red; those narrower\\nthan Λ-consistency gaps are colored green. It is\\nclear that the gaps are consistent between Λ/˜Λ-\\nconsistency experiments, where the gaps are even\\nlarger on many occasions. This confirms that the\\n2771'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 14, 'page_label': '2772'}, page_content='# of Entries Levy/Holt RTE-1\\nLLaMA GPT-3.5 PaLM LLaMA GPT-3.5 PaLM\\nVCONSISTENT 955 947 999 479 447 480\\nVADVERSARIAL 829 837 785 321 353 320\\nFCONSISTENT 972 286\\nFADVERSARIAL 220 247\\nTable 9: Subsets defined by the consistency between entailment label L and either Λ (hypothesis attestation\\nprediction from each LLM) or Φ (model-agnostic relative frequency bias). CONSISTENT subsets are where L agrees\\nwith Λ/Φ. A DVERSARIAL subsets are where L disagrees with Λ/Φ.\\nLevy/Holt\\nModel Task ˜Λcons. ˜Λadv. diff.\\nLLaMA I 65.3 6.5 -58.8\\nGPT-3.5 I 70.8 23.5 -47.3\\nPaLM I 80.7 28.3 -52.4\\nLLaMA IGenArg 54.4 29.6 -24.8\\nGPT-3.5 IGenArg 56.2 35.5 -20.7\\nPaLM IGenArg 59.3 40.1 -19.2\\nTable 10: LLM performance on Levy/Holt subsets\\nwhere Attestation ˜Λ is Consistent/Adversarial to the\\nlabels, measured with AUCnorm (0% = random chance\\nperformance). Performance drops from ˜Λcons to ˜Λadv\\nare presented in the diff. columns, sharper decreases\\nthan Λ-comparisons in Table 5 are colored red, milder\\nones are colored green.\\nperformance gaps in Λ-consistency experiments\\ncan be credited to the attestation bias, rather than\\nmodel-specific idiosyncrasies.\\nIt is also to be noted that, since theΦ-consistency\\nsubsets are separated based on the model-agnostic\\ncriterion Φ, model-specific idiosyncrasies are not a\\nproblem for Φ-consistency comparisons.\\nF Impacts of Bias on GPT-4 Performance\\nGPT-4 (OpenAI, 2023) is a recent, strong LLM\\nclaiming SOTA performance on various NLP tasks.\\nDue to its closed-source nature and the impossibil-\\nity of fully tracking the sources of its behaviors, we\\nrefrain from reporting results with it in the main\\ncontent of this paper.\\nHowever, in order to provide a richer con-\\ntext for the attestation bias and the relative fre-\\nquency bias, in this section we report the perfor-\\nmance differences of GPT-4 between subsets con-\\nsistent/adversarial to the two biases.\\nAs a light-weight experiment, we elicit GPT-4\\npredictions in the original I task in the zero-shot\\nsetting, and re-use subsets from experiments in\\n§8. Specifically, for the attestation bias, we use\\nthe majority vote ˜Λ among LLaMA, GPT-3.5 and\\nPaLM, to approximate Λ predictions from GPT-4\\nitself; for the relative frequency bias, we keep the\\nΦ measure for approximating corpus-frequency of\\nterms.\\nBecause GPT-4 is a commercial service and does\\nnot provide logit confidence with their discrete pre-\\ndictions, AUCnorm values could not be calculated.\\nTherefore, we are forced to report the F-1 scores\\nat the binary prediction point of confidence. As\\nresults in Table 12 show, we observe the same trend\\nas in §8: for the subset adversarial to each factor,\\nGPT-4 performance also drops substantially.\\nThis experiment is designed to provide more con-\\ntext for the two biases discussed in the paper and\\nNOT to compare GPT-4 with other models; how-\\never, we can conclude that GPT-4 is subject to the\\nsame fragilities as the other LLMs w.r.t. the two bi-\\nases, where our conclusions and recommendations\\nalso apply.\\nG Dataset Statistics and Dev Set\\nPerformance\\nIn the paper, we have examined the behavior and\\nperformance of three major LLM families on two\\nNLI datasets: Levy/Holt and RTE-1.\\nThe directional portion of Levy/Holt dataset 9\\ncontains 630 entries in its dev set, and 1784 entries\\nin its test set; the RTE-1 dataset 10 contains 567\\nentries in its dev set, and 800 entries in its test\\nset. Each dataset has a 50%/50% class distribution\\nbetween Entail and No-Entail (for RTE-1\\ndev set, the numbers of entries in the two label\\nclasses differ by 1).\\nIn Table 11, we report dev set performance and\\nthe best prompt template used for each model on\\neach dataset. Note that no training is involved in\\n9https://github.com/mjhosseini/\\nentgraph_eval/tree/master/LevyHoltDS\\n10https://www.kaggle.com/datasets/\\nnltkdata/rte-corpus?resource=download\\n2772'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 15, 'page_label': '2773'}, page_content='Levy/Holt RTE-1\\nModel Task Best tplt. ID DEV set AUCnorm Best tplt. ID DEV set AUCnorm\\nLLaMA\\nI #4 30.0 #3 62.5\\nIGenArg #1 34.6 #3 52.3\\nIRandArg↓ #1 31.8 #1 51.3\\nIRandArg↑ #1 26.3 #3 43.8\\nGPT-3.5\\nI #1 49.2 #3 74.8\\nIGenArg #1 39.8 #3 64.8\\nIRandArg↓ #1 43.4 #3 63.6\\nIRandArg↑ #1 34.2 #3 66.0\\nPaLM\\nI #1 60.9 #4 84.5\\nIGenArg #1 48.1 #4 79.4\\nIRandArg↓ #1 43.6 #3 79.8\\nIRandArg↑ #1 35.3 #3 78.3\\nTable 11: LLM dev set performance on the two datasets, measured with AUCnorm (0% = random chance\\nperformance). AUC is calculated using estimated model scores as in §4.2 and then normalized into AUCnorm. We\\nselect the highest scoring template on each dev task (shown in this table) and use this in the corresponding test set\\nevaluation (shown in the main text).\\nF-1 score Task Levy/Holt\\n˜ΛCons ˜ΛAdv\\nrandom baseline I 70.3 62.0\\nGPT-4 I 85.1 (+14.8) 67.6 (+5.6)\\nΦCons ΦAdv\\nrandom baseline I 66.7 66.7\\nGPT-4 I 74.6 (+7.9) 69.7 (+3.0)\\nTable 12: LLM performance on Levy/Holt subsets\\nwhere Attestation ˜Λ is Consistent/Adversarial to the\\nlabels, measured with F-1 score. random baseline is the\\nhighest F-1 score from a random classifier, by reaching\\nrandom precision and 100% recall. For each GPT-4\\nscore, we also show the improvement over random (in\\nparentheses).\\nthis paper, and prompt template selection is the\\nonly hyper-parameter tuned on the dev sets. These\\nselected best prompt templates are then used on the\\nrespective test sets, where the results are used for\\nthe analysis throughout the paper.\\nFor random-premise experiments, AUC values\\ncannot be meaningfully calculated because gold\\nlabels are always No-Entail. For these ex-\\nperiments, we use the most frequently-selected\\nprompt template on each dataset, namely template\\n#1 for Levy/Holt dataset, and template #3 for RTE-\\n1 dataset.\\n2773'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-07T11:54:45+08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2023-12-07T11:54:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/drive/My Drive/documents_RAG/2023.findings-emnlp.182.pdf', 'total_pages': 17, 'page': 16, 'page_label': '2774'}, page_content='A. Zero-shot Example Instantiated Prompt\\nPlease check the entailments between the following statements.\\nIf kanamycin kills infections, then kanamycin is useful in infections.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nB. Few-shot Example Instantiated Prompt\\nIf Google bought Youtube, then Google owns Youtube.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nAnswer: A) Entailment. Owning is a consequence of buying.\\nIf Google owns Youtube, then Google bought Youtube.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nAnswer: B) Neutral. Owning does not imply buying, the ownership may come from other means.\\nIf John went to the mall, then John drove to the mall.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nAnswer: B) Neutral. John may have gone to the mall by other means.\\nIf John drove to the mall, then John went to the mall.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nAnswer: A) Entailment. Driving is a means of going to the mall.\\nIf ephedrine is widely used in medicine, then ephedrine is used in medicine.\\nA) Entailment\\nB) Neutral\\nC) Contradiction\\nAnswer:\\nC. Hypothesis-only Example Instantiated Prompt\\nGoogle bought Youtube.\\nA) True\\nB) Unknown\\nC) False\\nAnswer: A) True.\\nYoshua Bengio likes oak trees.\\nA) True\\nB) Unknown\\nC) False\\nAnswer: B) Unknown.\\nThe sun rises from the west.\\nA) True\\nB) Unknown\\nC) False\\nAnswer: C) False.\\nephedrine is used in medicine.\\nA) True\\nB) Unknown\\nC) False\\nAnswer:\\nTable 13: Example instantiated prompts in Zero-shot / Few-shot settings, for the sample “ PREMISE : [ephedrine\\nis widely used in medicine], HYPOTHESIS : [ephedrine is used in medicine]”. The few-shot prompts in part B are\\nused throughout the main experiments in this paper. We also present an example of the prompts we use for the\\nhypothesis-only Λ measure as described in §3.1.\\n2774')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LMO4zUtqCWXP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UqW6jCBnBju8"
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R69z-irACOsk"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a medical expert specialized in mental health disorders.\n",
    "Based on the following statement, classify it into one of these seven categories:\n",
    "\n",
    "1. Depression  \n",
    "2. Anxiety  \n",
    "3. Bipolar  \n",
    "4. Stress  \n",
    "5. Suicidal  \n",
    "6. Personality disorder  \n",
    "7. Normal\n",
    "\n",
    "If the statement does not indicate any significant mental health condition affecting thinking, feeling, or behavior, classify it as \"Normal\".\n",
    "\n",
    "Provide only the category name.\n",
    "\n",
    "Statement:\n",
    "{context}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iyUuIQ5_DOaP"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "retriever.search_kwargs[\"k\"] = 1\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" concatenates retrieved docs and feeds them into the prompt\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "A_EN4WUgDTdH"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAM7AvffDT5j",
    "outputId": "23dd0161-486c-4581-8dca-e85f76b8e3bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 0: This statement does not fall under any of the six categories listed. It is discussing research on neural machine translation and does not mention any mental health disorders.\n",
      "Processed row 1: 6. Personality disorder\n",
      "Processed row 2: Personality disorder\n",
      "Processed row 3: 6. Personality disorder\n",
      "Processed row 4: 6. Personality disorder\n",
      "Processed row 5: 6. Personality disorder\n",
      "Processed row 6: Personality disorder\n",
      "Processed row 7: 6. Personality disorder\n",
      "Processed row 8: 6. Personality disorder\n",
      "Processed row 9: 6. Personality disorder\n",
      "Processed row 10: 6. Personality disorder\n",
      "Processed row 11: Anxiety\n",
      "Processed row 12: 6. Personality disorder\n",
      "Processed row 13: Personality disorder\n",
      "Processed row 14: Anxiety\n",
      "Processed row 15: 6. Personality disorder\n",
      "Processed row 16: 6. Personality disorder\n",
      "Processed row 17: 6. Personality disorder\n",
      "Processed row 18: 6. Personality disorder\n",
      "Processed row 19: 6. Personality disorder\n",
      "Processed row 20: Personality disorder\n",
      "Processed row 21: 6. Personality disorder\n",
      "Processed row 22: 6. Personality disorder\n",
      "Processed row 23: 6. Personality disorder\n",
      "Processed row 24: 6. Personality disorder\n",
      "Processed row 25: 6. Personality disorder\n",
      "Processed row 26: 6. Personality disorder\n",
      "Processed row 27: Stress\n",
      "Processed row 28: 6. Personality disorder\n",
      "Processed row 29: 6. Personality disorder\n",
      "Processed row 30: Personality disorder\n",
      "Processed row 31: Anxiety\n",
      "Processed row 32: Personality disorder\n",
      "Processed row 33: Personality disorder\n",
      "Processed row 34: Personality disorder\n",
      "Processed row 35: Personality disorder\n",
      "Processed row 36: Stress\n",
      "Processed row 37: C) False\n",
      "Processed row 38: 6. Personality disorder\n",
      "Processed row 39: 6. Personality disorder\n",
      "Processed row 40: Anxiety\n",
      "Processed row 41: 6. Personality disorder\n",
      "Processed row 42: Personality disorder\n",
      "Processed row 43: Personality disorder\n",
      "Processed row 44: 6. Personality disorder\n",
      "Processed row 45: 6. Personality disorder\n",
      "Processed row 46: 6. Personality disorder\n",
      "Processed row 47: Personality disorder\n",
      "Processed row 48: Stress\n",
      "Processed row 49: 6. Personality disorder\n",
      "Processed row 50: Stress\n",
      "Processed row 51: 6. Personality disorder\n",
      "Processed row 52: 6. Personality disorder\n",
      "Processed row 53: Personality disorder\n",
      "Processed row 54: 6. Personality disorder\n",
      "Processed row 55: 6. Personality disorder\n",
      "Processed row 56: 6. Personality disorder\n",
      "Processed row 57: Personality disorder\n",
      "Processed row 58: 6. Personality disorder\n",
      "Processed row 59: Anxiety\n",
      "Processed row 60: 6. Personality disorder\n",
      "Processed row 61: Personality disorder\n",
      "Processed row 62: 6. Personality disorder\n",
      "Processed row 63: 6. Personality disorder\n",
      "Processed row 64: Personality disorder\n",
      "Processed row 65: 6. Personality disorder\n",
      "Processed row 66: Personality disorder\n",
      "Processed row 67: 6. Personality disorder\n",
      "Processed row 68: Personality disorder\n",
      "Processed row 69: 6. Personality disorder\n",
      "Processed row 70: Personality disorder\n",
      "Processed row 71: Stress\n",
      "Processed row 72: 6. Personality disorder\n",
      "Processed row 73: 6. Personality disorder\n",
      "Processed row 74: 6. Personality disorder\n",
      "Processed row 75: 6. Personality disorder\n",
      "Processed row 76: Personality disorder\n",
      "Processed row 77: 6. Personality disorder\n",
      "Processed row 78: Personality disorder\n",
      "Processed row 79: Personality disorder\n",
      "Processed row 80: 6. Personality disorder\n",
      "Processed row 81: 6. Personality disorder\n",
      "Processed row 82: Personality disorder\n",
      "Processed row 83: 6. Personality disorder\n",
      "Processed row 84: Personality disorder\n",
      "Processed row 85: 6. Personality disorder\n",
      "Processed row 86: 6. Personality disorder\n",
      "Processed row 87: 6. Personality disorder\n",
      "Processed row 88: 6. Personality disorder\n",
      "Processed row 89: Personality disorder\n",
      "Processed row 90: 6. Personality disorder\n",
      "Processed row 91: 6. Personality disorder\n",
      "Processed row 92: 6. Personality disorder\n",
      "Processed row 93: 6. Personality disorder\n",
      "Processed row 94: Personality disorder\n",
      "Processed row 95: 6. Personality disorder\n",
      "Processed row 96: 6. Personality disorder\n",
      "Processed row 97: Personality disorder\n",
      "Processed row 98: Personality disorder\n",
      "Processed row 99: 6. Personality disorder\n",
      "Processed row 100: Stress\n",
      "Processed row 101: 6. Personality disorder\n",
      "Processed row 102: 6. Personality disorder\n",
      "Processed row 103: 6. Personality disorder\n",
      "Processed row 104: 6. Personality disorder\n",
      "Processed row 105: Anxiety\n",
      "Processed row 106: Personality disorder\n",
      "Processed row 107: Personality disorder\n",
      "Processed row 108: Stress\n",
      "Processed row 109: 6. Personality disorder\n",
      "Processed row 110: 6. Personality disorder\n",
      "Processed row 111: 6. Personality disorder\n",
      "Processed row 112: 6. Personality disorder\n",
      "Processed row 113: 6. Personality disorder\n",
      "Processed row 114: 6. Personality disorder\n",
      "Processed row 115: 6. Personality disorder\n",
      "Processed row 116: Stress\n",
      "Processed row 117: Personality disorder\n",
      "Processed row 118: 6. Personality disorder\n",
      "Processed row 119: 6. Personality disorder\n",
      "Processed row 120: Personality disorder\n",
      "Processed row 121: 6. Personality disorder\n",
      "Processed row 122: Stress\n",
      "Processed row 123: 6. Personality disorder\n",
      "Processed row 124: Anxiety\n",
      "Processed row 125: 6. Personality disorder\n",
      "Processed row 126: 6. Personality disorder\n",
      "Processed row 127: C) False\n",
      "Processed row 128: 6. Personality disorder\n",
      "Processed row 129: Anxiety\n",
      "Processed row 130: 6. Personality disorder\n",
      "Processed row 131: 6. Personality disorder\n",
      "Processed row 132: Personality disorder\n",
      "Processed row 133: Anxiety\n",
      "Processed row 134: 6. Personality disorder\n",
      "Processed row 135: 6. Personality disorder\n",
      "Processed row 136: 6. Personality disorder\n",
      "Processed row 137: Stress\n",
      "Processed row 138: 6. Personality disorder\n",
      "Processed row 139: Personality disorder\n",
      "Processed row 140: 6. Personality disorder\n",
      "Processed row 141: 6. Personality disorder\n",
      "Processed row 142: Personality disorder\n",
      "Processed row 143: 6. Personality disorder\n",
      "Processed row 144: 6. Personality disorder\n",
      "Processed row 145: Personality disorder\n",
      "Processed row 146: 6. Personality disorder\n",
      "Processed row 147: 6. Personality disorder\n",
      "Processed row 148: 6. Personality disorder\n",
      "Processed row 149: Stress\n",
      "Processed row 150: Personality disorder\n",
      "Processed row 151: Personality disorder\n",
      "Processed row 152: Anxiety\n",
      "Processed row 153: 6. Personality disorder\n",
      "Processed row 154: Anxiety\n",
      "Processed row 155: Personality disorder\n",
      "Processed row 156: Personality disorder\n",
      "Processed row 157: Personality disorder\n",
      "Processed row 158: Personality disorder\n",
      "Processed row 159: 6. Personality disorder\n",
      "Processed row 160: 6. Personality disorder\n",
      "Processed row 161: Personality disorder\n",
      "Processed row 162: Personality disorder\n",
      "Processed row 163: 6. Personality disorder\n",
      "Processed row 164: 6. Personality disorder\n",
      "Processed row 165: Stress\n",
      "Processed row 166: 6. Personality disorder\n",
      "Processed row 167: 6. Personality disorder\n",
      "Processed row 168: 6. Personality disorder\n",
      "Processed row 169: 6. Personality disorder\n",
      "Processed row 170: 6. Personality disorder\n",
      "Processed row 171: 6. Personality disorder\n",
      "Processed row 172: Personality disorder\n",
      "Processed row 173: Anxiety\n",
      "Processed row 174: 6. Personality disorder\n",
      "Processed row 175: 6. Personality disorder\n",
      "Processed row 176: 6. Personality disorder\n",
      "Processed row 177: Personality disorder\n",
      "Processed row 178: 6. Personality disorder\n",
      "Processed row 179: Personality disorder\n",
      "Classification outputs saved to: /content/drive/My Drive/documents_RAG/my_dataset_classified_RAG.csv\n"
     ]
    }
   ],
   "source": [
    "if 'processed_statement' not in df.columns:\n",
    "    print(\"Error: 'statement' column not found in the dataset.\")\n",
    "else:\n",
    "    classifications = []\n",
    "    for idx, row in df.iterrows():\n",
    "        statement = row['processed_statement']\n",
    "        # Optionally, truncate the statement if it's very long\n",
    "        input_text = statement[:300]\n",
    "        # Use the statement as the query for the RAG chain.\n",
    "        classification = qa_chain.run(input_text)\n",
    "        classifications.append(classification.strip())\n",
    "        print(f\"Processed row {idx}: {classification.strip()}\")\n",
    "\n",
    "    # Add a new column to the dataset with the classification output.\n",
    "    df['classification'] = classifications\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. Save the updated dataset to a new CSV file\n",
    "    # -----------------------------\n",
    "    output_path = '/content/drive/My Drive/documents_RAG/my_dataset_classified_RAG.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(\"Classification outputs saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "3sXfxECFFiFb",
    "outputId": "49944ba3-fe55-4370-c91b-758d10ade079"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 180,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52,\n        \"min\": 0,\n        \"max\": 179,\n        \"num_unique_values\": 180,\n        \"samples\": [\n          19,\n          42,\n          153\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"if the guy is sick the girl actually feels sick indirectly he must be feeling restless and restless\",\n          \"did your depression start after a time of serious stressanxiety if so this may be why you feel so disconnected after times of intense stress where we do not give ourselves any leeway or breaks our body naturally will downregulate hormones to essentially force us to stop what were doing so we may have super high levels of glutamate and norepinepherine keeping us in a stress response but low levels of dopamine keeping us in a depression and the high amount of anxiety neurotransmitters because us to feel both depressed and depersonalized freeze response if you feel disconnected from reality this may be why\",\n          \"i am hurting lately i just feel like garbage i havent left the house much in like 2 weeks and ive been missing class it all feels too overwhelming for me but being at home makes me feel like trash tooi cant win i cant sleep right either i wake up every other hour and im so tired i feel so angry and anxious lately too because i feel as if everyone hates me and that theyre happier without me my friends hardly talk to me lately i see them on social media chatting and making plans to callplay games and im never included lately im not surprised though whod want to be friends with someone as depressed and broken as me i want to crawl in a hole and disappear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Anxiety\",\n          \"Depression\",\n          \"Personality disorder\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predictions_p3\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Anxiety\",\n          \"Bipolar\",\n          \"Anxiety \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a7bc7a49-7bcb-4aa7-ab3f-a4c26831bb73\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>processed_statement</th>\n",
       "      <th>status</th>\n",
       "      <th>predictions_p3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>trouble sleeping confused mind restless heart ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>all wrong back off dear forward doubt stay in ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ive shifted my focus to something else but im ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>im restless and restless its been a month now ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>who knew omg im home finally a place i belong</td>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>faking yourself does anybody else feel the nee...</td>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>177</td>\n",
       "      <td>im screwed maybe many of you are too one thing...</td>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>does avpd stem from childhood trauma hi lovely...</td>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>179</td>\n",
       "      <td>anxiety after the gym ever since i started goi...</td>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7bc7a49-7bcb-4aa7-ab3f-a4c26831bb73')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a7bc7a49-7bcb-4aa7-ab3f-a4c26831bb73 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a7bc7a49-7bcb-4aa7-ab3f-a4c26831bb73');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-faa0ff44-24e5-42c0-a543-f0b06b1d55d3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-faa0ff44-24e5-42c0-a543-f0b06b1d55d3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-faa0ff44-24e5-42c0-a543-f0b06b1d55d3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_637abdf3-4cc4-4597-b798-9e0ba57d1b43\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_637abdf3-4cc4-4597-b798-9e0ba57d1b43 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     Unnamed: 0                                processed_statement  \\\n",
       "0             0                                         oh my gosh   \n",
       "1             1  trouble sleeping confused mind restless heart ...   \n",
       "2             2  all wrong back off dear forward doubt stay in ...   \n",
       "3             3  ive shifted my focus to something else but im ...   \n",
       "4             4  im restless and restless its been a month now ...   \n",
       "..          ...                                                ...   \n",
       "175         175      who knew omg im home finally a place i belong   \n",
       "176         176  faking yourself does anybody else feel the nee...   \n",
       "177         177  im screwed maybe many of you are too one thing...   \n",
       "178         178  does avpd stem from childhood trauma hi lovely...   \n",
       "179         179  anxiety after the gym ever since i started goi...   \n",
       "\n",
       "                   status predictions_p3  \n",
       "0                 Anxiety       Anxiety   \n",
       "1                 Anxiety        Anxiety  \n",
       "2                 Anxiety        Anxiety  \n",
       "3                 Anxiety        Anxiety  \n",
       "4                 Anxiety        Anxiety  \n",
       "..                    ...            ...  \n",
       "175  Personality disorder         Normal  \n",
       "176  Personality disorder     Depression  \n",
       "177  Personality disorder     Depression  \n",
       "178  Personality disorder         Normal  \n",
       "179  Personality disorder        Anxiety  \n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkcmbbtyGYPu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
